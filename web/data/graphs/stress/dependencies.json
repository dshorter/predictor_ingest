{
  "meta": {
    "view": "dependencies",
    "nodeCount": 806,
    "edgeCount": 2362,
    "exportedAt": "2026-01-25T12:00:00Z",
    "dateRange": {
      "start": "2025-10-25",
      "end": "2026-01-25"
    }
  },
  "elements": {
    "nodes": [
      {
        "data": {
          "id": "repo:transformers",
          "label": "transformers",
          "type": "Repo",
          "aliases": [
            "huggingface/transformers"
          ],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 8,
          "mentionCount30d": 46,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tool:dify_v2",
          "label": "Dify v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 9,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe_lite",
          "label": "Tokenizer BPE Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-09",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 13,
          "mentionCount30d": 32,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization_edge",
          "label": "Direct Preference Optimization Edge",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 3,
          "mentionCount30d": 11,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_max",
          "label": "Claude Sonnet 4 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 12,
          "mentionCount30d": 41,
          "velocity": 0.92
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_mini",
          "label": "Mixtral 8x22B Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 20,
          "mentionCount30d": 91,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_v2",
          "label": "Tree of Thought v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-22",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 6,
          "mentionCount30d": 47,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "repo:autogpt",
          "label": "AutoGPT",
          "type": "Repo",
          "aliases": [
            "Significant-Gravitas/AutoGPT"
          ],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "benchmark:math_v2",
          "label": "MATH v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2",
          "label": "GPT-4o Mini 2",
          "type": "Model",
          "aliases": [
            "GPT4o Mini 2"
          ],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 1,
          "mentionCount30d": 7,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_core",
          "label": "Chain-of-Thought Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "tech:rlhf_ultra",
          "label": "RLHF Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 8,
          "mentionCount30d": 36,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tool:cursor",
          "label": "Cursor",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.73
        }
      },
      {
        "data": {
          "id": "tool:cody",
          "label": "Cody",
          "type": "Tool",
          "aliases": [
            "Sourcegraph Cody"
          ],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 18,
          "mentionCount30d": 27,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_ultra",
          "label": "GPT-4o Mini 2 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 17,
          "mentionCount30d": 46,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases",
          "label": "Weights & Biases",
          "type": "Tool",
          "aliases": [
            "W&B",
            "wandb"
          ],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 14,
          "mentionCount30d": 26,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tool:haystack_lite",
          "label": "Haystack Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-10",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 9,
          "mentionCount30d": 28,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "model:grok_3_v2",
          "label": "Grok-3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 14,
          "mentionCount30d": 18,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_pro",
          "label": "SlimPajama Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 0,
          "mentionCount30d": 1,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_ultra",
          "label": "FineWeb Ultra",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:gradio_plus",
          "label": "Gradio Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 5,
          "mentionCount30d": 8,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:llama_4_edge",
          "label": "Llama 4 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 30,
          "mentionCount30d": 38,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "model:aya_3_mini",
          "label": "Aya 3 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 16,
          "mentionCount30d": 32,
          "velocity": 0.35
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2_edge",
          "label": "OpenWebText2 Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 3,
          "mentionCount30d": 28,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_max",
          "label": "Mixtral 8x22B Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 7,
          "mentionCount30d": 48,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_v2",
          "label": "Midjourney V7 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 25,
          "mentionCount30d": 53,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_mini",
          "label": "SlimPajama Mini",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 1,
          "mentionCount30d": 8,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b",
          "label": "LAION-5B",
          "type": "Dataset",
          "aliases": [
            "LAION"
          ],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-11-13",
          "mentionCount7d": 13,
          "mentionCount30d": 47,
          "velocity": 0.18
        }
      },
      {
        "data": {
          "id": "tech:tool_use_mini",
          "label": "Tool Use Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 2,
          "mentionCount30d": 3,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "model:jamba_2",
          "label": "Jamba 2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 4,
          "mentionCount30d": 14,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "model:phi_4_max",
          "label": "Phi-4 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 19,
          "mentionCount30d": 30,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe_core",
          "label": "Tokenizer BPE Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 6,
          "mentionCount30d": 28,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation_next",
          "label": "Retrieval-Augmented Generation Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 8,
          "mentionCount30d": 27,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "model:gpt_5_next",
          "label": "GPT-5 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 38,
          "mentionCount30d": 87,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "tool:mlflow_plus",
          "label": "MLflow Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 4,
          "mentionCount30d": 10,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "tool:vllm_core",
          "label": "vLLM Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-21",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 14,
          "mentionCount30d": 34,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_max",
          "label": "Multimodal Fusion Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 5,
          "mentionCount30d": 17,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe_next",
          "label": "Tokenizer BPE Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 5,
          "mentionCount30d": 12,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "tech:lora",
          "label": "LoRA",
          "type": "Tech",
          "aliases": [
            "Low-Rank Adaptation"
          ],
          "firstSeen": "2025-12-22",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 33,
          "mentionCount30d": 36,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tool:dify_lite",
          "label": "Dify Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 21,
          "mentionCount30d": 44,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_mini",
          "label": "Nemotron-5 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 23,
          "mentionCount30d": 58,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "paper:llm_agents:_a_survey_pro",
          "label": "LLM Agents: A Survey Pro",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 19,
          "mentionCount30d": 41,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:gpt_5_lite",
          "label": "GPT-5 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2025-11-29",
          "mentionCount7d": 9,
          "mentionCount30d": 29,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tech:qlora_mini",
          "label": "QLoRA Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 7,
          "mentionCount30d": 17,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:localai_mini",
          "label": "LocalAI Mini",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-11-30",
          "mentionCount7d": 2,
          "mentionCount30d": 23,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_lite",
          "label": "Chain-of-Thought Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_ultra",
          "label": "Red Teaming Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 7,
          "mentionCount30d": 24,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_plus",
          "label": "Constitutional AI Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 5,
          "mentionCount30d": 29,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "tool:gradio_mini",
          "label": "Gradio Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 9,
          "mentionCount30d": 21,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_edge",
          "label": "Speculative Decoding Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 21,
          "mentionCount30d": 47,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "repo:transformers_plus",
          "label": "transformers Plus",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 6,
          "mentionCount30d": 37,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "model:grok_3_ultra",
          "label": "Grok-3 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 41,
          "mentionCount30d": 64,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "tool:cursor_v2",
          "label": "Cursor v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 22,
          "mentionCount30d": 30,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "paper:chain_of_thought_prompting_elicits_reaso",
          "label": "Chain-of-Thought Prompting Elicits Reasoning",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 3,
          "mentionCount30d": 5,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tool:mlflow_v2",
          "label": "MLflow v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 9,
          "mentionCount30d": 20,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_lite",
          "label": "Mixture of Experts Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 8,
          "mentionCount30d": 13,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tool:flowise",
          "label": "Flowise",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 10,
          "mentionCount30d": 20,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "dataset:dolma_next",
          "label": "Dolma Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 18,
          "mentionCount30d": 19,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_core",
          "label": "Tree of Thought Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 36,
          "mentionCount30d": 62,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data_mini",
          "label": "StarCoder Data Mini",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 12,
          "mentionCount30d": 37,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention_core",
          "label": "Group Query Attention Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2025-11-23",
          "mentionCount7d": 10,
          "mentionCount30d": 31,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "tech:dpo_lite",
          "label": "DPO Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-11-16",
          "mentionCount7d": 7,
          "mentionCount30d": 35,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "repo:open_interpreter_core",
          "label": "open-interpreter Core",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 6,
          "mentionCount30d": 53,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "tool:flowise_plus",
          "label": "Flowise Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-12-07",
          "mentionCount7d": 5,
          "mentionCount30d": 25,
          "velocity": 0.16
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_edge",
          "label": "Tree of Thought Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 8,
          "mentionCount30d": 51,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_edge",
          "label": "Mixtral 8x22B Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 14,
          "mentionCount30d": 49,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "model:gemma_3_pro",
          "label": "Gemma 3 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-11",
          "mentionCount7d": 14,
          "mentionCount30d": 78,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_max",
          "label": "RedPajama v2 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 8,
          "mentionCount30d": 47,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_mini",
          "label": "Flash Attention Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 9,
          "mentionCount30d": 16,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "model:phi_4_pro",
          "label": "Phi-4 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 9,
          "mentionCount30d": 54,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_plus",
          "label": "Mixtral 8x22B Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 43,
          "mentionCount30d": 122,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa",
          "label": "GPQA",
          "type": "Benchmark",
          "aliases": [
            "Graduate-Level Google-Proof QA"
          ],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 6,
          "mentionCount30d": 44,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "tool:streamlit_lite",
          "label": "Streamlit Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 0,
          "mentionCount30d": 2,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation_lite",
          "label": "Retrieval-Augmented Generation Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 27,
          "mentionCount30d": 39,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande_edge",
          "label": "WinoGrande Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 3,
          "mentionCount30d": 25,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "model:qwen_3",
          "label": "Qwen-3",
          "type": "Model",
          "aliases": [
            "Qwen3",
            "Qwen 3"
          ],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 17,
          "mentionCount30d": 42,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard",
          "label": "BigBench Hard",
          "type": "Benchmark",
          "aliases": [
            "BBH"
          ],
          "firstSeen": "2025-11-16",
          "lastSeen": "2025-11-25",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_edge",
          "label": "Chain-of-Thought Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 9,
          "mentionCount30d": 69,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_lite",
          "label": "Speculative Decoding Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 10,
          "mentionCount30d": 16,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_lite",
          "label": "FineWeb Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 5,
          "mentionCount30d": 41,
          "velocity": 0.35
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_pro",
          "label": "Claude Opus 4.5 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 15,
          "mentionCount30d": 75,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "repo:ollama_edge",
          "label": "ollama Edge",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 2,
          "mentionCount30d": 15,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "tool:haystack_edge",
          "label": "Haystack Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 9,
          "mentionCount30d": 22,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_edge",
          "label": "Nemotron-5 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 11,
          "mentionCount30d": 41,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_core",
          "label": "GSM8K Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 1,
          "mentionCount30d": 15,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu",
          "label": "MMLU",
          "type": "Benchmark",
          "aliases": [
            "Massive Multitask Language Understanding"
          ],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 15,
          "mentionCount30d": 26,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "repo:vllm_pro",
          "label": "vllm Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 11,
          "mentionCount30d": 20,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_edge",
          "label": "Transformer Architecture Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 5,
          "mentionCount30d": 33,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_lite",
          "label": "Nemotron-5 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 12,
          "mentionCount30d": 37,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "repo:open_interpreter_v2",
          "label": "open-interpreter v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 3,
          "mentionCount30d": 11,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "tech:dpo_mini",
          "label": "DPO Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 16,
          "mentionCount30d": 60,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "model:codex_2_v2",
          "label": "Codex 2 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 7,
          "mentionCount30d": 41,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "tool:ollama_edge",
          "label": "Ollama Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-11-08",
          "mentionCount7d": 15,
          "mentionCount30d": 33,
          "velocity": 0.09
        }
      },
      {
        "data": {
          "id": "model:grok_3_next",
          "label": "Grok-3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 25,
          "mentionCount30d": 90,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "repo:text_generation_webui_next",
          "label": "text-generation-webui Next",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:cody_pro",
          "label": "Cody Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 11,
          "mentionCount30d": 31,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "dataset:mc4_edge",
          "label": "mC4 Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 14,
          "mentionCount30d": 33,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_edge",
          "label": "Stable Diffusion 4 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 14,
          "mentionCount30d": 35,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45",
          "label": "Claude Opus 4.5",
          "type": "Model",
          "aliases": [
            "Opus 4.5",
            "Claude Opus"
          ],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 44,
          "mentionCount30d": 76,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_next",
          "label": "GPT-4o Mini 2 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 22,
          "mentionCount30d": 55,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "model:gpt_5_pro",
          "label": "GPT-5 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 4,
          "mentionCount30d": 31,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "dataset:c4_core",
          "label": "C4 Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 2,
          "mentionCount30d": 16,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_v2",
          "label": "TensorRT-LLM v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 2,
          "mentionCount30d": 8,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_mini",
          "label": "DALL-E 4 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 18,
          "mentionCount30d": 35,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_next",
          "label": "Sliding Window Attention Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "dataset:mc4_max",
          "label": "mC4 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 10,
          "mentionCount30d": 31,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_max",
          "label": "Stable Diffusion 4 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 35,
          "mentionCount30d": 98,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_lite",
          "label": "Red Teaming Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-22",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:llamacpp_v2",
          "label": "llama.cpp v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-07",
          "lastSeen": "2025-11-28",
          "mentionCount7d": 14,
          "mentionCount30d": 34,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_ultra",
          "label": "Transformer Architecture Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 17,
          "mentionCount30d": 63,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "tech:gguf",
          "label": "GGUF",
          "type": "Tech",
          "aliases": [
            "GGML"
          ],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 10,
          "mentionCount30d": 21,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "model:gpt_5_core",
          "label": "GPT-5 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 19,
          "mentionCount30d": 43,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "tool:cody_plus",
          "label": "Cody Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 11,
          "mentionCount30d": 44,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2_next",
          "label": "OpenWebText2 Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 10,
          "mentionCount30d": 28,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "tool:dify_max",
          "label": "Dify Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 18,
          "mentionCount30d": 25,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "tool:vllm_next",
          "label": "vLLM Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 17,
          "mentionCount30d": 37,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_lite",
          "label": "Weights & Biases Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 4,
          "mentionCount30d": 5,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:grok_3_max",
          "label": "Grok-3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 13,
          "mentionCount30d": 49,
          "velocity": 0.21
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_lite",
          "label": "Whisper v4 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2025-12-16",
          "mentionCount7d": 4,
          "mentionCount30d": 29,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "dataset:dolma",
          "label": "Dolma",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "repo:langchain_edge",
          "label": "langchain Edge",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 18,
          "mentionCount30d": 26,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:qwen_3_pro",
          "label": "Qwen-3 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 4,
          "mentionCount30d": 9,
          "velocity": 0.72
        }
      },
      {
        "data": {
          "id": "model:gpt_5_max",
          "label": "GPT-5 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 28,
          "mentionCount30d": 30,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm",
          "label": "TensorRT-LLM",
          "type": "Tool",
          "aliases": [
            "TensorRT LLM"
          ],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 1,
          "mentionCount30d": 2,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_mini",
          "label": "Midjourney V7 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 20,
          "mentionCount30d": 50,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:distillation_pro",
          "label": "Distillation Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 5,
          "mentionCount30d": 21,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama",
          "label": "SlimPajama",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 1,
          "mentionCount30d": 9,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "paper:scaling_laws_for_neural_language_models_",
          "label": "Scaling Laws for Neural Language Models (2025)",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 7,
          "mentionCount30d": 31,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "model:phi_4_edge",
          "label": "Phi-4 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 8,
          "mentionCount30d": 62,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_plus",
          "label": "Mixture of Experts Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "paper:scaling_data_constrained_language_models",
          "label": "Scaling Data-Constrained Language Models",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 4,
          "mentionCount30d": 8,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_max",
          "label": "Attention Is All You Need v2 Max",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-12-18",
          "mentionCount7d": 20,
          "mentionCount30d": 39,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi_lite",
          "label": "ARC-AGI Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 9,
          "mentionCount30d": 32,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_pro",
          "label": "Semantic Kernel Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 12,
          "mentionCount30d": 18,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:distillation_next",
          "label": "Distillation Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-10-30",
          "mentionCount7d": 6,
          "mentionCount30d": 39,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa_edge",
          "label": "TruthfulQA Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 11,
          "mentionCount30d": 25,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_core",
          "label": "Synthetic Data Generation Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 15,
          "mentionCount30d": 29,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "paper:self_play_fine_tuning_for_language_model",
          "label": "Self-Play Fine-Tuning for Language Models",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 0,
          "mentionCount30d": 1,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization",
          "label": "Direct Preference Optimization",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 19,
          "mentionCount30d": 45,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tool:dify_mini",
          "label": "Dify Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 1,
          "mentionCount30d": 15,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_plus",
          "label": "SlimPajama Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 7,
          "mentionCount30d": 41,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_edge",
          "label": "Command R+ Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 31,
          "mentionCount30d": 78,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "tech:gguf_plus",
          "label": "GGUF Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 2,
          "mentionCount30d": 14,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "model:yi_large_max",
          "label": "Yi-Large Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2025-12-16",
          "mentionCount7d": 23,
          "mentionCount30d": 39,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_next",
          "label": "Mixtral 8x22B Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 22,
          "mentionCount30d": 44,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "repo:transformers_next",
          "label": "transformers Next",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 4,
          "mentionCount30d": 13,
          "velocity": 0.18
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_plus",
          "label": "Attention Is All You Need v2 Plus",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 12,
          "mentionCount30d": 15,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "tool:vllm_max",
          "label": "vLLM Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 15,
          "mentionCount30d": 19,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande",
          "label": "WinoGrande",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 10,
          "mentionCount30d": 36,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "tool:autogpt_edge",
          "label": "AutoGPT Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 14,
          "velocity": 0.99
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_max",
          "label": "TensorRT-LLM Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 4,
          "mentionCount30d": 14,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "tool:gradio_max",
          "label": "Gradio Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 4,
          "mentionCount30d": 9,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "dataset:dolma_lite",
          "label": "Dolma Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_mini",
          "label": "Sliding Window Attention Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 8,
          "mentionCount30d": 66,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_pro",
          "label": "Gemini Ultra 2 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 16,
          "mentionCount30d": 125,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization_ultra",
          "label": "Direct Preference Optimization Ultra",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 8,
          "mentionCount30d": 38,
          "velocity": 0.05
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena_max",
          "label": "LMSYS Chatbot Arena Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 17,
          "mentionCount30d": 30,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "tool:cody_ultra",
          "label": "Cody Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 11,
          "mentionCount30d": 16,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_pro",
          "label": "Mixtral 8x22B Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-08",
          "mentionCount7d": 28,
          "mentionCount30d": 69,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "tool:copilot_core",
          "label": "Copilot Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-11",
          "mentionCount7d": 6,
          "mentionCount30d": 10,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "dataset:c4",
          "label": "C4",
          "type": "Dataset",
          "aliases": [
            "Colossal Clean Crawled Corpus"
          ],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention_ultra",
          "label": "Group Query Attention Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tech:tool_use_lite",
          "label": "Tool Use Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 6,
          "mentionCount30d": 17,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "tech:quantization_lite",
          "label": "Quantization Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 20,
          "mentionCount30d": 37,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_max",
          "label": "RefinedWeb Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 11,
          "mentionCount30d": 24,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "repo:llamacpp_pro",
          "label": "llama.cpp Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 20,
          "mentionCount30d": 26,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande_plus",
          "label": "WinoGrande Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 9,
          "mentionCount30d": 44,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_next",
          "label": "KV Cache Optimization Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-06",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 17,
          "mentionCount30d": 70,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:crewai_pro",
          "label": "CrewAI Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 14,
          "mentionCount30d": 28,
          "velocity": 0.25
        }
      },
      {
        "data": {
          "id": "tech:quantization",
          "label": "Quantization",
          "type": "Tech",
          "aliases": [
            "GPTQ",
            "AWQ"
          ],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 4,
          "mentionCount30d": 16,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_next",
          "label": "DeepSeek-V3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 34,
          "mentionCount30d": 77,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "tool:gradio_core",
          "label": "Gradio Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 4,
          "mentionCount30d": 15,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "tech:gguf_lite",
          "label": "GGUF Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 13,
          "mentionCount30d": 74,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_next",
          "label": "Chain-of-Thought Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 26,
          "mentionCount30d": 78,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_lite",
          "label": "SlimPajama Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 19,
          "mentionCount30d": 42,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "tool:haystack_core",
          "label": "Haystack Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2025-12-11",
          "mentionCount7d": 10,
          "mentionCount30d": 32,
          "velocity": 0.22
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_ultra",
          "label": "Flash Attention Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 3,
          "mentionCount30d": 11,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_edge",
          "label": "HumanEval Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 8,
          "mentionCount30d": 39,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "model:palm_3",
          "label": "PaLM 3",
          "type": "Model",
          "aliases": [
            "PaLM3"
          ],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 39,
          "mentionCount30d": 96,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "paper:mixture_of_experts_meets_instruction_tun",
          "label": "Mixture of Experts Meets Instruction Tuning",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 11,
          "mentionCount30d": 30,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "tech:tool_use",
          "label": "Tool Use",
          "type": "Tech",
          "aliases": [
            "Function Calling"
          ],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 12,
          "mentionCount30d": 59,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_plus",
          "label": "RedPajama v2 Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 7,
          "mentionCount30d": 25,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2",
          "label": "Gemini Ultra 2",
          "type": "Model",
          "aliases": [
            "Gemini 2",
            "Gemini Ultra"
          ],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 4,
          "mentionCount30d": 34,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_plus",
          "label": "TensorRT-LLM Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-22",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 8,
          "mentionCount30d": 16,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "tech:dpo_max",
          "label": "DPO Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 1,
          "mentionCount30d": 10,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tool:copilot_ultra",
          "label": "Copilot Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_v2",
          "label": "Sliding Window Attention v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 10,
          "mentionCount30d": 24,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:nemotron_5",
          "label": "Nemotron-5",
          "type": "Model",
          "aliases": [
            "Nemotron 5"
          ],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 36,
          "mentionCount30d": 67,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_mini",
          "label": "HellaSwag Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 10,
          "mentionCount30d": 34,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "model:falcon_3_v2",
          "label": "Falcon 3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 31,
          "mentionCount30d": 93,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "tool:localai_pro",
          "label": "LocalAI Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 10,
          "mentionCount30d": 24,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "model:jamba_2_edge",
          "label": "Jamba 2 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 8,
          "mentionCount30d": 36,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena_core",
          "label": "LMSYS Chatbot Arena Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 6,
          "mentionCount30d": 17,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena_plus",
          "label": "LMSYS Chatbot Arena Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 5,
          "mentionCount30d": 25,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "tool:cursor_max",
          "label": "Cursor Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 10,
          "mentionCount30d": 28,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "dataset:c4_max",
          "label": "C4 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 9,
          "mentionCount30d": 19,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_plus",
          "label": "HumanEval Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 6,
          "mentionCount30d": 22,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2",
          "label": "RedPajama v2",
          "type": "Dataset",
          "aliases": [
            "RedPajama"
          ],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-11",
          "mentionCount7d": 12,
          "mentionCount30d": 29,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "repo:langchain_lite",
          "label": "langchain Lite",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 3,
          "mentionCount30d": 17,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_core",
          "label": "Command R+ Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 13,
          "mentionCount30d": 21,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_mini",
          "label": "Tree of Thought Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 25,
          "mentionCount30d": 40,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe",
          "label": "Tokenizer BPE",
          "type": "Tech",
          "aliases": [
            "Byte Pair Encoding"
          ],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 34,
          "mentionCount30d": 49,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:gemma_3_mini",
          "label": "Gemma 3 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 11,
          "mentionCount30d": 82,
          "velocity": 0.99
        }
      },
      {
        "data": {
          "id": "benchmark:math",
          "label": "MATH",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-11-14",
          "mentionCount7d": 8,
          "mentionCount30d": 18,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "repo:text_generation_webui",
          "label": "text-generation-webui",
          "type": "Repo",
          "aliases": [
            "oobabooga"
          ],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:yi_large_mini",
          "label": "Yi-Large Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 7,
          "mentionCount30d": 31,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "paper:textbooks_are_all_you_need_max",
          "label": "Textbooks Are All You Need Max",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 9,
          "mentionCount30d": 28,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "paper:flash_attention:_fast_and_memory_efficie",
          "label": "Flash Attention: Fast and Memory-Efficient Attention",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-11-18",
          "mentionCount7d": 4,
          "mentionCount30d": 24,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_core",
          "label": "DALL-E 4 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-11-30",
          "mentionCount7d": 21,
          "mentionCount30d": 138,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_mini",
          "label": "The Stack v2 Mini",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 12,
          "mentionCount30d": 31,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_ultra",
          "label": "GSM8K Ultra",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "model:aya_3",
          "label": "Aya 3",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 16,
          "mentionCount30d": 62,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "tech:rlhf_core",
          "label": "RLHF Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 5,
          "mentionCount30d": 10,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "model:gemma_3_lite",
          "label": "Gemma 3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 30,
          "mentionCount30d": 72,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "dataset:c4_ultra",
          "label": "C4 Ultra",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 2,
          "mentionCount30d": 2,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tech:rlhf",
          "label": "RLHF",
          "type": "Tech",
          "aliases": [
            "Reinforcement Learning from Human Feedback"
          ],
          "firstSeen": "2025-12-12",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 20,
          "mentionCount30d": 26,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding",
          "label": "Speculative Decoding",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 9,
          "mentionCount30d": 33,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3",
          "label": "DeepSeek-V3",
          "type": "Model",
          "aliases": [
            "DeepSeek V3"
          ],
          "firstSeen": "2025-11-12",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 23,
          "mentionCount30d": 50,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_lite",
          "label": "GPT-4o Mini 2 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 11,
          "mentionCount30d": 80,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_plus",
          "label": "HellaSwag Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 5,
          "mentionCount30d": 15,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "model:falcon_3_max",
          "label": "Falcon 3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 43,
          "mentionCount30d": 64,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "model:sora_2_next",
          "label": "Sora 2 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 16,
          "mentionCount30d": 60,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_v2",
          "label": "DeepSeek-V3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 5,
          "mentionCount30d": 28,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_v2",
          "label": "Claude Opus 4.5 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 5,
          "mentionCount30d": 37,
          "velocity": 0.11
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_edge",
          "label": "Red Teaming Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe_ultra",
          "label": "Tokenizer BPE Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 15,
          "mentionCount30d": 41,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:langchain_pro",
          "label": "LangChain Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 2,
          "mentionCount30d": 19,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "tech:gguf_max",
          "label": "GGUF Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-27",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 16,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "model:qwen_3_core",
          "label": "Qwen-3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 8,
          "mentionCount30d": 51,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_max",
          "label": "LlamaIndex Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 3,
          "mentionCount30d": 15,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_next",
          "label": "MMLU Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 11,
          "mentionCount30d": 14,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation",
          "label": "Retrieval-Augmented Generation",
          "type": "Tech",
          "aliases": [
            "RAG"
          ],
          "firstSeen": "2026-01-21",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 6,
          "mentionCount30d": 29,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_mini",
          "label": "Whisper v4 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 52,
          "mentionCount30d": 117,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_v2",
          "label": "Multimodal Fusion v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 14,
          "mentionCount30d": 54,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "repo:text_generation_webui_pro",
          "label": "text-generation-webui Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 7,
          "mentionCount30d": 37,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande_pro",
          "label": "WinoGrande Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-07",
          "mentionCount7d": 1,
          "mentionCount30d": 12,
          "velocity": 0.73
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench_max",
          "label": "SWE-bench Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-11-14",
          "mentionCount7d": 0,
          "mentionCount30d": 2,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:localai_max",
          "label": "LocalAI Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-11-06",
          "mentionCount7d": 6,
          "mentionCount30d": 15,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_mini",
          "label": "GSM8K Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 3,
          "mentionCount30d": 20,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "tech:qlora_core",
          "label": "QLoRA Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 11,
          "mentionCount30d": 43,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "tool:dify_edge",
          "label": "Dify Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_plus",
          "label": "BigBench Hard Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-12-07",
          "mentionCount7d": 6,
          "mentionCount30d": 28,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "model:qwen_3_next",
          "label": "Qwen-3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 47,
          "mentionCount30d": 97,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_plus",
          "label": "MMLU Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 15,
          "mentionCount30d": 43,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "tech:dpo",
          "label": "DPO",
          "type": "Tech",
          "aliases": [
            "Direct Preference Optimization"
          ],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 6,
          "mentionCount30d": 16,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_edge",
          "label": "BigBench Hard Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 3,
          "mentionCount30d": 25,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "tool:autogpt_pro",
          "label": "AutoGPT Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 4,
          "mentionCount30d": 6,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl_edge",
          "label": "Common Crawl Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 18,
          "mentionCount30d": 41,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:palm_3_max",
          "label": "PaLM 3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 4,
          "mentionCount30d": 9,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "tool:haystack_plus",
          "label": "Haystack Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-21",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 3,
          "mentionCount30d": 22,
          "velocity": 0.72
        }
      },
      {
        "data": {
          "id": "model:falcon_3_ultra",
          "label": "Falcon 3 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 14,
          "mentionCount30d": 22,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "paper:toolformer:_language_models_can_teach_th",
          "label": "Toolformer: Language Models Can Teach Themselves to Use Tools",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 5,
          "mentionCount30d": 12,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena_edge",
          "label": "LMSYS Chatbot Arena Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 4,
          "mentionCount30d": 20,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "dataset:c4_next",
          "label": "C4 Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 6,
          "mentionCount30d": 17,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa_pro",
          "label": "TruthfulQA Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 0,
          "mentionCount30d": 1,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization_v2",
          "label": "Direct Preference Optimization v2",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 18,
          "mentionCount30d": 41,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention_lite",
          "label": "Sparse Attention Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 14,
          "mentionCount30d": 45,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_plus",
          "label": "FineWeb Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 4,
          "mentionCount30d": 14,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_plus",
          "label": "Claude Sonnet 4 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-07",
          "lastSeen": "2025-11-12",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "dataset:the_pile_max",
          "label": "The Pile Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 22,
          "mentionCount30d": 34,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "tech:qlora_plus",
          "label": "QLoRA Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 3,
          "mentionCount30d": 27,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl_core",
          "label": "Common Crawl Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 12,
          "mentionCount30d": 21,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_plus",
          "label": "Rotary Position Embedding Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 26,
          "mentionCount30d": 38,
          "velocity": 0.06
        }
      },
      {
        "data": {
          "id": "tech:lora_max",
          "label": "LoRA Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 21,
          "mentionCount30d": 49,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_core",
          "label": "Claude Sonnet 4 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 12,
          "mentionCount30d": 66,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_edge",
          "label": "Gemini Ultra 2 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 3,
          "mentionCount30d": 11,
          "velocity": 0.1
        }
      },
      {
        "data": {
          "id": "tool:crewai_v2",
          "label": "CrewAI v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-31",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 7,
          "mentionCount30d": 22,
          "velocity": 0.92
        }
      },
      {
        "data": {
          "id": "tool:mlflow_ultra",
          "label": "MLflow Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-01",
          "mentionCount7d": 6,
          "mentionCount30d": 14,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_lite",
          "label": "Semantic Kernel Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 8,
          "mentionCount30d": 18,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_max",
          "label": "Transformer Architecture Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 16,
          "mentionCount30d": 64,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:codex_2_lite",
          "label": "Codex 2 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-10",
          "mentionCount7d": 22,
          "mentionCount30d": 46,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "model:yi_large_next",
          "label": "Yi-Large Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 17,
          "mentionCount30d": 106,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_core",
          "label": "Whisper v4 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 21,
          "mentionCount30d": 83,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "benchmark:math_max",
          "label": "MATH Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 7,
          "mentionCount30d": 17,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_next",
          "label": "BigBench Hard Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 7,
          "mentionCount30d": 15,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "model:gemma_3",
          "label": "Gemma 3",
          "type": "Model",
          "aliases": [
            "Gemma3"
          ],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-11-25",
          "mentionCount7d": 6,
          "mentionCount30d": 10,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "tool:cursor_plus",
          "label": "Cursor Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "tech:rlhf_next",
          "label": "RLHF Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 8,
          "mentionCount30d": 25,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_lite",
          "label": "Gemini Ultra 2 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 30,
          "mentionCount30d": 66,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai",
          "label": "Constitutional AI",
          "type": "Tech",
          "aliases": [
            "CAI"
          ],
          "firstSeen": "2025-12-29",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 23,
          "mentionCount30d": 60,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "tool:cursor_core",
          "label": "Cursor Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 10,
          "mentionCount30d": 26,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_ultra",
          "label": "Sliding Window Attention Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 7,
          "mentionCount30d": 59,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "dataset:fineweb",
          "label": "FineWeb",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 11,
          "mentionCount30d": 45,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "repo:autogpt_plus",
          "label": "AutoGPT Plus",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_pro",
          "label": "RedPajama v2 Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 17,
          "mentionCount30d": 44,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion",
          "label": "Multimodal Fusion",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 9,
          "mentionCount30d": 50,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "model:gpt_5_ultra",
          "label": "GPT-5 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 29,
          "mentionCount30d": 60,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tool:dify_core",
          "label": "Dify Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 4,
          "mentionCount30d": 11,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi_core",
          "label": "ARC-AGI Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 2,
          "mentionCount30d": 9,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "tool:mlflow_mini",
          "label": "MLflow Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 17,
          "mentionCount30d": 29,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:langchain_edge",
          "label": "LangChain Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 7,
          "mentionCount30d": 42,
          "velocity": 0.09
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_v2",
          "label": "Red Teaming v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-11-30",
          "mentionCount7d": 2,
          "mentionCount30d": 3,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_max",
          "label": "Speculative Decoding Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 16,
          "mentionCount30d": 30,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "tech:rlhf_max",
          "label": "RLHF Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2025-12-24",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_edge",
          "label": "HellaSwag Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 25,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "model:grok_3_lite",
          "label": "Grok-3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-11-29",
          "mentionCount7d": 16,
          "mentionCount30d": 54,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "tool:cursor_mini",
          "label": "Cursor Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 0,
          "mentionCount30d": 5,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:mlflow",
          "label": "MLflow",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 1,
          "mentionCount30d": 2,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention",
          "label": "Group Query Attention",
          "type": "Tech",
          "aliases": [
            "GQA"
          ],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-11-03",
          "mentionCount7d": 15,
          "mentionCount30d": 51,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_core",
          "label": "DeepSeek-V3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 5,
          "mentionCount30d": 9,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "tech:qlora",
          "label": "QLoRA",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 7,
          "mentionCount30d": 44,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "tool:streamlit_mini",
          "label": "Streamlit Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 12,
          "mentionCount30d": 16,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2_lite",
          "label": "OpenWebText2 Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 4,
          "mentionCount30d": 12,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "tool:langchain_lite",
          "label": "LangChain Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 2,
          "mentionCount30d": 15,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_edge",
          "label": "Weights & Biases Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:grok_3",
          "label": "Grok-3",
          "type": "Model",
          "aliases": [
            "Grok 3"
          ],
          "firstSeen": "2025-12-11",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_pro",
          "label": "RefinedWeb Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 10,
          "mentionCount30d": 28,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "tech:dpo_plus",
          "label": "DPO Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 12,
          "mentionCount30d": 29,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention",
          "label": "Sparse Attention",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 6,
          "mentionCount30d": 14,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "benchmark:mbpp_next",
          "label": "MBPP Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 6,
          "mentionCount30d": 35,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_next",
          "label": "Wikipedia Dump 2025 Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 6,
          "mentionCount30d": 19,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_max",
          "label": "The Stack v2 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-06",
          "mentionCount7d": 5,
          "mentionCount30d": 34,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_ultra",
          "label": "Rotary Position Embedding Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 59,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_plus",
          "label": "AlpacaEval 2 Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 21,
          "mentionCount30d": 22,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench_plus",
          "label": "SWE-bench Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 18,
          "mentionCount30d": 36,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "dataset:mc4",
          "label": "mC4",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 13,
          "mentionCount30d": 19,
          "velocity": 0.15
        }
      },
      {
        "data": {
          "id": "tool:crewai_next",
          "label": "CrewAI Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 7,
          "mentionCount30d": 17,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_v2",
          "label": "Attention Is All You Need v2 v2",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 9,
          "mentionCount30d": 19,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "repo:open_interpreter_pro",
          "label": "open-interpreter Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 6,
          "mentionCount30d": 19,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_lite",
          "label": "Claude Opus 4.5 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-22",
          "mentionCount7d": 16,
          "mentionCount30d": 72,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "dataset:roots_edge",
          "label": "ROOTS Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 18,
          "mentionCount30d": 43,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "tool:copilot_max",
          "label": "Copilot Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 5,
          "mentionCount30d": 28,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "tool:langchain_next",
          "label": "LangChain Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-12-08",
          "mentionCount7d": 3,
          "mentionCount30d": 13,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention_pro",
          "label": "Sparse Attention Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 16,
          "mentionCount30d": 21,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tool:copilot",
          "label": "Copilot",
          "type": "Tool",
          "aliases": [
            "GitHub Copilot"
          ],
          "firstSeen": "2025-11-27",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 2,
          "mentionCount30d": 21,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench_next",
          "label": "SWE-bench Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_next",
          "label": "Midjourney V7 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-23",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 1,
          "mentionCount30d": 13,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation",
          "label": "Synthetic Data Generation",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 4,
          "mentionCount30d": 13,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "model:sora_2_ultra",
          "label": "Sora 2 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 5,
          "mentionCount30d": 24,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_lite",
          "label": "DeepSeek-V3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 5,
          "mentionCount30d": 22,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "benchmark:math_next",
          "label": "MATH Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 9,
          "mentionCount30d": 20,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "tool:dify",
          "label": "Dify",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 6,
          "mentionCount30d": 20,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization_pro",
          "label": "Direct Preference Optimization Pro",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 1,
          "mentionCount30d": 8,
          "velocity": 0.98
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench_lite",
          "label": "SWE-bench Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-11-25",
          "mentionCount7d": 20,
          "mentionCount30d": 27,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_mini",
          "label": "MMLU Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.06
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b_v2",
          "label": "LAION-5B v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 2,
          "mentionCount30d": 14,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "tool:flowise_next",
          "label": "Flowise Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-09",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 3,
          "mentionCount30d": 22,
          "velocity": 0.1
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_mini",
          "label": "Chain-of-Thought Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 23,
          "mentionCount30d": 72,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tool:vllm",
          "label": "vLLM",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "model:palm_3_v2",
          "label": "PaLM 3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 13,
          "mentionCount30d": 96,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "tech:tool_use_v2",
          "label": "Tool Use v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 8,
          "mentionCount30d": 20,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_core",
          "label": "Gemini Ultra 2 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 26,
          "mentionCount30d": 77,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "model:falcon_3_lite",
          "label": "Falcon 3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 12,
          "mentionCount30d": 70,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_lite",
          "label": "Command R+ Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 11,
          "mentionCount30d": 33,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa_max",
          "label": "GPQA Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 13,
          "mentionCount30d": 36,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "benchmark:mbpp",
          "label": "MBPP",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 2,
          "mentionCount30d": 4,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_mini",
          "label": "AlpacaEval 2 Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 2,
          "mentionCount30d": 4,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_core",
          "label": "KV Cache Optimization Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 5,
          "mentionCount30d": 15,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:gemma_3_edge",
          "label": "Gemma 3 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 11,
          "mentionCount30d": 45,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tech:lora_edge",
          "label": "LoRA Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "model:aya_3_max",
          "label": "Aya 3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-11-13",
          "mentionCount7d": 35,
          "mentionCount30d": 86,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tool:ollama_plus",
          "label": "Ollama Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 11,
          "mentionCount30d": 25,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "model:sora_2_core",
          "label": "Sora 2 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 23,
          "mentionCount30d": 29,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "model:llama_4_ultra",
          "label": "Llama 4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 32,
          "mentionCount30d": 63,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "tool:langchain_core",
          "label": "LangChain Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 19,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_lite",
          "label": "Multimodal Fusion Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 34,
          "mentionCount30d": 53,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "tool:litellm_mini",
          "label": "LiteLLM Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 2,
          "mentionCount30d": 4,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_max",
          "label": "BigBench Hard Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:litellm_pro",
          "label": "LiteLLM Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 11,
          "mentionCount30d": 28,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_next",
          "label": "Stable Diffusion 4 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 41,
          "mentionCount30d": 111,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_lite",
          "label": "Transformer Architecture Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 30,
          "mentionCount30d": 49,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi_max",
          "label": "ARC-AGI Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tech:gguf_pro",
          "label": "GGUF Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-23",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 19,
          "mentionCount30d": 26,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "tool:litellm_ultra",
          "label": "LiteLLM Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 12,
          "mentionCount30d": 17,
          "velocity": 0.92
        }
      },
      {
        "data": {
          "id": "model:codex_2",
          "label": "Codex 2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 14,
          "mentionCount30d": 34,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_plus",
          "label": "Gemini Ultra 2 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 17,
          "mentionCount30d": 42,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "model:falcon_3",
          "label": "Falcon 3",
          "type": "Model",
          "aliases": [
            "Falcon3"
          ],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 28,
          "mentionCount30d": 63,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_plus",
          "label": "GSM8K Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 19,
          "mentionCount30d": 35,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_mini",
          "label": "Constitutional AI Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 10,
          "mentionCount30d": 18,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_plus",
          "label": "Claude Opus 4.5 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 17,
          "mentionCount30d": 26,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "benchmark:math_core",
          "label": "MATH Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 12,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa_lite",
          "label": "GPQA Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 10,
          "mentionCount30d": 19,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention_mini",
          "label": "Group Query Attention Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-02",
          "mentionCount7d": 18,
          "mentionCount30d": 39,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_v2",
          "label": "LlamaIndex v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 3,
          "mentionCount30d": 25,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "model:gpt_5_plus",
          "label": "GPT-5 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 46,
          "mentionCount30d": 68,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_v2",
          "label": "Nemotron-5 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-06",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 50,
          "mentionCount30d": 63,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "repo:vllm_max",
          "label": "vllm Max",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-10",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 18,
          "mentionCount30d": 49,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "benchmark:math_pro",
          "label": "MATH Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 7,
          "mentionCount30d": 31,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_pro",
          "label": "Rotary Position Embedding Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 21,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_mini",
          "label": "HumanEval Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 9,
          "mentionCount30d": 32,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_ultra",
          "label": "RefinedWeb Ultra",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_v2",
          "label": "Constitutional AI v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 16,
          "mentionCount30d": 39,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "tech:distillation_mini",
          "label": "Distillation Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-12-18",
          "mentionCount7d": 13,
          "mentionCount30d": 55,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa",
          "label": "TruthfulQA",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025",
          "label": "Wikipedia Dump 2025",
          "type": "Dataset",
          "aliases": [
            "Wikipedia"
          ],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 13,
          "mentionCount30d": 24,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_max",
          "label": "FineWeb Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 4,
          "mentionCount30d": 29,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_mini",
          "label": "TensorRT-LLM Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 17,
          "mentionCount30d": 41,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_edge",
          "label": "DALL-E 4 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-15",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 3,
          "mentionCount30d": 10,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_core",
          "label": "Transformer Architecture Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 6,
          "mentionCount30d": 9,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "tool:langchain",
          "label": "LangChain",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 10,
          "mentionCount30d": 30,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:gpt_5_v2",
          "label": "GPT-5 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 37,
          "mentionCount30d": 140,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention_plus",
          "label": "Group Query Attention Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 16,
          "mentionCount30d": 32,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_edge",
          "label": "RedPajama v2 Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 2,
          "mentionCount30d": 9,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_lite",
          "label": "Tree of Thought Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 18,
          "mentionCount30d": 69,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "paper:llm_agents:_a_survey_next",
          "label": "LLM Agents: A Survey Next",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 21,
          "mentionCount30d": 46,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "tech:tokenizer_bpe_edge",
          "label": "Tokenizer BPE Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 48,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_mini",
          "label": "KV Cache Optimization Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_ultra",
          "label": "TensorRT-LLM Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-12-02",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_v2",
          "label": "AlpacaEval 2 v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 2,
          "mentionCount30d": 18,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_mini",
          "label": "Semantic Kernel Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 13,
          "mentionCount30d": 23,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_pro",
          "label": "Constitutional AI Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 21,
          "mentionCount30d": 26,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_ultra",
          "label": "DeepSeek-V3 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 16,
          "mentionCount30d": 43,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "repo:ollama_core",
          "label": "ollama Core",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 2,
          "mentionCount30d": 9,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "model:falcon_3_plus",
          "label": "Falcon 3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2025-12-16",
          "mentionCount7d": 8,
          "mentionCount30d": 28,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2",
          "label": "Attention Is All You Need v2",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "repo:localai_plus",
          "label": "LocalAI Plus",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2025-12-15",
          "mentionCount7d": 1,
          "mentionCount30d": 2,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_lite",
          "label": "Constitutional AI Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 2,
          "mentionCount30d": 17,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande_v2",
          "label": "WinoGrande v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 6,
          "mentionCount30d": 46,
          "velocity": 0.09
        }
      },
      {
        "data": {
          "id": "model:sora_2",
          "label": "Sora 2",
          "type": "Model",
          "aliases": [
            "Sora V2"
          ],
          "firstSeen": "2025-12-14",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 10,
          "mentionCount30d": 58,
          "velocity": 0.16
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_next",
          "label": "Constitutional AI Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 11,
          "mentionCount30d": 39,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "repo:gpt4all",
          "label": "gpt4all",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 5,
          "mentionCount30d": 20,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "tech:gguf_mini",
          "label": "GGUF Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 3,
          "mentionCount30d": 4,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_pro",
          "label": "KV Cache Optimization Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 6,
          "mentionCount30d": 17,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_v2",
          "label": "Claude Sonnet 4 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-12",
          "mentionCount7d": 6,
          "mentionCount30d": 41,
          "velocity": 0.06
        }
      },
      {
        "data": {
          "id": "tech:quantization_mini",
          "label": "Quantization Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 22,
          "mentionCount30d": 54,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_next",
          "label": "HellaSwag Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "paper:textbooks_are_all_you_need",
          "label": "Textbooks Are All You Need",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 6,
          "mentionCount30d": 16,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_core",
          "label": "RedPajama v2 Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 5,
          "mentionCount30d": 53,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "model:gemma_3_max",
          "label": "Gemma 3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-13",
          "mentionCount7d": 39,
          "mentionCount30d": 55,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2",
          "label": "The Stack v2",
          "type": "Dataset",
          "aliases": [
            "The Stack"
          ],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-10",
          "mentionCount7d": 12,
          "mentionCount30d": 25,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_max",
          "label": "Midjourney V7 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 26,
          "mentionCount30d": 71,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "model:llama_4_v2",
          "label": "Llama 4 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 20,
          "mentionCount30d": 64,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:qwen_3_mini",
          "label": "Qwen-3 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 6,
          "mentionCount30d": 58,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "tech:qlora_ultra",
          "label": "QLoRA Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 4,
          "mentionCount30d": 8,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa_max",
          "label": "TruthfulQA Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:flowise_edge",
          "label": "Flowise Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 1,
          "mentionCount30d": 10,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_pro",
          "label": "Weights & Biases Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-02",
          "lastSeen": "2025-12-02",
          "mentionCount7d": 7,
          "mentionCount30d": 10,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "repo:vllm",
          "label": "vllm",
          "type": "Repo",
          "aliases": [
            "vllm-project/vllm"
          ],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 19,
          "mentionCount30d": 23,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation_plus",
          "label": "Retrieval-Augmented Generation Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 10,
          "mentionCount30d": 44,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:gpt_5",
          "label": "GPT-5",
          "type": "Model",
          "aliases": [
            "GPT 5",
            "gpt-5"
          ],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 8,
          "mentionCount30d": 17,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa_ultra",
          "label": "GPQA Ultra",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 18,
          "mentionCount30d": 26,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "model:yi_large_ultra",
          "label": "Yi-Large Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 26,
          "mentionCount30d": 45,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:jamba_2_next",
          "label": "Jamba 2 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-16",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 4,
          "mentionCount30d": 23,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "paper:lora:_low_rank_adaptation_of_large_langu",
          "label": "LoRA: Low-Rank Adaptation of Large Language Models",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 10,
          "mentionCount30d": 20,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_core",
          "label": "Nemotron-5 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 13,
          "mentionCount30d": 27,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "tool:streamlit_edge",
          "label": "Streamlit Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-23",
          "lastSeen": "2025-11-30",
          "mentionCount7d": 9,
          "mentionCount30d": 55,
          "velocity": 0.92
        }
      },
      {
        "data": {
          "id": "repo:gpt4all_v2",
          "label": "gpt4all v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 19,
          "mentionCount30d": 50,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "tech:qlora_v2",
          "label": "QLoRA v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 13,
          "mentionCount30d": 60,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "tool:copilot_next",
          "label": "Copilot Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:transformers_edge",
          "label": "transformers Edge",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 9,
          "mentionCount30d": 37,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench_edge",
          "label": "MT-Bench Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 5,
          "mentionCount30d": 35,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "tech:lora_next",
          "label": "LoRA Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 1,
          "mentionCount30d": 7,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "model:palm_3_core",
          "label": "PaLM 3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 6,
          "mentionCount30d": 33,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention_next",
          "label": "Sparse Attention Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 9,
          "mentionCount30d": 29,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_core",
          "label": "AlpacaEval 2 Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-11-28",
          "mentionCount7d": 2,
          "mentionCount30d": 15,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "model:qwen_3_ultra",
          "label": "Qwen-3 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 5,
          "mentionCount30d": 16,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "repo:llamacpp",
          "label": "llama.cpp",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 23,
          "mentionCount30d": 32,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_edge",
          "label": "Whisper v4 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 8,
          "mentionCount30d": 78,
          "velocity": 0.1
        }
      },
      {
        "data": {
          "id": "tech:lora_core",
          "label": "LoRA Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 4,
          "mentionCount30d": 22,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "model:falcon_3_pro",
          "label": "Falcon 3 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 10,
          "mentionCount30d": 34,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "tool:litellm",
          "label": "LiteLLM",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-11-23",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation_core",
          "label": "Retrieval-Augmented Generation Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 7,
          "mentionCount30d": 57,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_v2",
          "label": "KV Cache Optimization v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 10,
          "mentionCount30d": 28,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_ultra",
          "label": "Mixtral 8x22B Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-11-09",
          "mentionCount7d": 3,
          "mentionCount30d": 23,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding",
          "label": "Rotary Position Embedding",
          "type": "Tech",
          "aliases": [
            "RoPE"
          ],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 7,
          "mentionCount30d": 27,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_v2",
          "label": "Weights & Biases v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 1,
          "mentionCount30d": 2,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_v2",
          "label": "GPT-4o Mini 2 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 18,
          "mentionCount30d": 39,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_next",
          "label": "Nemotron-5 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 54,
          "mentionCount30d": 77,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization_next",
          "label": "Direct Preference Optimization Next",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 3,
          "mentionCount30d": 18,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_plus",
          "label": "The Stack v2 Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-23",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 1,
          "mentionCount30d": 1,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_ultra",
          "label": "MMLU Ultra",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 5,
          "mentionCount30d": 11,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tech:dpo_core",
          "label": "DPO Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 9,
          "mentionCount30d": 44,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "model:grok_3_core",
          "label": "Grok-3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 12,
          "mentionCount30d": 74,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "dataset:dolma_pro",
          "label": "Dolma Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 11,
          "mentionCount30d": 34,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_max",
          "label": "Claude Opus 4.5 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 39,
          "mentionCount30d": 62,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "model:llama_4_pro",
          "label": "Llama 4 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-07",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 26,
          "mentionCount30d": 97,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "tool:vllm_plus",
          "label": "vLLM Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 5,
          "mentionCount30d": 44,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_edge",
          "label": "Claude Opus 4.5 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 9,
          "mentionCount30d": 13,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "tool:localai_edge",
          "label": "LocalAI Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-09",
          "lastSeen": "2025-12-12",
          "mentionCount7d": 12,
          "mentionCount30d": 22,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench",
          "label": "MT-Bench",
          "type": "Benchmark",
          "aliases": [
            "MTBench"
          ],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 9,
          "mentionCount30d": 37,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "repo:llamacpp_plus",
          "label": "llama.cpp Plus",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 7,
          "mentionCount30d": 43,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "tech:quantization_v2",
          "label": "Quantization v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-21",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_plus",
          "label": "Nemotron-5 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-27",
          "lastSeen": "2025-11-12",
          "mentionCount7d": 28,
          "mentionCount30d": 95,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench_plus",
          "label": "MT-Bench Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 1,
          "mentionCount30d": 9,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "dataset:dolma_core",
          "label": "Dolma Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-09",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 4,
          "mentionCount30d": 37,
          "velocity": 0.35
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_max",
          "label": "Command R+ Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 14,
          "mentionCount30d": 23,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_ultra",
          "label": "Multimodal Fusion Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "model:sora_2_pro",
          "label": "Sora 2 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 13,
          "mentionCount30d": 71,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_edge",
          "label": "MMLU Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-11",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "model:codex_2_edge",
          "label": "Codex 2 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 27,
          "mentionCount30d": 36,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_core",
          "label": "Speculative Decoding Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 5,
          "mentionCount30d": 43,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_v2",
          "label": "SlimPajama v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-08",
          "mentionCount7d": 3,
          "mentionCount30d": 6,
          "velocity": 0.15
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench_v2",
          "label": "MT-Bench v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-07",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 7,
          "mentionCount30d": 42,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_mini",
          "label": "RefinedWeb Mini",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 6,
          "mentionCount30d": 14,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "repo:langchain_max",
          "label": "langchain Max",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 12,
          "mentionCount30d": 44,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_next",
          "label": "Weights & Biases Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 1,
          "mentionCount30d": 9,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_ultra",
          "label": "DALL-E 4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 15,
          "mentionCount30d": 46,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tool:vllm_lite",
          "label": "vLLM Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 3,
          "mentionCount30d": 18,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "repo:gpt4all_pro",
          "label": "gpt4all Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 4,
          "mentionCount30d": 20,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_plus",
          "label": "DALL-E 4 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 37,
          "mentionCount30d": 52,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_next",
          "label": "FineWeb Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_edge",
          "label": "The Stack v2 Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 9,
          "mentionCount30d": 37,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "tool:streamlit_next",
          "label": "Streamlit Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 10,
          "mentionCount30d": 24,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_mini",
          "label": "Claude Sonnet 4 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 36,
          "mentionCount30d": 66,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_ultra",
          "label": "Attention Is All You Need v2 Ultra",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 1,
          "mentionCount30d": 11,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa_mini",
          "label": "TruthfulQA Mini",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 12,
          "mentionCount30d": 21,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_edge",
          "label": "Midjourney V7 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 4,
          "mentionCount30d": 33,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "tool:haystack",
          "label": "Haystack",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 16,
          "mentionCount30d": 25,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_pro",
          "label": "Command R+ Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 15,
          "mentionCount30d": 66,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "tool:crewai",
          "label": "CrewAI",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-07",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_lite",
          "label": "Midjourney V7 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 5,
          "mentionCount30d": 7,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "model:qwen_3_max",
          "label": "Qwen-3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 14,
          "mentionCount30d": 20,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_max",
          "label": "Constitutional AI Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 10,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b_core",
          "label": "LAION-5B Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 11,
          "mentionCount30d": 32,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "repo:vllm_edge",
          "label": "vllm Edge",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tool:copilot_plus",
          "label": "Copilot Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-28",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 12,
          "mentionCount30d": 20,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "dataset:mc4_plus",
          "label": "mC4 Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 8,
          "mentionCount30d": 26,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "dataset:dolma_v2",
          "label": "Dolma v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 1,
          "mentionCount30d": 15,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag",
          "label": "HellaSwag",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_lite",
          "label": "Stable Diffusion 4 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 9,
          "mentionCount30d": 56,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data_core",
          "label": "StarCoder Data Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 6,
          "mentionCount30d": 24,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_lite",
          "label": "RefinedWeb Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 4,
          "mentionCount30d": 8,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "model:whisper_v4",
          "label": "Whisper v4",
          "type": "Model",
          "aliases": [
            "Whisper V4"
          ],
          "firstSeen": "2025-11-25",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 7,
          "mentionCount30d": 31,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "dataset:the_pile_next",
          "label": "The Pile Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 5,
          "mentionCount30d": 43,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "tool:autogpt_next",
          "label": "AutoGPT Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2025-11-29",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "model:yi_large",
          "label": "Yi-Large",
          "type": "Model",
          "aliases": [
            "Yi Large"
          ],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 10,
          "mentionCount30d": 19,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "repo:langchain",
          "label": "langchain",
          "type": "Repo",
          "aliases": [
            "langchain-ai/langchain"
          ],
          "firstSeen": "2025-12-19",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 5,
          "mentionCount30d": 23,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_ultra",
          "label": "Chain-of-Thought Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-11-21",
          "mentionCount7d": 9,
          "mentionCount30d": 25,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_max",
          "label": "Sliding Window Attention Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 18,
          "mentionCount30d": 43,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_core",
          "label": "The Stack v2 Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 13,
          "mentionCount30d": 15,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_v2",
          "label": "Command R+ v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 8,
          "mentionCount30d": 13,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench_next",
          "label": "MT-Bench Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.22
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_v2",
          "label": "Stable Diffusion 4 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 14,
          "mentionCount30d": 90,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_mini",
          "label": "Transformer Architecture Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 10,
          "mentionCount30d": 71,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:ollama_ultra",
          "label": "ollama Ultra",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 13,
          "mentionCount30d": 30,
          "velocity": 0.15
        }
      },
      {
        "data": {
          "id": "model:falcon_3_core",
          "label": "Falcon 3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-11-10",
          "mentionCount7d": 11,
          "mentionCount30d": 22,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "model:aya_3_plus",
          "label": "Aya 3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 22,
          "mentionCount30d": 61,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "tool:langchain_v2",
          "label": "LangChain v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2_max",
          "label": "OpenWebText2 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 2,
          "mentionCount30d": 23,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_pro",
          "label": "Whisper v4 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-11-01",
          "mentionCount7d": 9,
          "mentionCount30d": 28,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "model:yi_large_lite",
          "label": "Yi-Large Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 10,
          "mentionCount30d": 46,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "benchmark:mbpp_v2",
          "label": "MBPP v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 4,
          "mentionCount30d": 8,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_max",
          "label": "Synthetic Data Generation Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 9,
          "mentionCount30d": 45,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation_ultra",
          "label": "Retrieval-Augmented Generation Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 12,
          "mentionCount30d": 63,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_max",
          "label": "DeepSeek-V3 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 26,
          "mentionCount30d": 93,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi_edge",
          "label": "ARC-AGI Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl_mini",
          "label": "Common Crawl Mini",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 19,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "tech:qlora_pro",
          "label": "QLoRA Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 3,
          "mentionCount30d": 17,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought",
          "label": "Chain-of-Thought",
          "type": "Tech",
          "aliases": [
            "CoT"
          ],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 2,
          "mentionCount30d": 4,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi",
          "label": "ARC-AGI",
          "type": "Benchmark",
          "aliases": [
            "ARC AGI"
          ],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.25
        }
      },
      {
        "data": {
          "id": "model:jamba_2_lite",
          "label": "Jamba 2 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 21,
          "mentionCount30d": 82,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "dataset:fineweb_pro",
          "label": "FineWeb Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 3,
          "mentionCount30d": 17,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl",
          "label": "Common Crawl",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2025-12-12",
          "mentionCount7d": 1,
          "mentionCount30d": 15,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "dataset:c4_edge",
          "label": "C4 Edge",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-22",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 3,
          "mentionCount30d": 8,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4",
          "label": "Claude Sonnet 4",
          "type": "Model",
          "aliases": [
            "Sonnet 4"
          ],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 41,
          "mentionCount30d": 80,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "repo:vllm_v2",
          "label": "vllm v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 7,
          "mentionCount30d": 19,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "repo:llamacpp_ultra",
          "label": "llama.cpp Ultra",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-25",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 8,
          "mentionCount30d": 27,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_mini",
          "label": "Command R+ Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 43,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2",
          "label": "OpenWebText2",
          "type": "Dataset",
          "aliases": [
            "OWT2"
          ],
          "firstSeen": "2025-11-25",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 9,
          "mentionCount30d": 24,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2",
          "label": "AlpacaEval 2",
          "type": "Benchmark",
          "aliases": [
            "AlpacaEval"
          ],
          "firstSeen": "2025-10-29",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "tool:mlflow_lite",
          "label": "MLflow Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_next",
          "label": "Attention Is All You Need v2 Next",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 12,
          "mentionCount30d": 36,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_ultra",
          "label": "LlamaIndex Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 3,
          "mentionCount30d": 19,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "benchmark:mbpp_core",
          "label": "MBPP Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 9,
          "mentionCount30d": 30,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval",
          "label": "HumanEval",
          "type": "Benchmark",
          "aliases": [
            "Human Eval"
          ],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 5,
          "mentionCount30d": 27,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_mini",
          "label": "Synthetic Data Generation Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 7,
          "mentionCount30d": 14,
          "velocity": 0.09
        }
      },
      {
        "data": {
          "id": "tech:distillation",
          "label": "Distillation",
          "type": "Tech",
          "aliases": [
            "Knowledge Distillation"
          ],
          "firstSeen": "2026-01-15",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 15,
          "mentionCount30d": 47,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_core",
          "label": "Weights & Biases Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 4,
          "mentionCount30d": 18,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "model:phi_4_ultra",
          "label": "Phi-4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 18,
          "mentionCount30d": 92,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "benchmark:math_edge",
          "label": "MATH Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 6,
          "mentionCount30d": 19,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "tool:litellm_plus",
          "label": "LiteLLM Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 5,
          "mentionCount30d": 7,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_lite",
          "label": "Rotary Position Embedding Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 13,
          "mentionCount30d": 60,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_edge",
          "label": "GSM8K Edge",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-22",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 8,
          "mentionCount30d": 18,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena_lite",
          "label": "LMSYS Chatbot Arena Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-11-10",
          "mentionCount7d": 4,
          "mentionCount30d": 27,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tool:crewai_edge",
          "label": "CrewAI Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 8,
          "mentionCount30d": 23,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "repo:transformers_pro",
          "label": "transformers Pro",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 7,
          "mentionCount30d": 25,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:dall_e_4",
          "label": "DALL-E 4",
          "type": "Model",
          "aliases": [
            "DALLE 4"
          ],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 7,
          "mentionCount30d": 70,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "label": "Tree of Thoughts: Deliberate Problem Solving with LLMs",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-05",
          "lastSeen": "2025-12-17",
          "mentionCount7d": 2,
          "mentionCount30d": 11,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "model:jamba_2_ultra",
          "label": "Jamba 2 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 43,
          "mentionCount30d": 57,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "dataset:roots_max",
          "label": "ROOTS Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 3,
          "mentionCount30d": 18,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_max",
          "label": "Chain-of-Thought Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 7,
          "mentionCount30d": 30,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa_plus",
          "label": "GPQA Plus",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 12,
          "mentionCount30d": 26,
          "velocity": 0.16
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_next",
          "label": "Transformer Architecture Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 29,
          "mentionCount30d": 40,
          "velocity": 0.98
        }
      },
      {
        "data": {
          "id": "paper:textbooks_are_all_you_need_next",
          "label": "Textbooks Are All You Need Next",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 14,
          "mentionCount30d": 41,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture_plus",
          "label": "Transformer Architecture Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 16,
          "mentionCount30d": 59,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7",
          "label": "Midjourney V7",
          "type": "Model",
          "aliases": [
            "MJ V7"
          ],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 46,
          "mentionCount30d": 79,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi_ultra",
          "label": "ARC-AGI Ultra",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_v2",
          "label": "Wikipedia Dump 2025 v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-11-23",
          "mentionCount7d": 10,
          "mentionCount30d": 21,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "model:aya_3_next",
          "label": "Aya 3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-18",
          "mentionCount7d": 3,
          "mentionCount30d": 25,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "dataset:mc4_v2",
          "label": "mC4 v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 1,
          "mentionCount30d": 16,
          "velocity": 0.87
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_v2",
          "label": "Semantic Kernel v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-12-11",
          "mentionCount7d": 5,
          "mentionCount30d": 11,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "model:phi_4",
          "label": "Phi-4",
          "type": "Model",
          "aliases": [
            "Phi 4"
          ],
          "firstSeen": "2025-12-10",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 10,
          "mentionCount30d": 36,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_pro",
          "label": "HellaSwag Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-11-20",
          "mentionCount7d": 5,
          "mentionCount30d": 18,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "model:yi_large_pro",
          "label": "Yi-Large Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 15,
          "mentionCount30d": 53,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "model:command_r_plus",
          "label": "Command R+",
          "type": "Model",
          "aliases": [
            "Command R Plus"
          ],
          "firstSeen": "2025-11-14",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 15,
          "mentionCount30d": 96,
          "velocity": 0.18
        }
      },
      {
        "data": {
          "id": "paper:llm_agents:_a_survey_core",
          "label": "LLM Agents: A Survey Core",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 18,
          "mentionCount30d": 52,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "tech:quantization_edge",
          "label": "Quantization Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 4,
          "mentionCount30d": 49,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:langchain_plus",
          "label": "LangChain Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 1,
          "mentionCount30d": 7,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "tool:haystack_next",
          "label": "Haystack Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 12,
          "mentionCount30d": 29,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_next",
          "label": "Mixture of Experts Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 14,
          "mentionCount30d": 31,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_v2",
          "label": "Whisper v4 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 10,
          "mentionCount30d": 21,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:litellm_v2",
          "label": "LiteLLM v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 9,
          "mentionCount30d": 25,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_next",
          "label": "HumanEval Next",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-10-28",
          "mentionCount7d": 6,
          "mentionCount30d": 37,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tool:gradio_edge",
          "label": "Gradio Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 5,
          "mentionCount30d": 36,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_ultra",
          "label": "Wikipedia Dump 2025 Ultra",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 4,
          "mentionCount30d": 7,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "model:sora_2_mini",
          "label": "Sora 2 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 7,
          "mentionCount30d": 16,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "model:gemma_3_v2",
          "label": "Gemma 3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 21,
          "mentionCount30d": 47,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tool:autogpt",
          "label": "AutoGPT",
          "type": "Tool",
          "aliases": [
            "Auto-GPT"
          ],
          "firstSeen": "2025-12-24",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 4,
          "mentionCount30d": 35,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "model:palm_3_next",
          "label": "PaLM 3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 11,
          "mentionCount30d": 41,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases_plus",
          "label": "Weights & Biases Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 15,
          "mentionCount30d": 31,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "model:qwen_3_plus",
          "label": "Qwen-3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-23",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 11,
          "mentionCount30d": 66,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tech:flash_attention",
          "label": "Flash Attention",
          "type": "Tech",
          "aliases": [
            "FlashAttention"
          ],
          "firstSeen": "2025-10-31",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 1,
          "mentionCount30d": 9,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "dataset:the_pile_plus",
          "label": "The Pile Plus",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "model:sora_2_edge",
          "label": "Sora 2 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 27,
          "mentionCount30d": 83,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tool:vllm_pro",
          "label": "vLLM Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 7,
          "mentionCount30d": 10,
          "velocity": 0.99
        }
      },
      {
        "data": {
          "id": "model:jamba_2_core",
          "label": "Jamba 2 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 11,
          "mentionCount30d": 75,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "model:dall_e_4_max",
          "label": "DALL-E 4 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2025-12-16",
          "mentionCount7d": 3,
          "mentionCount30d": 8,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tool:cody_edge",
          "label": "Cody Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-10",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 9,
          "mentionCount30d": 49,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_core",
          "label": "BigBench Hard Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 20,
          "mentionCount30d": 35,
          "velocity": 0.28
        }
      },
      {
        "data": {
          "id": "tech:quantization_max",
          "label": "Quantization Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 4,
          "mentionCount30d": 12,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "repo:localai",
          "label": "LocalAI",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2025-12-19",
          "mentionCount7d": 2,
          "mentionCount30d": 10,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:aya_3_core",
          "label": "Aya 3 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-15",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 23,
          "mentionCount30d": 34,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_edge",
          "label": "Synthetic Data Generation Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 25,
          "mentionCount30d": 72,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "repo:text_generation_webui_max",
          "label": "text-generation-webui Max",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 4,
          "mentionCount30d": 30,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "model:jamba_2_max",
          "label": "Jamba 2 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 30,
          "mentionCount30d": 81,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "dataset:roots_next",
          "label": "ROOTS Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 16,
          "mentionCount30d": 23,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "model:sora_2_plus",
          "label": "Sora 2 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 22,
          "mentionCount30d": 95,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_next",
          "label": "Flash Attention Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 7,
          "mentionCount30d": 20,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "model:codex_2_core",
          "label": "Codex 2 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-11",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 38,
          "mentionCount30d": 90,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "paper:llm_agents:_a_survey_mini",
          "label": "LLM Agents: A Survey Mini",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_ultra",
          "label": "Speculative Decoding Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 25,
          "mentionCount30d": 62,
          "velocity": 0.92
        }
      },
      {
        "data": {
          "id": "model:jamba_2_mini",
          "label": "Jamba 2 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-18",
          "mentionCount7d": 36,
          "mentionCount30d": 108,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "model:llama_4",
          "label": "Llama 4",
          "type": "Model",
          "aliases": [
            "LLaMA 4",
            "Llama-4"
          ],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 5,
          "mentionCount30d": 40,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "model:falcon_3_next",
          "label": "Falcon 3 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 12,
          "mentionCount30d": 52,
          "velocity": 0.06
        }
      },
      {
        "data": {
          "id": "tech:gguf_core",
          "label": "GGUF Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 21,
          "mentionCount30d": 26,
          "velocity": 0.18
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_edge",
          "label": "Flash Attention Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 24,
          "mentionCount30d": 57,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "repo:ollama",
          "label": "ollama",
          "type": "Repo",
          "aliases": [
            "ollama/ollama"
          ],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-11-08",
          "mentionCount7d": 4,
          "mentionCount30d": 19,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b_lite",
          "label": "LAION-5B Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-11-20",
          "mentionCount7d": 6,
          "mentionCount30d": 18,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought",
          "label": "Tree of Thought",
          "type": "Tech",
          "aliases": [
            "ToT"
          ],
          "firstSeen": "2025-12-12",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 1,
          "mentionCount30d": 11,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "tool:flowise_max",
          "label": "Flowise Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 5,
          "mentionCount30d": 13,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_pro",
          "label": "HumanEval Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 14,
          "mentionCount30d": 38,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "dataset:the_pile_core",
          "label": "The Pile Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-11-08",
          "mentionCount7d": 6,
          "mentionCount30d": 35,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tech:tool_use_core",
          "label": "Tool Use Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-12-01",
          "mentionCount7d": 13,
          "mentionCount30d": 41,
          "velocity": 0.19
        }
      },
      {
        "data": {
          "id": "tech:dpo_v2",
          "label": "DPO v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 25,
          "mentionCount30d": 66,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "tool:copilot_edge",
          "label": "Copilot Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 4,
          "mentionCount30d": 13,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_ultra",
          "label": "Mixture of Experts Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 34,
          "mentionCount30d": 47,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench_pro",
          "label": "SWE-bench Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 9,
          "mentionCount30d": 18,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "model:phi_4_mini",
          "label": "Phi-4 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-11-16",
          "mentionCount7d": 24,
          "mentionCount30d": 62,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "tool:gradio_v2",
          "label": "Gradio v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_v2",
          "label": "Rotary Position Embedding v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 7,
          "mentionCount30d": 46,
          "velocity": 0.86
        }
      },
      {
        "data": {
          "id": "repo:vllm_core",
          "label": "vllm Core",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 9,
          "mentionCount30d": 21,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tech:lora_lite",
          "label": "LoRA Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 15,
          "mentionCount30d": 62,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_plus",
          "label": "Tree of Thought Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-23",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 23,
          "mentionCount30d": 74,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k_v2",
          "label": "GSM8K v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 3,
          "mentionCount30d": 28,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "tool:streamlit",
          "label": "Streamlit",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 3,
          "mentionCount30d": 21,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "tool:crewai_max",
          "label": "CrewAI Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 12,
          "mentionCount30d": 20,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_core",
          "label": "LlamaIndex Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 14,
          "mentionCount30d": 26,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "model:aya_3_v2",
          "label": "Aya 3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 4,
          "mentionCount30d": 7,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "model:palm_3_lite",
          "label": "PaLM 3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 19,
          "mentionCount30d": 86,
          "velocity": 0.99
        }
      },
      {
        "data": {
          "id": "repo:langchain_plus",
          "label": "langchain Plus",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 0,
          "mentionCount30d": 7,
          "velocity": 0.22
        }
      },
      {
        "data": {
          "id": "tool:flowise_pro",
          "label": "Flowise Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 6,
          "mentionCount30d": 31,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_plus",
          "label": "LlamaIndex Plus",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-21",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 18,
          "mentionCount30d": 48,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "dataset:the_pile",
          "label": "The Pile",
          "type": "Dataset",
          "aliases": [
            "Pile"
          ],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "model:codex_2_next",
          "label": "Codex 2 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 4,
          "mentionCount30d": 14,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tool:haystack_pro",
          "label": "Haystack Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 14,
          "mentionCount30d": 30,
          "velocity": 0.34
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama_core",
          "label": "SlimPajama Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-31",
          "lastSeen": "2025-12-15",
          "mentionCount7d": 4,
          "mentionCount30d": 28,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_max",
          "label": "Nemotron-5 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-30",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 5,
          "mentionCount30d": 9,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data_lite",
          "label": "StarCoder Data Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-30",
          "lastSeen": "2025-12-16",
          "mentionCount7d": 8,
          "mentionCount30d": 16,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tool:ollama_pro",
          "label": "Ollama Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.78
        }
      },
      {
        "data": {
          "id": "repo:localai_lite",
          "label": "LocalAI Lite",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-27",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 18,
          "mentionCount30d": 54,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "model:llama_4_next",
          "label": "Llama 4 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 19,
          "mentionCount30d": 42,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "tech:distillation_lite",
          "label": "Distillation Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_core",
          "label": "Claude Opus 4.5 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-15",
          "mentionCount7d": 59,
          "mentionCount30d": 108,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tool:gradio",
          "label": "Gradio",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 10,
          "mentionCount30d": 22,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought_plus",
          "label": "Chain-of-Thought Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 11,
          "mentionCount30d": 25,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b_next",
          "label": "LAION-5B Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 2,
          "mentionCount30d": 27,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_max",
          "label": "Flash Attention Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-15",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 11,
          "mentionCount30d": 25,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:sora_2_max",
          "label": "Sora 2 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 6,
          "mentionCount30d": 19,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tech:tool_use_plus",
          "label": "Tool Use Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 6,
          "mentionCount30d": 41,
          "velocity": 0.72
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_edge",
          "label": "Multimodal Fusion Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-02",
          "lastSeen": "2025-12-30",
          "mentionCount7d": 4,
          "mentionCount30d": 24,
          "velocity": 0.16
        }
      },
      {
        "data": {
          "id": "model:aya_3_pro",
          "label": "Aya 3 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 29,
          "mentionCount30d": 106,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_max",
          "label": "KV Cache Optimization Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-07",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 4,
          "mentionCount30d": 19,
          "velocity": 0.07
        }
      },
      {
        "data": {
          "id": "repo:open_interpreter",
          "label": "open-interpreter",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 8,
          "mentionCount30d": 9,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_core",
          "label": "Flash Attention Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 6,
          "mentionCount30d": 13,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "model:llama_4_plus",
          "label": "Llama 4 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 50,
          "mentionCount30d": 92,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "tech:qlora_edge",
          "label": "QLoRA Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-11-13",
          "mentionCount7d": 20,
          "mentionCount30d": 28,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_edge",
          "label": "LlamaIndex Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 10,
          "mentionCount30d": 35,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_v2",
          "label": "Gemini Ultra 2 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2026-01-12",
          "mentionCount7d": 18,
          "mentionCount30d": 94,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "tool:ollama_v2",
          "label": "Ollama v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 6,
          "mentionCount30d": 11,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_max",
          "label": "Rotary Position Embedding Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-09",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 12,
          "mentionCount30d": 43,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "tech:lora_ultra",
          "label": "LoRA Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 21,
          "mentionCount30d": 41,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:lmsys_chatbot_arena",
          "label": "LMSYS Chatbot Arena",
          "type": "Benchmark",
          "aliases": [
            "Chatbot Arena"
          ],
          "firstSeen": "2025-12-28",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 0,
          "mentionCount30d": 5,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:langchain_ultra",
          "label": "langchain Ultra",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 9,
          "mentionCount30d": 18,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tool:autogpt_v2",
          "label": "AutoGPT v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-11-18",
          "mentionCount7d": 2,
          "mentionCount30d": 5,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_pro",
          "label": "Red Teaming Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 3,
          "mentionCount30d": 14,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "paper:constitutional_ai:_harmlessness_from_ai_",
          "label": "Constitutional AI: Harmlessness from AI Feedback",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b_ultra",
          "label": "LAION-5B Ultra",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 15,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion_pro",
          "label": "Multimodal Fusion Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 18,
          "mentionCount30d": 77,
          "velocity": 0.6
        }
      },
      {
        "data": {
          "id": "paper:llm_agents:_a_survey",
          "label": "LLM Agents: A Survey",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 8,
          "mentionCount30d": 17,
          "velocity": 0.75
        }
      },
      {
        "data": {
          "id": "model:llama_4_core",
          "label": "Llama 4 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-06",
          "mentionCount7d": 17,
          "mentionCount30d": 133,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_pro",
          "label": "Claude Sonnet 4 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 6,
          "mentionCount30d": 14,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "benchmark:bigbench_hard_v2",
          "label": "BigBench Hard v2",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 4,
          "mentionCount30d": 8,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "dataset:roots_v2",
          "label": "ROOTS v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 5,
          "mentionCount30d": 12,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl_v2",
          "label": "Common Crawl v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 2,
          "mentionCount30d": 19,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_pro",
          "label": "Synthetic Data Generation Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 7,
          "mentionCount30d": 51,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval_max",
          "label": "HumanEval Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 14,
          "mentionCount30d": 21,
          "velocity": 0.54
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention_mini",
          "label": "Sparse Attention Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 2,
          "mentionCount30d": 4,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_lite",
          "label": "Synthetic Data Generation Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-15",
          "mentionCount7d": 25,
          "mentionCount30d": 40,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "model:jamba_2_v2",
          "label": "Jamba 2 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 14,
          "mentionCount30d": 34,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_ultra",
          "label": "Midjourney V7 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-25",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 8,
          "mentionCount30d": 16,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tech:rlhf_mini",
          "label": "RLHF Mini",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-15",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 11,
          "mentionCount30d": 20,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_ultra",
          "label": "Stable Diffusion 4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 29,
          "mentionCount30d": 38,
          "velocity": 0.73
        }
      },
      {
        "data": {
          "id": "tool:autogpt_ultra",
          "label": "AutoGPT Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 4,
          "mentionCount30d": 17,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tech:distillation_core",
          "label": "Distillation Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 4,
          "mentionCount30d": 33,
          "velocity": 0.12
        }
      },
      {
        "data": {
          "id": "repo:ollama_mini",
          "label": "ollama Mini",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 12,
          "mentionCount30d": 42,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "tech:lora_v2",
          "label": "LoRA v2",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2025-11-16",
          "mentionCount7d": 10,
          "mentionCount30d": 55,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "tool:llamaindex_pro",
          "label": "LlamaIndex Pro",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 6,
          "mentionCount30d": 36,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "repo:localai_v2",
          "label": "LocalAI v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 7,
          "mentionCount30d": 38,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_ultra",
          "label": "Semantic Kernel Ultra",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 20,
          "mentionCount30d": 35,
          "velocity": 0.69
        }
      },
      {
        "data": {
          "id": "tool:ollama_next",
          "label": "Ollama Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "tool:flowise_v2",
          "label": "Flowise v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:grok_3_plus",
          "label": "Grok-3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 42,
          "mentionCount30d": 107,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_max",
          "label": "GPT-4o Mini 2 Max",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-29",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 9,
          "mentionCount30d": 25,
          "velocity": 0.76
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7_core",
          "label": "Midjourney V7 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-09",
          "mentionCount7d": 14,
          "mentionCount30d": 78,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_max",
          "label": "AlpacaEval 2 Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-02",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 18,
          "mentionCount30d": 37,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_edge",
          "label": "TensorRT-LLM Edge",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 19,
          "mentionCount30d": 48,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_max",
          "label": "Mixture of Experts Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 5,
          "mentionCount30d": 14,
          "velocity": 0.21
        }
      },
      {
        "data": {
          "id": "model:phi_4_plus",
          "label": "Phi-4 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-08",
          "lastSeen": "2025-12-01",
          "mentionCount7d": 19,
          "mentionCount30d": 40,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tech:red_teaming_core",
          "label": "Red Teaming Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 8,
          "mentionCount30d": 16,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "repo:autogpt_v2",
          "label": "AutoGPT v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 7,
          "mentionCount30d": 21,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "benchmark:winogrande_ultra",
          "label": "WinoGrande Ultra",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 4,
          "mentionCount30d": 7,
          "velocity": 0.25
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_lite",
          "label": "Sliding Window Attention Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-10",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 24,
          "mentionCount30d": 55,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm_lite",
          "label": "TensorRT-LLM Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-10-27",
          "lastSeen": "2025-11-07",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "repo:gpt4all_core",
          "label": "gpt4all Core",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 1,
          "mentionCount30d": 11,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention_ultra",
          "label": "Sparse Attention Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-22",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 17,
          "mentionCount30d": 50,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "repo:llamacpp_mini",
          "label": "llama.cpp Mini",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 13,
          "mentionCount30d": 48,
          "velocity": 0.91
        }
      },
      {
        "data": {
          "id": "benchmark:alpacaeval_2_lite",
          "label": "AlpacaEval 2 Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-01",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 8,
          "mentionCount30d": 39,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_plus",
          "label": "DeepSeek-V3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 16,
          "mentionCount30d": 49,
          "velocity": 0.64
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl_lite",
          "label": "Common Crawl Lite",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-21",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 1,
          "mentionCount30d": 3,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization",
          "label": "KV Cache Optimization",
          "type": "Tech",
          "aliases": [
            "KV Cache"
          ],
          "firstSeen": "2025-12-16",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 2,
          "mentionCount30d": 27,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4_plus",
          "label": "Stable Diffusion 4 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 30,
          "mentionCount30d": 120,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2_pro",
          "label": "GPT-4o Mini 2 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-06",
          "lastSeen": "2026-01-14",
          "mentionCount7d": 27,
          "mentionCount30d": 51,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization_ultra",
          "label": "KV Cache Optimization Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 7,
          "mentionCount30d": 37,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa_pro",
          "label": "GPQA Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-10",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 17,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench_pro",
          "label": "MT-Bench Pro",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-07",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.88
        }
      },
      {
        "data": {
          "id": "model:palm_3_edge",
          "label": "PaLM 3 Edge",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 49,
          "mentionCount30d": 113,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "model:gemma_3_plus",
          "label": "Gemma 3 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 38,
          "mentionCount30d": 104,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "tech:synthetic_data_generation_plus",
          "label": "Synthetic Data Generation Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 12,
          "mentionCount30d": 19,
          "velocity": 0.11
        }
      },
      {
        "data": {
          "id": "tool:cody_mini",
          "label": "Cody Mini",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 3,
          "mentionCount30d": 34,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding_pro",
          "label": "Speculative Decoding Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-11-28",
          "mentionCount7d": 12,
          "mentionCount30d": 44,
          "velocity": 0.21
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data_pro",
          "label": "StarCoder Data Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 0,
          "mentionCount30d": 2,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture",
          "label": "Transformer Architecture",
          "type": "Tech",
          "aliases": [
            "Transformers"
          ],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 6,
          "mentionCount30d": 16,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "tech:rlhf_plus",
          "label": "RLHF Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 5,
          "mentionCount30d": 17,
          "velocity": 0.96
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b",
          "label": "Mixtral 8x22B",
          "type": "Model",
          "aliases": [
            "Mixtral-8x22B"
          ],
          "firstSeen": "2025-11-21",
          "lastSeen": "2025-11-28",
          "mentionCount7d": 11,
          "mentionCount30d": 24,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_next",
          "label": "Claude Opus 4.5 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 40,
          "mentionCount30d": 62,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention_plus",
          "label": "Sliding Window Attention Plus",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-27",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 8,
          "mentionCount30d": 29,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "tool:streamlit_v2",
          "label": "Streamlit v2",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 8,
          "mentionCount30d": 9,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention_lite",
          "label": "Group Query Attention Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-12",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 18,
          "mentionCount30d": 32,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts_edge",
          "label": "Mixture of Experts Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data",
          "label": "StarCoder Data",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 6,
          "mentionCount30d": 28,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "tool:localai_core",
          "label": "LocalAI Core",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-26",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 5,
          "mentionCount30d": 34,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "model:jamba_2_plus",
          "label": "Jamba 2 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-01",
          "lastSeen": "2025-12-01",
          "mentionCount7d": 6,
          "mentionCount30d": 11,
          "velocity": 0.94
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel",
          "label": "Semantic Kernel",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-11-20",
          "mentionCount7d": 16,
          "mentionCount30d": 49,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2_pro",
          "label": "The Stack v2 Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-29",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 9,
          "mentionCount30d": 33,
          "velocity": 0.29
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_core",
          "label": "Wikipedia Dump 2025 Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-08",
          "mentionCount7d": 0,
          "mentionCount30d": 1,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb",
          "label": "RefinedWeb",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2025-11-21",
          "mentionCount7d": 4,
          "mentionCount30d": 13,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "dataset:openwebtext2_core",
          "label": "OpenWebText2 Core",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 22,
          "mentionCount30d": 27,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_next",
          "label": "Claude Sonnet 4 Next",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-20",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 17,
          "mentionCount30d": 86,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tech:tool_use_max",
          "label": "Tool Use Max",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 4,
          "mentionCount30d": 11,
          "velocity": 0.73
        }
      },
      {
        "data": {
          "id": "paper:textbooks_are_all_you_need_mini",
          "label": "Textbooks Are All You Need Mini",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-10-31",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 6,
          "mentionCount30d": 41,
          "velocity": 0.83
        }
      },
      {
        "data": {
          "id": "repo:autogpt_max",
          "label": "AutoGPT Max",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-05",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 9,
          "mentionCount30d": 19,
          "velocity": 0.73
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel_max",
          "label": "Semantic Kernel Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2026-01-04",
          "mentionCount7d": 12,
          "mentionCount30d": 39,
          "velocity": 0.82
        }
      },
      {
        "data": {
          "id": "model:qwen_3_lite",
          "label": "Qwen-3 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 8,
          "mentionCount30d": 21,
          "velocity": 0.27
        }
      },
      {
        "data": {
          "id": "dataset:starcoder_data_v2",
          "label": "StarCoder Data v2",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-11-28",
          "mentionCount7d": 7,
          "mentionCount30d": 29,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "model:qwen_3_v2",
          "label": "Qwen-3 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-21",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 18,
          "mentionCount30d": 63,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_max",
          "label": "Wikipedia Dump 2025 Max",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 2,
          "mentionCount30d": 18,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_pro",
          "label": "DeepSeek-V3 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 38,
          "mentionCount30d": 76,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts",
          "label": "Mixture of Experts",
          "type": "Tech",
          "aliases": [
            "MoE"
          ],
          "firstSeen": "2025-11-30",
          "lastSeen": "2025-12-08",
          "mentionCount7d": 6,
          "mentionCount30d": 12,
          "velocity": 0.93
        }
      },
      {
        "data": {
          "id": "tech:gguf_edge",
          "label": "GGUF Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-11-05",
          "mentionCount7d": 5,
          "mentionCount30d": 12,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag_max",
          "label": "HellaSwag Max",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-21",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 11,
          "mentionCount30d": 18,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "tool:ollama",
          "label": "Ollama",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-08",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 19,
          "mentionCount30d": 29,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu_core",
          "label": "MMLU Core",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-06",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 16,
          "mentionCount30d": 37,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "tech:quantization_ultra",
          "label": "Quantization Ultra",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 17,
          "mentionCount30d": 57,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "model:nemotron_5_pro",
          "label": "Nemotron-5 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 16,
          "mentionCount30d": 65,
          "velocity": 0.55
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2_mini",
          "label": "Attention Is All You Need v2 Mini",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-25",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 3,
          "mentionCount30d": 27,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "repo:ollama_v2",
          "label": "ollama v2",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2025-12-04",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 7,
          "mentionCount30d": 20,
          "velocity": 0.62
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3_mini",
          "label": "DeepSeek-V3 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-30",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 23,
          "mentionCount30d": 84,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "tool:localai_next",
          "label": "LocalAI Next",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 8,
          "mentionCount30d": 11,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:flash_attention_lite",
          "label": "Flash Attention Lite",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 8,
          "mentionCount30d": 40,
          "velocity": 0.33
        }
      },
      {
        "data": {
          "id": "tool:localai",
          "label": "LocalAI",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2_next",
          "label": "RedPajama v2 Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-08",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 13,
          "mentionCount30d": 48,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "benchmark:swe_bench",
          "label": "SWE-bench",
          "type": "Benchmark",
          "aliases": [
            "SWE Bench"
          ],
          "firstSeen": "2025-11-07",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 16,
          "mentionCount30d": 34,
          "velocity": 0.11
        }
      },
      {
        "data": {
          "id": "model:whisper_v4_ultra",
          "label": "Whisper v4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-14",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 23,
          "mentionCount30d": 40,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "tool:litellm_max",
          "label": "LiteLLM Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-10",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 0,
          "mentionCount30d": 1,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:ollama_lite",
          "label": "Ollama Lite",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-14",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 13,
          "mentionCount30d": 32,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tool:mlflow_max",
          "label": "MLflow Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-23",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 7,
          "mentionCount30d": 16,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "model:codex_2_mini",
          "label": "Codex 2 Mini",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 35,
          "mentionCount30d": 50,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k",
          "label": "GSM8K",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-24",
          "lastSeen": "2025-12-04",
          "mentionCount7d": 4,
          "mentionCount30d": 10,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "dataset:refinedweb_next",
          "label": "RefinedWeb Next",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2025-12-18",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:codex_2_plus",
          "label": "Codex 2 Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-05",
          "lastSeen": "2025-11-26",
          "mentionCount7d": 33,
          "mentionCount30d": 81,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:codex_2_pro",
          "label": "Codex 2 Pro",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 5,
          "mentionCount30d": 21,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "tech:distillation_edge",
          "label": "Distillation Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 19,
          "mentionCount30d": 28,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "tool:llamaindex",
          "label": "LlamaIndex",
          "type": "Tool",
          "aliases": [
            "Llama Index"
          ],
          "firstSeen": "2025-12-02",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 4,
          "mentionCount30d": 38,
          "velocity": 0.11
        }
      },
      {
        "data": {
          "id": "paper:retrieval_augmented_generation_for_knowl",
          "label": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-10",
          "lastSeen": "2025-12-08",
          "mentionCount7d": 4,
          "mentionCount30d": 10,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai_edge",
          "label": "Constitutional AI Edge",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-10-31",
          "mentionCount7d": 7,
          "mentionCount30d": 28,
          "velocity": 0.49
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought_next",
          "label": "Tree of Thought Next",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-12-18",
          "mentionCount7d": 12,
          "mentionCount30d": 33,
          "velocity": 0.1
        }
      },
      {
        "data": {
          "id": "tech:rlhf_pro",
          "label": "RLHF Pro",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-04",
          "lastSeen": "2025-11-17",
          "mentionCount7d": 33,
          "mentionCount30d": 40,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "model:phi_4_lite",
          "label": "Phi-4 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-22",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 43,
          "mentionCount30d": 82,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4",
          "label": "Stable Diffusion 4",
          "type": "Model",
          "aliases": [
            "SD4",
            "SD 4"
          ],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-25",
          "mentionCount7d": 8,
          "mentionCount30d": 18,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b_core",
          "label": "Mixtral 8x22B Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 8,
          "mentionCount30d": 24,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding_core",
          "label": "Rotary Position Embedding Core",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 4,
          "mentionCount30d": 17,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4_ultra",
          "label": "Claude Sonnet 4 Ultra",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 13,
          "mentionCount30d": 42,
          "velocity": 0.31
        }
      },
      {
        "data": {
          "id": "model:command_r_plus_plus",
          "label": "Command R+ Plus",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-18",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 14,
          "mentionCount30d": 52,
          "velocity": 0.42
        }
      },
      {
        "data": {
          "id": "dataset:wikipedia_dump_2025_pro",
          "label": "Wikipedia Dump 2025 Pro",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 10,
          "mentionCount30d": 55,
          "velocity": 0.35
        }
      },
      {
        "data": {
          "id": "tool:streamlit_max",
          "label": "Streamlit Max",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-16",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "benchmark:mbpp_lite",
          "label": "MBPP Lite",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 1,
          "mentionCount30d": 11,
          "velocity": 0.94
        }
      }
    ],
    "edges": [
      {
        "data": {
          "id": "e:claude_sonnet_4_sora_2_edge_depends_on",
          "source": "model:claude_sonnet_4",
          "target": "model:sora_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:vllm_plus_qwen_3_integrates_with",
          "source": "tool:vllm_plus",
          "target": "model:qwen_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_wired_8634",
              "url": "https://wired.com/2026/01/25/vllm_plus_qwen_3",
              "published": "2026-01-25",
              "snippet": "vLLM Plus announced official support for Qwen-3..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_7812",
              "url": "https://ai.meta.com/blog/2026/01/25/vllm_plus_qwen_3",
              "published": "2026-01-25",
              "snippet": "The latest release of vLLM Plus adds native Qwen-3 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_mt_bench_next_evaluated_on",
          "source": "model:stable_diffusion_4_max",
          "target": "benchmark:mt_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-25_nvidia_blog_8854",
              "url": "https://blogs.nvidia.com/2025/12/25/stable_diffusion_4_max_mt_benc",
              "published": "2025-12-25",
              "snippet": "Stable Diffusion 4 Max achieves 90% on MT-Bench Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_core_phi_4_depends_on",
          "source": "model:grok_3_core",
          "target": "model:phi_4",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_the_pile_plus_trained_on",
          "source": "model:claude_opus_45_pro",
          "target": "dataset:the_pile_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-10_arxiv_4837",
              "url": "https://arxiv.org/abs/2026/01/10/claude_opus_45_pro_the_pile_pl",
              "published": "2026-01-10",
              "snippet": "The training corpus for Claude Opus 4.5 Pro includes The Pile Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_mixtral_8x22b_mini_integrates_with",
          "source": "tool:mlflow_v2",
          "target": "model:mixtral_8x22b_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_4154",
              "url": "https://blog.langchain.dev/2026/01/17/mlflow_v2_mixtral_8x22b_mini",
              "published": "2026-01-17",
              "snippet": "MLflow v2 now supports Mixtral 8x22B Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_mixtral_8x22b_next_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:mixtral_8x22b_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:llamaindex_max_midjourney_v7_lite_integrates_with",
          "source": "tool:llamaindex_max",
          "target": "model:midjourney_v7_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-23_microsoft_resea_9475",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/llamaindex_max_midjourney_v7_l",
              "published": "2026-01-23",
              "snippet": "LlamaIndex Max announced official support for Midjourney V7 Lite..."
            },
            {
              "docId": "2026-01-23_openai_blog_7189",
              "url": "https://openai.com/blog/2026/01/23/llamaindex_max_midjourney_v7_l",
              "published": "2026-01-23",
              "snippet": "LlamaIndex Max announced official support for Midjourney V7 Lite..."
            },
            {
              "docId": "2026-01-23_reuters_4036",
              "url": "https://reuters.com/technology/2026/01/23/llamaindex_max_midjourney_v7_l",
              "published": "2026-01-23",
              "snippet": "LlamaIndex Max now supports Midjourney V7 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_qwen_3_ultra_integrates_with",
          "source": "repo:transformers_next",
          "target": "model:qwen_3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-03_the_gradient_9548",
              "url": "https://thegradient.pub/2026/01/03/transformers_next_qwen_3_ultra",
              "published": "2026-01-03",
              "snippet": "The latest release of transformers Next adds native Qwen-3 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__mixture_of_experts_ultra_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:mixture_of_experts_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-27_venturebeat_6542",
              "url": "https://venturebeat.com/2025/12/27/scaling_laws_for_neural_langua",
              "published": "2025-12-27",
              "snippet": "Under the hood, Scaling Laws for Neural Language Models (2025) implements Mixture of Experts Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_midjourney_v7_core_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:midjourney_v7_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:humaneval_pro_palm_3_lite_measures",
          "source": "benchmark:humaneval_pro",
          "target": "model:palm_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-22_meta_ai_blog_2025",
              "url": "https://ai.meta.com/blog/2026/01/22/humaneval_pro_palm_3_lite",
              "published": "2026-01-22",
              "snippet": "HumanEval Pro has become the standard for evaluating PaLM 3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_edge_claude_opus_45_plus_measures",
          "source": "benchmark:truthfulqa_edge",
          "target": "model:claude_opus_45_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:jamba_2_max_slimpajama_plus_trained_on",
          "source": "model:jamba_2_max",
          "target": "dataset:slimpajama_plus",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_sliding_window_attention_ultra_uses_tech",
          "source": "tool:mlflow_mini",
          "target": "tech:sliding_window_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:mt_bench_v2_codex_2_plus_measures",
          "source": "benchmark:mt_bench_v2",
          "target": "model:codex_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-12_wired_6745",
              "url": "https://wired.com/2026/01/12/mt_bench_v2_codex_2_plus",
              "published": "2026-01-12",
              "snippet": "MT-Bench v2 has become the standard for evaluating Codex 2 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_max_multimodal_fusion_edge_uses_tech",
          "source": "tool:crewai_max",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:phi_4_ultra_dall_e_4_max_depends_on",
          "source": "model:phi_4_ultra",
          "target": "model:dall_e_4_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_claude_opus_45_v2_depends_on",
          "source": "model:claude_opus_45_pro",
          "target": "model:claude_opus_45_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_redpajama_v2_next_trained_on",
          "source": "model:gemma_3_lite",
          "target": "dataset:redpajama_v2_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-16_meta_ai_blog_3364",
              "url": "https://ai.meta.com/blog/2026/01/16/gemma_3_lite_redpajama_v2_next",
              "published": "2026-01-16",
              "snippet": "Gemma 3 Lite utilized RedPajama v2 Next as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-19_anthropic_blog_8898",
              "url": "https://anthropic.com/news/2026/01/19/gemma_3_lite_redpajama_v2_next",
              "published": "2026-01-19",
              "snippet": "The training corpus for Gemma 3 Lite includes RedPajama v2 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_claude_sonnet_4_v2_integrates_with",
          "source": "tool:ollama_v2",
          "target": "model:claude_sonnet_4_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_4482",
              "url": "https://anthropic.com/news/2026/01/24/ollama_v2_claude_sonnet_4_v2",
              "published": "2026-01-24",
              "snippet": "Ollama v2 announced official support for Claude Sonnet 4 v2..."
            },
            {
              "docId": "2026-01-25_openai_blog_8419",
              "url": "https://openai.com/blog/2026/01/25/ollama_v2_claude_sonnet_4_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of Ollama v2 adds native Claude Sonnet 4 v2 integration..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_5776",
              "url": "https://wandb.ai/articles/2026/01/25/ollama_v2_claude_sonnet_4_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of Ollama v2 adds native Claude Sonnet 4 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_falcon_3_ultra_evaluated_on",
          "source": "paper:textbooks_are_all_you_need",
          "target": "model:falcon_3_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_flash_attention_uses_tech",
          "source": "model:claude_sonnet_4",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-15_arxiv_3464",
              "url": "https://arxiv.org/abs/2026/01/15/claude_sonnet_4_flash_attentio",
              "published": "2026-01-15",
              "snippet": "Technical details reveal Claude Sonnet 4 relies heavily on Flash Attention..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_gemma_3_lite_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:gemma_3_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_speculative_decoding_core_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:speculative_decoding_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-24_google_ai_blog_2020",
              "url": "https://blog.google/technology/ai/2025/12/24/flash_attention:_fast_and_memo",
              "published": "2025-12-24",
              "snippet": "Technical details reveal Flash Attention: Fast and Memory-Efficient Attention relies heavily on Speculative Decoding Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_rlhf_plus_uses_tech",
          "source": "repo:vllm_pro",
          "target": "tech:rlhf_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_nemotron_5_max_integrates_with",
          "source": "repo:ollama_ultra",
          "target": "model:nemotron_5_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2025-12-19_nvidia_blog_2724",
              "url": "https://blogs.nvidia.com/2025/12/19/ollama_ultra_nemotron_5_max",
              "published": "2025-12-19",
              "snippet": "The latest release of ollama Ultra adds native Nemotron-5 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_tool_use_core_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:localai_core_semantic_kernel_pro_integrates_with",
          "source": "tool:localai_core",
          "target": "tool:semantic_kernel_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-09_venturebeat_3971",
              "url": "https://venturebeat.com/2026/01/09/localai_core_semantic_kernel_p",
              "published": "2026-01-09",
              "snippet": "LocalAI Core announced official support for Semantic Kernel Pro..."
            },
            {
              "docId": "2026-01-12_reuters_3331",
              "url": "https://reuters.com/technology/2026/01/12/localai_core_semantic_kernel_p",
              "published": "2026-01-12",
              "snippet": "LocalAI Core now supports Semantic Kernel Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:roots_max_rlhf_pro_uses_tech",
          "source": "dataset:roots_max",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_lora_next_uses_tech",
          "source": "model:command_r_plus_edge",
          "target": "tech:lora_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:jamba_2_next_falcon_3_next_depends_on",
          "source": "model:jamba_2_next",
          "target": "model:falcon_3_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_deepseek_v3_integrates_with",
          "source": "repo:autogpt_v2",
          "target": "model:deepseek_v3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:localai_v2_transformer_architecture_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-12_arxiv_2152",
              "url": "https://arxiv.org/abs/2026/01/12/localai_v2_transformer_archite",
              "published": "2026-01-12",
              "snippet": "LocalAI v2 leverages Transformer Architecture to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-16_mit_technology__8050",
              "url": "https://technologyreview.com/2026/01/16/localai_v2_transformer_archite",
              "published": "2026-01-16",
              "snippet": "LocalAI v2 leverages Transformer Architecture to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_ultra_dpo_plus_uses_tech",
          "source": "model:gpt_5_ultra",
          "target": "tech:dpo_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_next_hellaswag_next_evaluated_on",
          "source": "model:claude_opus_45_next",
          "target": "benchmark:hellaswag_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_lite_fineweb_pro_trained_on",
          "source": "model:gpt_4o_mini_2_lite",
          "target": "dataset:fineweb_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:streamlit_v2_ollama_pro_integrates_with",
          "source": "tool:streamlit_v2",
          "target": "tool:ollama_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:localai_core_tree_of_thought_uses_tech",
          "source": "tool:localai_core",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_chain_of_thought_ultra_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_tokenizer_bpe_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:tokenizer_bpe",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_2384",
              "url": "https://wandb.ai/articles/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Direct Preference Optimization v2 leverages Tokenizer BPE to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_mini_mixture_of_experts_uses_tech",
          "source": "tool:dify_mini",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_qwen_3_v2_depends_on",
          "source": "model:command_r_plus_edge",
          "target": "model:qwen_3_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:llamaindex_llamaindex_ultra_integrates_with",
          "source": "tool:llamaindex",
          "target": "tool:llamaindex_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:yi_large_max_gguf_plus_uses_tech",
          "source": "model:yi_large_max",
          "target": "tech:gguf_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:mbpp_core_claude_opus_45_v2_measures",
          "source": "benchmark:mbpp_core",
          "target": "model:claude_opus_45_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-12_anthropic_blog_9630",
              "url": "https://anthropic.com/news/2026/01/12/mbpp_core_claude_opus_45_v2",
              "published": "2026-01-12",
              "snippet": "MBPP Core provides standardized evaluation of Claude Opus 4.5 v2..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_4373",
              "url": "https://huggingface.co/blog/2026/01/25/mbpp_core_claude_opus_45_v2",
              "published": "2026-01-25",
              "snippet": "MBPP Core provides standardized evaluation of Claude Opus 4.5 v2..."
            },
            {
              "docId": "2026-01-25_techcrunch_5927",
              "url": "https://techcrunch.com/2026/01/25/mbpp_core_claude_opus_45_v2",
              "published": "2026-01-25",
              "snippet": "The MBPP Core benchmark measures Claude Opus 4.5 v2 across multiple tasks..."
            },
            {
              "docId": "2026-01-25_reuters_8145",
              "url": "https://reuters.com/technology/2026/01/25/mbpp_core_claude_opus_45_v2",
              "published": "2026-01-25",
              "snippet": "MBPP Core provides standardized evaluation of Claude Opus 4.5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_constitutional_ai_pro_uses_tech",
          "source": "repo:ollama_edge",
          "target": "tech:constitutional_ai_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2025-12-03_nextgov_9830",
              "url": "https://nextgov.com/2025/12/03/ollama_edge_constitutional_ai_",
              "published": "2025-12-03",
              "snippet": "ollama Edge leverages Constitutional AI Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_distillation_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:distillation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_4451",
              "url": "https://arstechnica.com/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 Mini relies heavily on Distillation Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_plus_wikipedia_dump_2025_trained_on",
          "source": "model:deepseek_v3_plus",
          "target": "dataset:wikipedia_dump_2025",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_max_copilot_next_integrates_with",
          "source": "tool:tensorrt_llm_max",
          "target": "tool:copilot_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-10_meta_ai_blog_3773",
              "url": "https://ai.meta.com/blog/2026/01/10/tensorrt_llm_max_copilot_next",
              "published": "2026-01-10",
              "snippet": "TensorRT-LLM Max now supports Copilot Next with full feature parity..."
            },
            {
              "docId": "2026-01-24_reuters_2259",
              "url": "https://reuters.com/technology/2026/01/24/tensorrt_llm_max_copilot_next",
              "published": "2026-01-24",
              "snippet": "TensorRT-LLM Max now supports Copilot Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_edge_gpt_5_lite_depends_on",
          "source": "model:llama_4_edge",
          "target": "model:gpt_5_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:palm_3_v2_sora_2_mini_depends_on",
          "source": "model:palm_3_v2",
          "target": "model:sora_2_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_pro_gsm8k_plus_evaluated_on",
          "source": "model:deepseek_v3_pro",
          "target": "benchmark:gsm8k_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_lora_ultra_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:lora_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:grok_3_lite_wikipedia_dump_2025_core_trained_on",
          "source": "model:grok_3_lite",
          "target": "dataset:wikipedia_dump_2025_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-24_hugging_face_bl_4110",
              "url": "https://huggingface.co/blog/2026/01/24/grok_3_lite_wikipedia_dump_202",
              "published": "2026-01-24",
              "snippet": "Grok-3 Lite was trained on Wikipedia Dump 2025 Core comprising billions of tokens..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_7990",
              "url": "https://wandb.ai/articles/2026/01/25/grok_3_lite_wikipedia_dump_202",
              "published": "2026-01-25",
              "snippet": "Grok-3 Lite was trained on Wikipedia Dump 2025 Core comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_pro_autogpt_integrates_with",
          "source": "tool:autogpt_pro",
          "target": "tool:autogpt",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-23_arxiv_2298",
              "url": "https://arxiv.org/abs/2026/01/23/autogpt_pro_autogpt",
              "published": "2026-01-23",
              "snippet": "AutoGPT Pro now supports AutoGPT with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_pro_the_stack_v2_edge_trained_on",
          "source": "model:command_r_plus_pro",
          "target": "dataset:the_stack_v2_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-19_openai_blog_3102",
              "url": "https://openai.com/blog/2026/01/19/command_r_plus_pro_the_stack_v",
              "published": "2026-01-19",
              "snippet": "Command R+ Pro was trained on The Stack v2 Edge comprising billions of tokens..."
            },
            {
              "docId": "2026-01-20_openai_blog_7905",
              "url": "https://openai.com/blog/2026/01/20/command_r_plus_pro_the_stack_v",
              "published": "2026-01-20",
              "snippet": "The training corpus for Command R+ Pro includes The Stack v2 Edge..."
            },
            {
              "docId": "2026-01-24_langchain_blog_6930",
              "url": "https://blog.langchain.dev/2026/01/24/command_r_plus_pro_the_stack_v",
              "published": "2026-01-24",
              "snippet": "Command R+ Pro was trained on The Stack v2 Edge comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_mini_gpt_5_v2_depends_on",
          "source": "model:phi_4_mini",
          "target": "model:gpt_5_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_tool_use_max_uses_tech",
          "source": "model:gemma_3_mini",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_dpo_plus_uses_tech",
          "source": "paper:direct_preference_optimization_edge",
          "target": "tech:dpo_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_5217",
              "url": "https://techcrunch.com/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Direct Preference Optimization Edge leverages DPO Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_flash_attention_next_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "tech:flash_attention_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-22_meta_ai_blog_5363",
              "url": "https://ai.meta.com/blog/2026/01/22/attention_is_all_you_need_v2_m",
              "published": "2026-01-22",
              "snippet": "Under the hood, Attention Is All You Need v2 Max implements Flash Attention Next for improved efficiency..."
            },
            {
              "docId": "2026-01-25_venturebeat_8721",
              "url": "https://venturebeat.com/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Max leverages Flash Attention Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_max_midjourney_v7_edge_integrates_with",
          "source": "tool:gradio_max",
          "target": "model:midjourney_v7_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:gsm8k_v2_grok_3_core_measures",
          "source": "benchmark:gsm8k_v2",
          "target": "model:grok_3_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-17_weights_and_bia_4044",
              "url": "https://wandb.ai/articles/2026/01/17/gsm8k_v2_grok_3_core",
              "published": "2026-01-17",
              "snippet": "GSM8K v2 provides standardized evaluation of Grok-3 Core..."
            },
            {
              "docId": "2026-01-22_openai_blog_4276",
              "url": "https://openai.com/blog/2026/01/22/gsm8k_v2_grok_3_core",
              "published": "2026-01-22",
              "snippet": "GSM8K v2 has become the standard for evaluating Grok-3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_grok_3_lite_integrates_with",
          "source": "tool:cursor_mini",
          "target": "model:grok_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-11-24_hugging_face_bl_3594",
              "url": "https://huggingface.co/blog/2025/11/24/cursor_mini_grok_3_lite",
              "published": "2025-11-24",
              "snippet": "The latest release of Cursor Mini adds native Grok-3 Lite integration..."
            },
            {
              "docId": "2025-12-04_nvidia_blog_9956",
              "url": "https://blogs.nvidia.com/2025/12/04/cursor_mini_grok_3_lite",
              "published": "2025-12-04",
              "snippet": "Cursor Mini announced official support for Grok-3 Lite..."
            },
            {
              "docId": "2026-01-07_reuters_6641",
              "url": "https://reuters.com/technology/2026/01/07/cursor_mini_grok_3_lite",
              "published": "2026-01-07",
              "snippet": "The latest release of Cursor Mini adds native Grok-3 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_bigbench_hard_core_evaluated_on",
          "source": "model:command_r_plus_edge",
          "target": "benchmark:bigbench_hard_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-21_langchain_blog_7900",
              "url": "https://blog.langchain.dev/2026/01/21/command_r_plus_edge_bigbench_h",
              "published": "2026-01-21",
              "snippet": "On the BigBench Hard Core benchmark, Command R+ Edge scored 75%..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_3987",
              "url": "https://wandb.ai/articles/2026/01/25/command_r_plus_edge_bigbench_h",
              "published": "2026-01-25",
              "snippet": "On the BigBench Hard Core benchmark, Command R+ Edge scored 80%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_humaneval_evaluated_on",
          "source": "model:nemotron_5_next",
          "target": "benchmark:humaneval",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-12-18_meta_ai_blog_1773",
              "url": "https://ai.meta.com/blog/2025/12/18/nemotron_5_next_humaneval",
              "published": "2025-12-18",
              "snippet": "Nemotron-5 Next achieves 84% on HumanEval, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_core_red_teaming_lite_uses_tech",
          "source": "dataset:common_crawl_core",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_quantization_edge_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:quantization_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-14_ars_technica_5969",
              "url": "https://arstechnica.com/2026/01/14/ollama_v2_quantization_edge",
              "published": "2026-01-14",
              "snippet": "Technical details reveal ollama v2 relies heavily on Quantization Edge..."
            },
            {
              "docId": "2026-01-21_weights_and_bia_5686",
              "url": "https://wandb.ai/articles/2026/01/21/ollama_v2_quantization_edge",
              "published": "2026-01-21",
              "snippet": "ollama v2 leverages Quantization Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_rotary_position_embedding_plus_uses_tech",
          "source": "model:deepseek_v3_mini",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-08_weights_and_bia_8269",
              "url": "https://wandb.ai/articles/2026/01/08/deepseek_v3_mini_rotary_positi",
              "published": "2026-01-08",
              "snippet": "DeepSeek-V3 Mini leverages Rotary Position Embedding Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_red_teaming_v2_uses_tech",
          "source": "repo:text_generation_webui_max",
          "target": "tech:red_teaming_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_7451",
              "url": "https://venturebeat.com/2026/01/23/text_generation_webui_max_red_",
              "published": "2026-01-23",
              "snippet": "Under the hood, text-generation-webui Max implements Red Teaming v2 for improved efficiency..."
            },
            {
              "docId": "2026-01-23_venturebeat_4025",
              "url": "https://venturebeat.com/2026/01/23/text_generation_webui_max_red_",
              "published": "2026-01-23",
              "snippet": "Under the hood, text-generation-webui Max implements Red Teaming v2 for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_transformer_architecture_max_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "tech:transformer_architecture_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-13_wired_5449",
              "url": "https://wired.com/2025/12/13/attention_is_all_you_need_v2_m",
              "published": "2025-12-13",
              "snippet": "Technical details reveal Attention Is All You Need v2 Max relies heavily on Transformer Architecture Max..."
            },
            {
              "docId": "2025-12-29_microsoft_resea_6550",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/29/attention_is_all_you_need_v2_m",
              "published": "2025-12-29",
              "snippet": "Technical details reveal Attention Is All You Need v2 Max relies heavily on Transformer Architecture Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_edge_llama_4_pro_measures",
          "source": "benchmark:math_edge",
          "target": "model:llama_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-18_meta_ai_blog_9349",
              "url": "https://ai.meta.com/blog/2025/12/18/math_edge_llama_4_pro",
              "published": "2025-12-18",
              "snippet": "The MATH Edge benchmark measures Llama 4 Pro across multiple tasks..."
            },
            {
              "docId": "2025-12-19_openai_blog_3033",
              "url": "https://openai.com/blog/2025/12/19/math_edge_llama_4_pro",
              "published": "2025-12-19",
              "snippet": "MATH Edge has become the standard for evaluating Llama 4 Pro..."
            },
            {
              "docId": "2025-12-24_anthropic_blog_1708",
              "url": "https://anthropic.com/news/2025/12/24/math_edge_llama_4_pro",
              "published": "2025-12-24",
              "snippet": "MATH Edge provides standardized evaluation of Llama 4 Pro..."
            },
            {
              "docId": "2026-01-06_openai_blog_7656",
              "url": "https://openai.com/blog/2026/01/06/math_edge_llama_4_pro",
              "published": "2026-01-06",
              "snippet": "MATH Edge provides standardized evaluation of Llama 4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_deepseek_v3_core_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:deepseek_v3_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_tool_use_max_uses_tech",
          "source": "tool:mlflow_v2",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-14_arxiv_9780",
              "url": "https://arxiv.org/abs/2026/01/14/mlflow_v2_tool_use_max",
              "published": "2026-01-14",
              "snippet": "Technical details reveal MLflow v2 relies heavily on Tool Use Max..."
            },
            {
              "docId": "2026-01-15_ars_technica_5679",
              "url": "https://arstechnica.com/2026/01/15/mlflow_v2_tool_use_max",
              "published": "2026-01-15",
              "snippet": "Under the hood, MLflow v2 implements Tool Use Max for improved efficiency..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_2819",
              "url": "https://blogs.nvidia.com/2026/01/24/mlflow_v2_tool_use_max",
              "published": "2026-01-24",
              "snippet": "Under the hood, MLflow v2 implements Tool Use Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_litellm_pro_integrates_with",
          "source": "tool:cursor",
          "target": "tool:litellm_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__1167",
              "url": "https://technologyreview.com/2026/01/24/cursor_litellm_pro",
              "published": "2026-01-24",
              "snippet": "Cursor announced official support for LiteLLM Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_gemma_3_edge_integrates_with",
          "source": "tool:ollama_v2",
          "target": "model:gemma_3_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_next_synthetic_data_generation_core_uses_tech",
          "source": "dataset:wikipedia_dump_2025_next",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_mixture_of_experts_plus_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_phi_4_ultra_evaluated_on",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "model:phi_4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:gpt_5_refinedweb_mini_trained_on",
          "source": "model:gpt_5",
          "target": "dataset:refinedweb_mini",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_dolma_trained_on",
          "source": "model:claude_opus_45",
          "target": "dataset:dolma",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_next_sliding_window_attention_max_uses_tech",
          "source": "dataset:openwebtext2_next",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:ollama_qlora_core_uses_tech",
          "source": "repo:ollama",
          "target": "tech:qlora_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2025-12-23_nvidia_blog_5891",
              "url": "https://blogs.nvidia.com/2025/12/23/ollama_qlora_core",
              "published": "2025-12-23",
              "snippet": "Under the hood, ollama implements QLoRA Core for improved efficiency..."
            },
            {
              "docId": "2025-12-31_mit_technology__5788",
              "url": "https://technologyreview.com/2025/12/31/ollama_qlora_core",
              "published": "2025-12-31",
              "snippet": "Technical details reveal ollama relies heavily on QLoRA Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_core_lora_next_uses_tech",
          "source": "dataset:laion_5b_core",
          "target": "tech:lora_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:localai_gpt_4o_mini_2_lite_integrates_with",
          "source": "repo:localai",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-16_mit_technology__2756",
              "url": "https://technologyreview.com/2026/01/16/localai_gpt_4o_mini_2_lite",
              "published": "2026-01-16",
              "snippet": "The latest release of LocalAI adds native GPT-4o Mini 2 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_grok_3_max_measures",
          "source": "benchmark:truthfulqa",
          "target": "model:grok_3_max",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_litellm_mini_integrates_with",
          "source": "tool:autogpt_v2",
          "target": "tool:litellm_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:transformers_next_gemma_3_mini_integrates_with",
          "source": "repo:transformers_next",
          "target": "model:gemma_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_4583",
              "url": "https://blog.google/technology/ai/2026/01/25/transformers_next_gemma_3_mini",
              "published": "2026-01-25",
              "snippet": "transformers Next announced official support for Gemma 3 Mini..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_3774",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/transformers_next_gemma_3_mini",
              "published": "2026-01-25",
              "snippet": "transformers Next announced official support for Gemma 3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_qwen_3_max_measures",
          "source": "benchmark:lmsys_chatbot_arena",
          "target": "model:qwen_3_max",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:gradio_max_synthetic_data_generation_pro_uses_tech",
          "source": "tool:gradio_max",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:autogpt_ultra_speculative_decoding_lite_uses_tech",
          "source": "tool:autogpt_ultra",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-21_langchain_blog_6612",
              "url": "https://blog.langchain.dev/2026/01/21/autogpt_ultra_speculative_deco",
              "published": "2026-01-21",
              "snippet": "Under the hood, AutoGPT Ultra implements Speculative Decoding Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_kv_cache_optimization_ultra_uses_tech",
          "source": "tool:dify",
          "target": "tech:kv_cache_optimization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:aya_3_v2_nemotron_5_v2_depends_on",
          "source": "model:aya_3_v2",
          "target": "model:nemotron_5_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:streamlit_v2_crewai_pro_integrates_with",
          "source": "tool:streamlit_v2",
          "target": "tool:crewai_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-21_openai_blog_6083",
              "url": "https://openai.com/blog/2026/01/21/streamlit_v2_crewai_pro",
              "published": "2026-01-21",
              "snippet": "The latest release of Streamlit v2 adds native CrewAI Pro integration..."
            },
            {
              "docId": "2026-01-24_bloomberg_4113",
              "url": "https://bloomberg.com/technology/2026/01/24/streamlit_v2_crewai_pro",
              "published": "2026-01-24",
              "snippet": "Streamlit v2 now supports CrewAI Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_qlora_edge_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:qlora_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-16_nvidia_blog_1723",
              "url": "https://blogs.nvidia.com/2026/01/16/self_play_fine_tuning_for_lang",
              "published": "2026-01-16",
              "snippet": "Self-Play Fine-Tuning for Language Models leverages QLoRA Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-18_ars_technica_8379",
              "url": "https://arstechnica.com/2026/01/18/self_play_fine_tuning_for_lang",
              "published": "2026-01-18",
              "snippet": "Self-Play Fine-Tuning for Language Models leverages QLoRA Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_stable_diffusion_4_integrates_with",
          "source": "tool:litellm_mini",
          "target": "model:stable_diffusion_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-19_mit_technology__6542",
              "url": "https://technologyreview.com/2026/01/19/litellm_mini_stable_diffusion_",
              "published": "2026-01-19",
              "snippet": "LiteLLM Mini announced official support for Stable Diffusion 4..."
            },
            {
              "docId": "2026-01-25_openai_blog_3217",
              "url": "https://openai.com/blog/2026/01/25/litellm_mini_stable_diffusion_",
              "published": "2026-01-25",
              "snippet": "The latest release of LiteLLM Mini adds native Stable Diffusion 4 integration..."
            },
            {
              "docId": "2026-01-25_mit_technology__5352",
              "url": "https://technologyreview.com/2026/01/25/litellm_mini_stable_diffusion_",
              "published": "2026-01-25",
              "snippet": "The latest release of LiteLLM Mini adds native Stable Diffusion 4 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_max_hellaswag_pro_evaluated_on",
          "source": "model:sora_2_max",
          "target": "benchmark:hellaswag_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_5859",
              "url": "https://blogs.nvidia.com/2026/01/25/sora_2_max_hellaswag_pro",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag Pro benchmark, Sora 2 Max scored 78%..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_3691",
              "url": "https://anthropic.com/news/2026/01/25/sora_2_max_hellaswag_pro",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag Pro benchmark, Sora 2 Max scored 92%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_core_math_evaluated_on",
          "source": "model:grok_3_core",
          "target": "benchmark:math",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-21_venturebeat_2520",
              "url": "https://venturebeat.com/2026/01/21/grok_3_core_math",
              "published": "2026-01-21",
              "snippet": "Grok-3 Core achieves 86% on MATH, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_c4_trained_on",
          "source": "model:deepseek_v3_max",
          "target": "dataset:c4",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-07_venturebeat_8315",
              "url": "https://venturebeat.com/2026/01/07/deepseek_v3_max_c4",
              "published": "2026-01-07",
              "snippet": "DeepSeek-V3 Max was trained on C4 comprising billions of tokens..."
            },
            {
              "docId": "2026-01-09_hugging_face_bl_6673",
              "url": "https://huggingface.co/blog/2026/01/09/deepseek_v3_max_c4",
              "published": "2026-01-09",
              "snippet": "DeepSeek-V3 Max utilized C4 as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-15_techcrunch_5021",
              "url": "https://techcrunch.com/2026/01/15/deepseek_v3_max_c4",
              "published": "2026-01-15",
              "snippet": "DeepSeek-V3 Max was trained on C4 comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_mixture_of_experts_max_uses_tech",
          "source": "repo:ollama_edge",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-11-28_openai_blog_6592",
              "url": "https://openai.com/blog/2025/11/28/ollama_edge_mixture_of_experts",
              "published": "2025-11-28",
              "snippet": "Technical details reveal ollama Edge relies heavily on Mixture of Experts Max..."
            },
            {
              "docId": "2025-12-23_wired_9525",
              "url": "https://wired.com/2025/12/23/ollama_edge_mixture_of_experts",
              "published": "2025-12-23",
              "snippet": "Under the hood, ollama Edge implements Mixture of Experts Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_edge_stable_diffusion_4_edge_measures",
          "source": "benchmark:humaneval_edge",
          "target": "model:stable_diffusion_4_edge",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_gpt_4o_mini_2_max_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "model:gpt_4o_mini_2_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_redpajama_v2_core_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:redpajama_v2_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:codex_2_edge_swe_bench_max_evaluated_on",
          "source": "model:codex_2_edge",
          "target": "benchmark:swe_bench_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_9395",
              "url": "https://anthropic.com/news/2026/01/25/codex_2_edge_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Codex 2 Edge achieves 99% on SWE-bench Max, setting a new record..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_2020",
              "url": "https://blogs.nvidia.com/2026/01/25/codex_2_edge_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Codex 2 Edge achieves 76% on SWE-bench Max, setting a new record..."
            },
            {
              "docId": "2026-01-25_nextgov_1124",
              "url": "https://nextgov.com/2026/01/25/codex_2_edge_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Codex 2 Edge achieves 97% on SWE-bench Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_dify_v2_integrates_with",
          "source": "tool:copilot_edge",
          "target": "tool:dify_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_gguf_core_uses_tech",
          "source": "repo:autogpt_v2",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_2250",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/autogpt_v2_gguf_core",
              "published": "2026-01-25",
              "snippet": "Under the hood, AutoGPT v2 implements GGUF Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_max_stable_diffusion_4_next_integrates_with",
          "source": "tool:litellm_max",
          "target": "model:stable_diffusion_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-15_google_ai_blog_7763",
              "url": "https://blog.google/technology/ai/2026/01/15/litellm_max_stable_diffusion_4",
              "published": "2026-01-15",
              "snippet": "LiteLLM Max now supports Stable Diffusion 4 Next with full feature parity..."
            },
            {
              "docId": "2026-01-15_langchain_blog_3401",
              "url": "https://blog.langchain.dev/2026/01/15/litellm_max_stable_diffusion_4",
              "published": "2026-01-15",
              "snippet": "LiteLLM Max now supports Stable Diffusion 4 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_midjourney_v7_lite_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:midjourney_v7_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-14_the_verge_1194",
              "url": "https://theverge.com/2026/01/14/langchain_plus_midjourney_v7_l",
              "published": "2026-01-14",
              "snippet": "langchain Plus announced official support for Midjourney V7 Lite..."
            },
            {
              "docId": "2026-01-21_ars_technica_2447",
              "url": "https://arstechnica.com/2026/01/21/langchain_plus_midjourney_v7_l",
              "published": "2026-01-21",
              "snippet": "The latest release of langchain Plus adds native Midjourney V7 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_humaneval_edge_evaluated_on",
          "source": "model:sora_2_plus",
          "target": "benchmark:humaneval_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-19_meta_ai_blog_3976",
              "url": "https://ai.meta.com/blog/2026/01/19/sora_2_plus_humaneval_edge",
              "published": "2026-01-19",
              "snippet": "Sora 2 Plus achieves 97% on HumanEval Edge, setting a new record..."
            },
            {
              "docId": "2026-01-21_openai_blog_1181",
              "url": "https://openai.com/blog/2026/01/21/sora_2_plus_humaneval_edge",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Sora 2 Plus reaching 99% on HumanEval Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_cursor_max_integrates_with",
          "source": "tool:streamlit",
          "target": "tool:cursor_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-24_openai_blog_3835",
              "url": "https://openai.com/blog/2026/01/24/streamlit_cursor_max",
              "published": "2026-01-24",
              "snippet": "The latest release of Streamlit adds native Cursor Max integration..."
            },
            {
              "docId": "2026-01-25_the_verge_7317",
              "url": "https://theverge.com/2026/01/25/streamlit_cursor_max",
              "published": "2026-01-25",
              "snippet": "Streamlit announced official support for Cursor Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_copilot_edge_integrates_with",
          "source": "tool:llamaindex_pro",
          "target": "tool:copilot_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__8894",
              "url": "https://technologyreview.com/2026/01/25/llamaindex_pro_copilot_edge",
              "published": "2026-01-25",
              "snippet": "The latest release of LlamaIndex Pro adds native Copilot Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_lora_ultra_uses_tech",
          "source": "repo:open_interpreter_v2",
          "target": "tech:lora_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-23_ars_technica_7202",
              "url": "https://arstechnica.com/2026/01/23/open_interpreter_v2_lora_ultra",
              "published": "2026-01-23",
              "snippet": "Technical details reveal open-interpreter v2 relies heavily on LoRA Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_group_query_attention_core_uses_tech",
          "source": "repo:langchain_edge",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-01_reuters_1989",
              "url": "https://reuters.com/technology/2026/01/01/langchain_edge_group_query_att",
              "published": "2026-01-01",
              "snippet": "Under the hood, langchain Edge implements Group Query Attention Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__grok_3_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:grok_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-23_arxiv_6295",
              "url": "https://arxiv.org/abs/2026/01/23/constitutional_ai:_harmlessnes",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Constitutional AI: Harmlessness from AI Feedback reaching 99% on Grok-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_core_mixture_of_experts_max_uses_tech",
          "source": "dataset:starcoder_data_core",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_swe_bench_lite_evaluated_on",
          "source": "model:mixtral_8x22b_mini",
          "target": "benchmark:swe_bench_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_3313",
              "url": "https://arstechnica.com/2026/01/25/mixtral_8x22b_mini_swe_bench_l",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Mixtral 8x22B Mini reaching 78% on SWE-bench Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_pro_qwen_3_measures",
          "source": "benchmark:humaneval_pro",
          "target": "model:qwen_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__9743",
              "url": "https://technologyreview.com/2026/01/25/humaneval_pro_qwen_3",
              "published": "2026-01-25",
              "snippet": "The HumanEval Pro benchmark measures Qwen-3 across multiple tasks..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_8868",
              "url": "https://anthropic.com/news/2026/01/25/humaneval_pro_qwen_3",
              "published": "2026-01-25",
              "snippet": "HumanEval Pro has become the standard for evaluating Qwen-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_tokenizer_bpe_next_uses_tech",
          "source": "model:phi_4_lite",
          "target": "tech:tokenizer_bpe_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-23_nextgov_2717",
              "url": "https://nextgov.com/2026/01/23/phi_4_lite_tokenizer_bpe_next",
              "published": "2026-01-23",
              "snippet": "Under the hood, Phi-4 Lite implements Tokenizer BPE Next for improved efficiency..."
            },
            {
              "docId": "2026-01-23_arxiv_1119",
              "url": "https://arxiv.org/abs/2026/01/23/phi_4_lite_tokenizer_bpe_next",
              "published": "2026-01-23",
              "snippet": "Phi-4 Lite leverages Tokenizer BPE Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_stable_diffusion_4_max_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:stable_diffusion_4_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_ultra_rotary_position_embedding_v2_uses_tech",
          "source": "model:deepseek_v3_ultra",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-29_weights_and_bia_6749",
              "url": "https://wandb.ai/articles/2025/12/29/deepseek_v3_ultra_rotary_posit",
              "published": "2025-12-29",
              "snippet": "Under the hood, DeepSeek-V3 Ultra implements Rotary Position Embedding v2 for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_nemotron_5_core_integrates_with",
          "source": "tool:litellm_mini",
          "target": "model:nemotron_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_kv_cache_optimization_uses_tech",
          "source": "repo:ollama_edge",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:hellaswag_plus_dall_e_4_mini_measures",
          "source": "benchmark:hellaswag_plus",
          "target": "model:dall_e_4_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:transformers_next_lora_edge_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:lora_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-11-07_meta_ai_blog_7324",
              "url": "https://ai.meta.com/blog/2025/11/07/transformers_next_lora_edge",
              "published": "2025-11-07",
              "snippet": "Technical details reveal transformers Next relies heavily on LoRA Edge..."
            },
            {
              "docId": "2025-12-06_ars_technica_1728",
              "url": "https://arstechnica.com/2025/12/06/transformers_next_lora_edge",
              "published": "2025-12-06",
              "snippet": "transformers Next leverages LoRA Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_v2_gguf_max_uses_tech",
          "source": "tool:gradio_v2",
          "target": "tech:gguf_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2025-12-28_nvidia_blog_1912",
              "url": "https://blogs.nvidia.com/2025/12/28/gradio_v2_gguf_max",
              "published": "2025-12-28",
              "snippet": "Gradio v2 leverages GGUF Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-22_langchain_blog_6337",
              "url": "https://blog.langchain.dev/2026/01/22/gradio_v2_gguf_max",
              "published": "2026-01-22",
              "snippet": "Gradio v2 leverages GGUF Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_edge_sora_2_next_measures",
          "source": "benchmark:gsm8k_edge",
          "target": "model:sora_2_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-17_bloomberg_9801",
              "url": "https://bloomberg.com/technology/2026/01/17/gsm8k_edge_sora_2_next",
              "published": "2026-01-17",
              "snippet": "GSM8K Edge provides standardized evaluation of Sora 2 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_mixtral_8x22b_core_integrates_with",
          "source": "repo:transformers",
          "target": "model:mixtral_8x22b_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2025-12-11_reuters_1611",
              "url": "https://reuters.com/technology/2025/12/11/transformers_mixtral_8x22b_cor",
              "published": "2025-12-11",
              "snippet": "transformers announced official support for Mixtral 8x22B Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_core_sliding_window_attention_plus_uses_tech",
          "source": "dataset:the_stack_v2_core",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:humaneval_plus_falcon_3_next_measures",
          "source": "benchmark:humaneval_plus",
          "target": "model:falcon_3_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-19_langchain_blog_1473",
              "url": "https://blog.langchain.dev/2025/12/19/humaneval_plus_falcon_3_next",
              "published": "2025-12-19",
              "snippet": "HumanEval Plus has become the standard for evaluating Falcon 3 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_mini_gpt_4o_mini_2_next_depends_on",
          "source": "model:whisper_v4_mini",
          "target": "model:gpt_4o_mini_2_next",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_quantization_ultra_uses_tech",
          "source": "tool:litellm_pro",
          "target": "tech:quantization_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-09_nvidia_blog_1404",
              "url": "https://blogs.nvidia.com/2025/12/09/litellm_pro_quantization_ultra",
              "published": "2025-12-09",
              "snippet": "Under the hood, LiteLLM Pro implements Quantization Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_localai_next_integrates_with",
          "source": "tool:cursor_v2",
          "target": "tool:localai_next",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_gpqa_lite_evaluated_on",
          "source": "model:dall_e_4_core",
          "target": "benchmark:gpqa_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_venturebeat_2094",
              "url": "https://venturebeat.com/2026/01/22/dall_e_4_core_gpqa_lite",
              "published": "2026-01-22",
              "snippet": "Evaluation results show DALL-E 4 Core reaching 72% on GPQA Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_max_vllm_integrates_with",
          "source": "tool:tensorrt_llm_max",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_midjourney_v7_v2_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:midjourney_v7_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-16_techcrunch_6731",
              "url": "https://techcrunch.com/2026/01/16/textbooks_are_all_you_need_min",
              "published": "2026-01-16",
              "snippet": "Textbooks Are All You Need Mini achieves 90% on Midjourney V7 v2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_pro_autogpt_pro_integrates_with",
          "source": "tool:cody_pro",
          "target": "tool:autogpt_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-23_the_gradient_8177",
              "url": "https://thegradient.pub/2026/01/23/cody_pro_autogpt_pro",
              "published": "2026-01-23",
              "snippet": "Cody Pro now supports AutoGPT Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_rlhf_ultra_uses_tech",
          "source": "dataset:the_stack_v2",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_tool_use_core_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-04_google_ai_blog_2748",
              "url": "https://blog.google/technology/ai/2026/01/04/attention_is_all_you_need_v2_m",
              "published": "2026-01-04",
              "snippet": "Under the hood, Attention Is All You Need v2 Max implements Tool Use Core for improved efficiency..."
            },
            {
              "docId": "2026-01-09_hugging_face_bl_1054",
              "url": "https://huggingface.co/blog/2026/01/09/attention_is_all_you_need_v2_m",
              "published": "2026-01-09",
              "snippet": "Attention Is All You Need v2 Max leverages Tool Use Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-20_venturebeat_7908",
              "url": "https://venturebeat.com/2026/01/20/attention_is_all_you_need_v2_m",
              "published": "2026-01-20",
              "snippet": "Technical details reveal Attention Is All You Need v2 Max relies heavily on Tool Use Core..."
            },
            {
              "docId": "2026-01-24_reuters_4812",
              "url": "https://reuters.com/technology/2026/01/24/attention_is_all_you_need_v2_m",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Attention Is All You Need v2 Max relies heavily on Tool Use Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_rotary_position_embedding_core_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:rotary_position_embedding_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-03_bloomberg_8098",
              "url": "https://bloomberg.com/technology/2026/01/03/gpt4all_pro_rotary_position_em",
              "published": "2026-01-03",
              "snippet": "Technical details reveal gpt4all Pro relies heavily on Rotary Position Embedding Core..."
            },
            {
              "docId": "2026-01-08_hugging_face_bl_8867",
              "url": "https://huggingface.co/blog/2026/01/08/gpt4all_pro_rotary_position_em",
              "published": "2026-01-08",
              "snippet": "gpt4all Pro leverages Rotary Position Embedding Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-17_weights_and_bia_9812",
              "url": "https://wandb.ai/articles/2026/01/17/gpt4all_pro_rotary_position_em",
              "published": "2026-01-17",
              "snippet": "gpt4all Pro leverages Rotary Position Embedding Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_next_codex_2_lite_integrates_with",
          "source": "tool:weights_and_biases_next",
          "target": "model:codex_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-17_nextgov_7350",
              "url": "https://nextgov.com/2026/01/17/weights_and_biases_next_codex_",
              "published": "2026-01-17",
              "snippet": "Weights & Biases Next now supports Codex 2 Lite with full feature parity..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_6057",
              "url": "https://blogs.nvidia.com/2026/01/22/weights_and_biases_next_codex_",
              "published": "2026-01-22",
              "snippet": "The latest release of Weights & Biases Next adds native Codex 2 Lite integration..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_3505",
              "url": "https://wandb.ai/articles/2026/01/24/weights_and_biases_next_codex_",
              "published": "2026-01-24",
              "snippet": "The latest release of Weights & Biases Next adds native Codex 2 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_mixtral_8x22b_pro_integrates_with",
          "source": "repo:open_interpreter_v2",
          "target": "model:mixtral_8x22b_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-16_ars_technica_5808",
              "url": "https://arstechnica.com/2026/01/16/open_interpreter_v2_mixtral_8x",
              "published": "2026-01-16",
              "snippet": "The latest release of open-interpreter v2 adds native Mixtral 8x22B Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_kv_cache_optimization_next_uses_tech",
          "source": "repo:ollama_mini",
          "target": "tech:kv_cache_optimization_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2025-12-28_microsoft_resea_9104",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/28/ollama_mini_kv_cache_optimizat",
              "published": "2025-12-28",
              "snippet": "Technical details reveal ollama Mini relies heavily on KV Cache Optimization Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_next_llama_4_next_integrates_with",
          "source": "tool:copilot_next",
          "target": "model:llama_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-19_nextgov_2062",
              "url": "https://nextgov.com/2026/01/19/copilot_next_llama_4_next",
              "published": "2026-01-19",
              "snippet": "Copilot Next announced official support for Llama 4 Next..."
            },
            {
              "docId": "2026-01-25_bloomberg_6714",
              "url": "https://bloomberg.com/technology/2026/01/25/copilot_next_llama_4_next",
              "published": "2026-01-25",
              "snippet": "Copilot Next now supports Llama 4 Next with full feature parity..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_2483",
              "url": "https://blogs.nvidia.com/2026/01/25/copilot_next_llama_4_next",
              "published": "2026-01-25",
              "snippet": "The latest release of Copilot Next adds native Llama 4 Next integration..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_5236",
              "url": "https://blogs.nvidia.com/2026/01/25/copilot_next_llama_4_next",
              "published": "2026-01-25",
              "snippet": "The latest release of Copilot Next adds native Llama 4 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_mini_claude_sonnet_4_measures",
          "source": "benchmark:mmlu_mini",
          "target": "model:claude_sonnet_4",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:vllm_rlhf_ultra_uses_tech",
          "source": "tool:vllm",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_tokenizer_bpe_next_uses_tech",
          "source": "repo:llamacpp_ultra",
          "target": "tech:tokenizer_bpe_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_next_dall_e_4_ultra_depends_on",
          "source": "model:gpt_4o_mini_2_next",
          "target": "model:dall_e_4_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_gradio_v2_integrates_with",
          "source": "tool:ollama_edge",
          "target": "tool:gradio_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-21_wired_1127",
              "url": "https://wired.com/2025/12/21/ollama_edge_gradio_v2",
              "published": "2025-12-21",
              "snippet": "The latest release of Ollama Edge adds native Gradio v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_edge_refinedweb_mini_trained_on",
          "source": "model:gemini_ultra_2_edge",
          "target": "dataset:refinedweb_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:ollama_plus_whisper_v4_pro_integrates_with",
          "source": "tool:ollama_plus",
          "target": "model:whisper_v4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-19_the_gradient_7929",
              "url": "https://thegradient.pub/2026/01/19/ollama_plus_whisper_v4_pro",
              "published": "2026-01-19",
              "snippet": "Ollama Plus now supports Whisper v4 Pro with full feature parity..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_8356",
              "url": "https://ai.meta.com/blog/2026/01/25/ollama_plus_whisper_v4_pro",
              "published": "2026-01-25",
              "snippet": "The latest release of Ollama Plus adds native Whisper v4 Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_gpt_5_pro_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:gpt_5_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:vllm_lite_nemotron_5_max_integrates_with",
          "source": "tool:vllm_lite",
          "target": "model:nemotron_5_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-12_microsoft_resea_2617",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/12/vllm_lite_nemotron_5_max",
              "published": "2025-12-12",
              "snippet": "vLLM Lite now supports Nemotron-5 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_next_whisper_v4_v2_measures",
          "source": "benchmark:mt_bench_next",
          "target": "model:whisper_v4_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_7603",
              "url": "https://huggingface.co/blog/2026/01/22/mt_bench_next_whisper_v4_v2",
              "published": "2026-01-22",
              "snippet": "MT-Bench Next has become the standard for evaluating Whisper v4 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_chain_of_thought_lite_uses_tech",
          "source": "repo:ollama_edge",
          "target": "tech:chain_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_gemma_3_max_integrates_with",
          "source": "repo:langchain_ultra",
          "target": "model:gemma_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-11_microsoft_resea_4397",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/11/langchain_ultra_gemma_3_max",
              "published": "2026-01-11",
              "snippet": "langchain Ultra now supports Gemma 3 Max with full feature parity..."
            },
            {
              "docId": "2026-01-12_ars_technica_5618",
              "url": "https://arstechnica.com/2026/01/12/langchain_ultra_gemma_3_max",
              "published": "2026-01-12",
              "snippet": "The latest release of langchain Ultra adds native Gemma 3 Max integration..."
            },
            {
              "docId": "2026-01-19_the_gradient_2786",
              "url": "https://thegradient.pub/2026/01/19/langchain_ultra_gemma_3_max",
              "published": "2026-01-19",
              "snippet": "langchain Ultra announced official support for Gemma 3 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_ultra_weights_and_biases_integrates_with",
          "source": "tool:litellm_ultra",
          "target": "tool:weights_and_biases",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-25_google_ai_blog_9521",
              "url": "https://blog.google/technology/ai/2025/12/25/litellm_ultra_weights_and_bias",
              "published": "2025-12-25",
              "snippet": "LiteLLM Ultra announced official support for Weights & Biases..."
            },
            {
              "docId": "2026-01-16_nextgov_2862",
              "url": "https://nextgov.com/2026/01/16/litellm_ultra_weights_and_bias",
              "published": "2026-01-16",
              "snippet": "The latest release of LiteLLM Ultra adds native Weights & Biases integration..."
            },
            {
              "docId": "2026-01-17_bloomberg_5041",
              "url": "https://bloomberg.com/technology/2026/01/17/litellm_ultra_weights_and_bias",
              "published": "2026-01-17",
              "snippet": "The latest release of LiteLLM Ultra adds native Weights & Biases integration..."
            },
            {
              "docId": "2026-01-25_openai_blog_7462",
              "url": "https://openai.com/blog/2026/01/25/litellm_ultra_weights_and_bias",
              "published": "2026-01-25",
              "snippet": "LiteLLM Ultra announced official support for Weights & Biases..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_falcon_3_v2_integrates_with",
          "source": "repo:ollama_mini",
          "target": "model:falcon_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_hellaswag_mini_evaluated_on",
          "source": "model:claude_sonnet_4_next",
          "target": "benchmark:hellaswag_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-20_langchain_blog_9909",
              "url": "https://blog.langchain.dev/2026/01/20/claude_sonnet_4_next_hellaswag",
              "published": "2026-01-20",
              "snippet": "On the HellaSwag Mini benchmark, Claude Sonnet 4 Next scored 71%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_v2_redpajama_v2_edge_trained_on",
          "source": "model:claude_sonnet_4_v2",
          "target": "dataset:redpajama_v2_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-15_weights_and_bia_8642",
              "url": "https://wandb.ai/articles/2026/01/15/claude_sonnet_4_v2_redpajama_v",
              "published": "2026-01-15",
              "snippet": "The training corpus for Claude Sonnet 4 v2 includes RedPajama v2 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_fineweb_next_trained_on",
          "source": "model:gpt_5_v2",
          "target": "dataset:fineweb_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_plus_group_query_attention_core_uses_tech",
          "source": "tool:weights_and_biases_plus",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-04_wired_7591",
              "url": "https://wired.com/2026/01/04/weights_and_biases_plus_group_",
              "published": "2026-01-04",
              "snippet": "Weights & Biases Plus leverages Group Query Attention Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-09_the_gradient_2884",
              "url": "https://thegradient.pub/2026/01/09/weights_and_biases_plus_group_",
              "published": "2026-01-09",
              "snippet": "Technical details reveal Weights & Biases Plus relies heavily on Group Query Attention Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_lite_midjourney_v7_v2_integrates_with",
          "source": "tool:dify_lite",
          "target": "model:midjourney_v7_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-10_langchain_blog_6400",
              "url": "https://blog.langchain.dev/2026/01/10/dify_lite_midjourney_v7_v2",
              "published": "2026-01-10",
              "snippet": "Dify Lite announced official support for Midjourney V7 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_speculative_decoding_ultra_uses_tech",
          "source": "tool:gradio",
          "target": "tech:speculative_decoding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-24_nvidia_blog_3232",
              "url": "https://blogs.nvidia.com/2026/01/24/gradio_speculative_decoding_ul",
              "published": "2026-01-24",
              "snippet": "Gradio leverages Speculative Decoding Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_bloomberg_1065",
              "url": "https://bloomberg.com/technology/2026/01/25/gradio_speculative_decoding_ul",
              "published": "2026-01-25",
              "snippet": "Under the hood, Gradio implements Speculative Decoding Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_5368",
              "url": "https://blogs.nvidia.com/2026/01/25/gradio_speculative_decoding_ul",
              "published": "2026-01-25",
              "snippet": "Under the hood, Gradio implements Speculative Decoding Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_2219",
              "url": "https://blogs.nvidia.com/2026/01/25/gradio_speculative_decoding_ul",
              "published": "2026-01-25",
              "snippet": "Under the hood, Gradio implements Speculative Decoding Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_arc_agi_edge_evaluated_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "benchmark:arc_agi_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-12-13_wired_8357",
              "url": "https://wired.com/2025/12/13/stable_diffusion_4_v2_arc_agi_",
              "published": "2025-12-13",
              "snippet": "On the ARC-AGI Edge benchmark, Stable Diffusion 4 v2 scored 89%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_edge_whisper_v4_v2_integrates_with",
          "source": "tool:localai_edge",
          "target": "model:whisper_v4_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-21_bloomberg_3203",
              "url": "https://bloomberg.com/technology/2026/01/21/localai_edge_whisper_v4_v2",
              "published": "2026-01-21",
              "snippet": "The latest release of LocalAI Edge adds native Whisper v4 v2 integration..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_4603",
              "url": "https://ai.meta.com/blog/2026/01/24/localai_edge_whisper_v4_v2",
              "published": "2026-01-24",
              "snippet": "The latest release of LocalAI Edge adds native Whisper v4 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_math_pro_evaluated_on",
          "source": "model:deepseek_v3_max",
          "target": "benchmark:math_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2026-01-16_mit_technology__3468",
              "url": "https://technologyreview.com/2026/01/16/deepseek_v3_max_math_pro",
              "published": "2026-01-16",
              "snippet": "On the MATH Pro benchmark, DeepSeek-V3 Max scored 86%..."
            },
            {
              "docId": "2026-01-25_reuters_1368",
              "url": "https://reuters.com/technology/2026/01/25/deepseek_v3_max_math_pro",
              "published": "2026-01-25",
              "snippet": "On the MATH Pro benchmark, DeepSeek-V3 Max scored 70%..."
            },
            {
              "docId": "2026-01-25_reuters_5414",
              "url": "https://reuters.com/technology/2026/01/25/deepseek_v3_max_math_pro",
              "published": "2026-01-25",
              "snippet": "DeepSeek-V3 Max achieves 74% on MATH Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_speculative_decoding_ultra_uses_tech",
          "source": "repo:text_generation_webui",
          "target": "tech:speculative_decoding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-24_langchain_blog_5333",
              "url": "https://blog.langchain.dev/2026/01/24/text_generation_webui_speculat",
              "published": "2026-01-24",
              "snippet": "text-generation-webui leverages Speculative Decoding Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_v2_midjourney_v7_mini_integrates_with",
          "source": "repo:localai_v2",
          "target": "model:midjourney_v7_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-22_techcrunch_8592",
              "url": "https://techcrunch.com/2025/12/22/localai_v2_midjourney_v7_mini",
              "published": "2025-12-22",
              "snippet": "The latest release of LocalAI v2 adds native Midjourney V7 Mini integration..."
            },
            {
              "docId": "2026-01-05_google_ai_blog_4275",
              "url": "https://blog.google/technology/ai/2026/01/05/localai_v2_midjourney_v7_mini",
              "published": "2026-01-05",
              "snippet": "LocalAI v2 announced official support for Midjourney V7 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_v2_falcon_3_core_measures",
          "source": "benchmark:alpacaeval_2_v2",
          "target": "model:falcon_3_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-11_langchain_blog_9640",
              "url": "https://blog.langchain.dev/2026/01/11/alpacaeval_2_v2_falcon_3_core",
              "published": "2026-01-11",
              "snippet": "AlpacaEval 2 v2 provides standardized evaluation of Falcon 3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_bigbench_hard_v2_evaluated_on",
          "source": "model:qwen_3_next",
          "target": "benchmark:bigbench_hard_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-23_google_ai_blog_5748",
              "url": "https://blog.google/technology/ai/2026/01/23/qwen_3_next_bigbench_hard_v2",
              "published": "2026-01-23",
              "snippet": "On the BigBench Hard v2 benchmark, Qwen-3 Next scored 93%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_command_r_plus_core_integrates_with",
          "source": "tool:litellm_pro",
          "target": "model:command_r_plus_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-08_google_ai_blog_1869",
              "url": "https://blog.google/technology/ai/2025/12/08/litellm_pro_command_r_plus_cor",
              "published": "2025-12-08",
              "snippet": "LiteLLM Pro now supports Command R+ Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_nemotron_5_v2_integrates_with",
          "source": "tool:dify",
          "target": "model:nemotron_5_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_copilot_plus_integrates_with",
          "source": "tool:litellm_mini",
          "target": "tool:copilot_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-16_bloomberg_1706",
              "url": "https://bloomberg.com/technology/2026/01/16/litellm_mini_copilot_plus",
              "published": "2026-01-16",
              "snippet": "LiteLLM Mini now supports Copilot Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_grok_3_v2_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:grok_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-11_bloomberg_2378",
              "url": "https://bloomberg.com/technology/2026/01/11/langchain_max_grok_3_v2",
              "published": "2026-01-11",
              "snippet": "The latest release of langchain Max adds native Grok-3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_qwen_3_next_integrates_with",
          "source": "repo:text_generation_webui_max",
          "target": "model:qwen_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:langchain_gradio_mini_integrates_with",
          "source": "tool:langchain",
          "target": "tool:gradio_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-24_nvidia_blog_6324",
              "url": "https://blogs.nvidia.com/2026/01/24/langchain_gradio_mini",
              "published": "2026-01-24",
              "snippet": "LangChain announced official support for Gradio Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_core_chain_of_thought_mini_uses_tech",
          "source": "model:claude_sonnet_4_core",
          "target": "tech:chain_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_nemotron_5_core_depends_on",
          "source": "model:grok_3_plus",
          "target": "model:nemotron_5_core",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_mixture_of_experts_max_uses_tech",
          "source": "model:dall_e_4_core",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_fineweb_next_trained_on",
          "source": "model:claude_opus_45_lite",
          "target": "dataset:fineweb_next",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_yi_large_depends_on",
          "source": "model:gemma_3_lite",
          "target": "model:yi_large",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:ollama_next_gpt_4o_mini_2_integrates_with",
          "source": "tool:ollama_next",
          "target": "model:gpt_4o_mini_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:falcon_3_next_math_max_evaluated_on",
          "source": "model:falcon_3_next",
          "target": "benchmark:math_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-15_bloomberg_2935",
              "url": "https://bloomberg.com/technology/2026/01/15/falcon_3_next_math_max",
              "published": "2026-01-15",
              "snippet": "Falcon 3 Next achieves 89% on MATH Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_core_rotary_position_embedding_ultra_uses_tech",
          "source": "dataset:laion_5b_core",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:cody_pro_sparse_attention_lite_uses_tech",
          "source": "tool:cody_pro",
          "target": "tech:sparse_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_core_winogrande_plus_evaluated_on",
          "source": "model:deepseek_v3_core",
          "target": "benchmark:winogrande_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-13_ars_technica_6811",
              "url": "https://arstechnica.com/2026/01/13/deepseek_v3_core_winogrande_pl",
              "published": "2026-01-13",
              "snippet": "Evaluation results show DeepSeek-V3 Core reaching 80% on WinoGrande Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_v2_palm_3_next_integrates_with",
          "source": "tool:litellm_v2",
          "target": "model:palm_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_falcon_3_plus_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:falcon_3_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-21_mit_technology__3777",
              "url": "https://technologyreview.com/2025/12/21/llm_agents:_a_survey_falcon_3_",
              "published": "2025-12-21",
              "snippet": "LLM Agents: A Survey achieves 87% on Falcon 3 Plus, setting a new record..."
            },
            {
              "docId": "2026-01-14_google_ai_blog_5198",
              "url": "https://blog.google/technology/ai/2026/01/14/llm_agents:_a_survey_falcon_3_",
              "published": "2026-01-14",
              "snippet": "LLM Agents: A Survey achieves 78% on Falcon 3 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_v2_chain_of_thought_max_uses_tech",
          "source": "model:gemini_ultra_2_v2",
          "target": "tech:chain_of_thought_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_mini_llama_4_core_integrates_with",
          "source": "tool:tensorrt_llm_mini",
          "target": "model:llama_4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_synthetic_data_generation_max_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-16_ars_technica_9633",
              "url": "https://arstechnica.com/2026/01/16/toolformer:_language_models_ca",
              "published": "2026-01-16",
              "snippet": "Toolformer: Language Models Can Teach Themselves to Use Tools leverages Synthetic Data Generation Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_bigbench_hard_max_evaluated_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "benchmark:bigbench_hard_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__2707",
              "url": "https://technologyreview.com/2026/01/24/gemini_ultra_2_pro_bigbench_ha",
              "published": "2026-01-24",
              "snippet": "On the BigBench Hard Max benchmark, Gemini Ultra 2 Pro scored 96%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_red_teaming_ultra_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:gpt_5_next_stable_diffusion_4_max_depends_on",
          "source": "model:gpt_5_next",
          "target": "model:stable_diffusion_4_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:langchain_core_dpo_v2_uses_tech",
          "source": "tool:langchain_core",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_dall_e_4_core_integrates_with",
          "source": "repo:text_generation_webui",
          "target": "model:dall_e_4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_math_core_evaluated_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "benchmark:math_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-11-18_hugging_face_bl_3152",
              "url": "https://huggingface.co/blog/2025/11/18/stable_diffusion_4_v2_math_cor",
              "published": "2025-11-18",
              "snippet": "Evaluation results show Stable Diffusion 4 v2 reaching 96% on MATH Core..."
            },
            {
              "docId": "2025-12-16_microsoft_resea_4918",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/16/stable_diffusion_4_v2_math_cor",
              "published": "2025-12-16",
              "snippet": "On the MATH Core benchmark, Stable Diffusion 4 v2 scored 81%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_midjourney_v7_max_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:midjourney_v7_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-20_ars_technica_2565",
              "url": "https://arstechnica.com/2026/01/20/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-20",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models achieves 92% on Midjourney V7 Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_falcon_3_max_integrates_with",
          "source": "repo:localai_plus",
          "target": "model:falcon_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-24_nextgov_9480",
              "url": "https://nextgov.com/2025/12/24/localai_plus_falcon_3_max",
              "published": "2025-12-24",
              "snippet": "LocalAI Plus announced official support for Falcon 3 Max..."
            },
            {
              "docId": "2026-01-02_google_ai_blog_3793",
              "url": "https://blog.google/technology/ai/2026/01/02/localai_plus_falcon_3_max",
              "published": "2026-01-02",
              "snippet": "The latest release of LocalAI Plus adds native Falcon 3 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_mbpp_lite_evaluated_on",
          "source": "model:aya_3_max",
          "target": "benchmark:mbpp_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-18_weights_and_bia_9011",
              "url": "https://wandb.ai/articles/2026/01/18/aya_3_max_mbpp_lite",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Aya 3 Max reaching 99% on MBPP Lite..."
            },
            {
              "docId": "2026-01-21_techcrunch_4582",
              "url": "https://techcrunch.com/2026/01/21/aya_3_max_mbpp_lite",
              "published": "2026-01-21",
              "snippet": "Aya 3 Max achieves 96% on MBPP Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_flash_attention_max_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-19_techcrunch_5612",
              "url": "https://techcrunch.com/2026/01/19/attention_is_all_you_need_v2_p",
              "published": "2026-01-19",
              "snippet": "Attention Is All You Need v2 Plus leverages Flash Attention Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_google_ai_blog_4167",
              "url": "https://blog.google/technology/ai/2026/01/23/attention_is_all_you_need_v2_p",
              "published": "2026-01-23",
              "snippet": "Under the hood, Attention Is All You Need v2 Plus implements Flash Attention Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_v2_constitutional_ai_uses_tech",
          "source": "repo:vllm_v2",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_pro_phi_4_mini_depends_on",
          "source": "model:claude_sonnet_4_pro",
          "target": "model:phi_4_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_stable_diffusion_4_edge_integrates_with",
          "source": "repo:vllm_edge",
          "target": "model:stable_diffusion_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-22_meta_ai_blog_5172",
              "url": "https://ai.meta.com/blog/2026/01/22/vllm_edge_stable_diffusion_4_e",
              "published": "2026-01-22",
              "snippet": "vllm Edge announced official support for Stable Diffusion 4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_v2_sparse_attention_pro_uses_tech",
          "source": "dataset:slimpajama_v2",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_arc_agi_evaluated_on",
          "source": "model:deepseek_v3_max",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-24_meta_ai_blog_6067",
              "url": "https://ai.meta.com/blog/2026/01/24/deepseek_v3_max_arc_agi",
              "published": "2026-01-24",
              "snippet": "Evaluation results show DeepSeek-V3 Max reaching 91% on ARC-AGI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_lite_rlhf_max_uses_tech",
          "source": "dataset:laion_5b_lite",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_dpo_mini_uses_tech",
          "source": "model:gemini_ultra_2_pro",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_gguf_mini_uses_tech",
          "source": "model:deepseek_v3_max",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:scaling_data_constrained_language_models_lora_lite_uses_tech",
          "source": "paper:scaling_data_constrained_language_models",
          "target": "tech:lora_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_gguf_core_uses_tech",
          "source": "tool:vllm_pro",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_ollama_next_integrates_with",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tool:ollama_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:codex_2_mini_falcon_3_next_depends_on",
          "source": "model:codex_2_mini",
          "target": "model:falcon_3_next",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:slimpajama_transformer_architecture_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_autogpt_integrates_with",
          "source": "tool:ollama_edge",
          "target": "tool:autogpt",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_aya_3_mini_integrates_with",
          "source": "tool:mlflow_v2",
          "target": "model:aya_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-07_weights_and_bia_4005",
              "url": "https://wandb.ai/articles/2026/01/07/mlflow_v2_aya_3_mini",
              "published": "2026-01-07",
              "snippet": "The latest release of MLflow v2 adds native Aya 3 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_plus_sora_2_edge_integrates_with",
          "source": "repo:transformers_plus",
          "target": "model:sora_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-25_wired_4640",
              "url": "https://wired.com/2025/12/25/transformers_plus_sora_2_edge",
              "published": "2025-12-25",
              "snippet": "transformers Plus now supports Sora 2 Edge with full feature parity..."
            },
            {
              "docId": "2026-01-04_reuters_1543",
              "url": "https://reuters.com/technology/2026/01/04/transformers_plus_sora_2_edge",
              "published": "2026-01-04",
              "snippet": "The latest release of transformers Plus adds native Sora 2 Edge integration..."
            },
            {
              "docId": "2026-01-21_venturebeat_2568",
              "url": "https://venturebeat.com/2026/01/21/transformers_plus_sora_2_edge",
              "published": "2026-01-21",
              "snippet": "transformers Plus announced official support for Sora 2 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_core_kv_cache_optimization_max_uses_tech",
          "source": "tool:cursor_core",
          "target": "tech:kv_cache_optimization_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_humaneval_edge_evaluated_on",
          "source": "model:claude_sonnet_4_next",
          "target": "benchmark:humaneval_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-20_hugging_face_bl_7582",
              "url": "https://huggingface.co/blog/2026/01/20/claude_sonnet_4_next_humaneval",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Claude Sonnet 4 Next reaching 83% on HumanEval Edge..."
            },
            {
              "docId": "2026-01-23_techcrunch_2914",
              "url": "https://techcrunch.com/2026/01/23/claude_sonnet_4_next_humaneval",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Claude Sonnet 4 Next reaching 76% on HumanEval Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_transformer_architecture_edge_uses_tech",
          "source": "repo:open_interpreter_core",
          "target": "tech:transformer_architecture_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-13_hugging_face_bl_1927",
              "url": "https://huggingface.co/blog/2026/01/13/open_interpreter_core_transfor",
              "published": "2026-01-13",
              "snippet": "open-interpreter Core leverages Transformer Architecture Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_mixtral_8x22b_plus_measures",
          "source": "benchmark:gpqa",
          "target": "model:mixtral_8x22b_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-21_weights_and_bia_8461",
              "url": "https://wandb.ai/articles/2026/01/21/gpqa_mixtral_8x22b_plus",
              "published": "2026-01-21",
              "snippet": "GPQA has become the standard for evaluating Mixtral 8x22B Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_v2_semantic_kernel_ultra_integrates_with",
          "source": "tool:streamlit_v2",
          "target": "tool:semantic_kernel_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:c4_ultra_transformer_architecture_core_uses_tech",
          "source": "dataset:c4_ultra",
          "target": "tech:transformer_architecture_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:copilot_max_claude_opus_45_max_integrates_with",
          "source": "tool:copilot_max",
          "target": "model:claude_opus_45_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__synthetic_data_generation_edge_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:autogpt_multimodal_fusion_lite_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:multimodal_fusion_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_rlhf_ultra_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-18_nvidia_blog_5819",
              "url": "https://blogs.nvidia.com/2026/01/18/attention_is_all_you_need_v2_n",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Attention Is All You Need v2 Next relies heavily on RLHF Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_dall_e_4_core_integrates_with",
          "source": "repo:autogpt_plus",
          "target": "model:dall_e_4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:langchain_max_dpo_max_uses_tech",
          "source": "repo:langchain_max",
          "target": "tech:dpo_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2025-12-12_reuters_3254",
              "url": "https://reuters.com/technology/2025/12/12/langchain_max_dpo_max",
              "published": "2025-12-12",
              "snippet": "Technical details reveal langchain Max relies heavily on DPO Max..."
            },
            {
              "docId": "2026-01-20_google_ai_blog_4701",
              "url": "https://blog.google/technology/ai/2026/01/20/langchain_max_dpo_max",
              "published": "2026-01-20",
              "snippet": "langchain Max leverages DPO Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_ultra_command_r_plus_pro_integrates_with",
          "source": "tool:autogpt_ultra",
          "target": "model:command_r_plus_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_llama_4_edge_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "model:llama_4_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-14_langchain_blog_2613",
              "url": "https://blog.langchain.dev/2026/01/14/attention_is_all_you_need_v2_m",
              "published": "2026-01-14",
              "snippet": "On the Llama 4 Edge benchmark, Attention Is All You Need v2 Max scored 93%..."
            },
            {
              "docId": "2026-01-18_weights_and_bia_8105",
              "url": "https://wandb.ai/articles/2026/01/18/attention_is_all_you_need_v2_m",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Attention Is All You Need v2 Max reaching 96% on Llama 4 Edge..."
            },
            {
              "docId": "2026-01-20_openai_blog_5993",
              "url": "https://openai.com/blog/2026/01/20/attention_is_all_you_need_v2_m",
              "published": "2026-01-20",
              "snippet": "On the Llama 4 Edge benchmark, Attention Is All You Need v2 Max scored 95%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_mini_jamba_2_ultra_measures",
          "source": "benchmark:gsm8k_mini",
          "target": "model:jamba_2_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_5311",
              "url": "https://theverge.com/2026/01/25/gsm8k_mini_jamba_2_ultra",
              "published": "2026-01-25",
              "snippet": "The GSM8K Mini benchmark measures Jamba 2 Ultra across multiple tasks..."
            },
            {
              "docId": "2026-01-25_nextgov_1619",
              "url": "https://nextgov.com/2026/01/25/gsm8k_mini_jamba_2_ultra",
              "published": "2026-01-25",
              "snippet": "GSM8K Mini provides standardized evaluation of Jamba 2 Ultra..."
            },
            {
              "docId": "2026-01-25_openai_blog_8263",
              "url": "https://openai.com/blog/2026/01/25/gsm8k_mini_jamba_2_ultra",
              "published": "2026-01-25",
              "snippet": "The GSM8K Mini benchmark measures Jamba 2 Ultra across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_lite_multimodal_fusion_max_uses_tech",
          "source": "tool:mlflow_lite",
          "target": "tech:multimodal_fusion_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-04_hugging_face_bl_3616",
              "url": "https://huggingface.co/blog/2026/01/04/mlflow_lite_multimodal_fusion_",
              "published": "2026-01-04",
              "snippet": "MLflow Lite leverages Multimodal Fusion Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-14_nextgov_6625",
              "url": "https://nextgov.com/2026/01/14/mlflow_lite_multimodal_fusion_",
              "published": "2026-01-14",
              "snippet": "Under the hood, MLflow Lite implements Multimodal Fusion Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_constitutional_ai_next_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:localai_plus_dpo_max_uses_tech",
          "source": "repo:localai_plus",
          "target": "tech:dpo_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-22_openai_blog_9808",
              "url": "https://openai.com/blog/2025/12/22/localai_plus_dpo_max",
              "published": "2025-12-22",
              "snippet": "Under the hood, LocalAI Plus implements DPO Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_max_nemotron_5_mini_integrates_with",
          "source": "tool:copilot_max",
          "target": "model:nemotron_5_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_1524",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/copilot_max_nemotron_5_mini",
              "published": "2026-01-25",
              "snippet": "The latest release of Copilot Max adds native Nemotron-5 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_pro_gguf_mini_uses_tech",
          "source": "tool:flowise_pro",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_codex_2_v2_integrates_with",
          "source": "repo:text_generation_webui_next",
          "target": "model:codex_2_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_gemini_ultra_2_lite_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:gemini_ultra_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-13_arxiv_1969",
              "url": "https://arxiv.org/abs/2026/01/13/textbooks_are_all_you_need_min",
              "published": "2026-01-13",
              "snippet": "Textbooks Are All You Need Mini achieves 71% on Gemini Ultra 2 Lite, setting a new record..."
            },
            {
              "docId": "2026-01-20_bloomberg_3356",
              "url": "https://bloomberg.com/technology/2026/01/20/textbooks_are_all_you_need_min",
              "published": "2026-01-20",
              "snippet": "On the Gemini Ultra 2 Lite benchmark, Textbooks Are All You Need Mini scored 84%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_pro_sliding_window_attention_plus_uses_tech",
          "source": "tool:langchain_pro",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_grok_3_ultra_depends_on",
          "source": "model:jamba_2_core",
          "target": "model:grok_3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:localai_edge_dpo_mini_uses_tech",
          "source": "tool:localai_edge",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-11_anthropic_blog_1000",
              "url": "https://anthropic.com/news/2026/01/11/localai_edge_dpo_mini",
              "published": "2026-01-11",
              "snippet": "Technical details reveal LocalAI Edge relies heavily on DPO Mini..."
            },
            {
              "docId": "2026-01-20_wired_2880",
              "url": "https://wired.com/2026/01/20/localai_edge_dpo_mini",
              "published": "2026-01-20",
              "snippet": "Under the hood, LocalAI Edge implements DPO Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-21_microsoft_resea_1997",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/localai_edge_dpo_mini",
              "published": "2026-01-21",
              "snippet": "LocalAI Edge leverages DPO Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_mini_starcoder_data_v2_trained_on",
          "source": "model:claude_sonnet_4_mini",
          "target": "dataset:starcoder_data_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:yi_large_constitutional_ai_uses_tech",
          "source": "model:yi_large",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-08_venturebeat_6983",
              "url": "https://venturebeat.com/2026/01/08/yi_large_constitutional_ai",
              "published": "2026-01-08",
              "snippet": "Under the hood, Yi-Large implements Constitutional AI for improved efficiency..."
            },
            {
              "docId": "2026-01-25_techcrunch_2345",
              "url": "https://techcrunch.com/2026/01/25/yi_large_constitutional_ai",
              "published": "2026-01-25",
              "snippet": "Under the hood, Yi-Large implements Constitutional AI for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_tree_of_thought_uses_tech",
          "source": "model:sora_2_plus",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_hellaswag_plus_evaluated_on",
          "source": "model:gemini_ultra_2",
          "target": "benchmark:hellaswag_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:mmlu_plus_gpt_5_lite_measures",
          "source": "benchmark:mmlu_plus",
          "target": "model:gpt_5_lite",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_aya_3_pro_integrates_with",
          "source": "repo:text_generation_webui_pro",
          "target": "model:aya_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-22_langchain_blog_5417",
              "url": "https://blog.langchain.dev/2026/01/22/text_generation_webui_pro_aya_",
              "published": "2026-01-22",
              "snippet": "text-generation-webui Pro now supports Aya 3 Pro with full feature parity..."
            },
            {
              "docId": "2026-01-23_reuters_8174",
              "url": "https://reuters.com/technology/2026/01/23/text_generation_webui_pro_aya_",
              "published": "2026-01-23",
              "snippet": "The latest release of text-generation-webui Pro adds native Aya 3 Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_codex_2_mini_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:codex_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_bigbench_hard_max_evaluated_on",
          "source": "model:aya_3_max",
          "target": "benchmark:bigbench_hard_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_palm_3_v2_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:palm_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_mini_transformer_architecture_mini_uses_tech",
          "source": "dataset:the_stack_v2_mini",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:codex_2_plus_stable_diffusion_4_max_depends_on",
          "source": "model:codex_2_plus",
          "target": "model:stable_diffusion_4_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:autogpt_gemma_3_edge_integrates_with",
          "source": "repo:autogpt",
          "target": "model:gemma_3_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-18_anthropic_blog_3090",
              "url": "https://anthropic.com/news/2026/01/18/autogpt_gemma_3_edge",
              "published": "2026-01-18",
              "snippet": "AutoGPT announced official support for Gemma 3 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_speculative_decoding_lite_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_3363",
              "url": "https://wandb.ai/articles/2026/01/25/llm_agents:_a_survey_core_spec",
              "published": "2026-01-25",
              "snippet": "LLM Agents: A Survey Core leverages Speculative Decoding Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_codex_2_lite_depends_on",
          "source": "model:gpt_5_v2",
          "target": "model:codex_2_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_gpt_4o_mini_2_max_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:gpt_4o_mini_2_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:humaneval_pro_grok_3_v2_measures",
          "source": "benchmark:humaneval_pro",
          "target": "model:grok_3_v2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.97
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_ultra_common_crawl_trained_on",
          "source": "model:whisper_v4_ultra",
          "target": "dataset:common_crawl",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_mlflow_mini_integrates_with",
          "source": "tool:litellm_pro",
          "target": "tool:mlflow_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_1382",
              "url": "https://blogs.nvidia.com/2026/01/25/litellm_pro_mlflow_mini",
              "published": "2026-01-25",
              "snippet": "LiteLLM Pro now supports MLflow Mini with full feature parity..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_6414",
              "url": "https://blog.google/technology/ai/2026/01/25/litellm_pro_mlflow_mini",
              "published": "2026-01-25",
              "snippet": "LiteLLM Pro now supports MLflow Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_edge_sliding_window_attention_lite_uses_tech",
          "source": "dataset:common_crawl_edge",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_mini_palm_3_edge_measures",
          "source": "benchmark:truthfulqa_mini",
          "target": "model:palm_3_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-23_openai_blog_2717",
              "url": "https://openai.com/blog/2026/01/23/truthfulqa_mini_palm_3_edge",
              "published": "2026-01-23",
              "snippet": "TruthfulQA Mini provides standardized evaluation of PaLM 3 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_max_yi_large_lite_depends_on",
          "source": "model:falcon_3_max",
          "target": "model:yi_large_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_quantization_v2_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:copilot_max_nemotron_5_next_integrates_with",
          "source": "tool:copilot_max",
          "target": "model:nemotron_5_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-25_bloomberg_8003",
              "url": "https://bloomberg.com/technology/2026/01/25/copilot_max_nemotron_5_next",
              "published": "2026-01-25",
              "snippet": "Copilot Max now supports Nemotron-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-25_venturebeat_1152",
              "url": "https://venturebeat.com/2026/01/25/copilot_max_nemotron_5_next",
              "published": "2026-01-25",
              "snippet": "Copilot Max announced official support for Nemotron-5 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_mini_command_r_plus_depends_on",
          "source": "model:nemotron_5_mini",
          "target": "model:command_r_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:humaneval_mini_deepseek_v3_mini_measures",
          "source": "benchmark:humaneval_mini",
          "target": "model:deepseek_v3_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-15_the_verge_9735",
              "url": "https://theverge.com/2026/01/15/humaneval_mini_deepseek_v3_min",
              "published": "2026-01-15",
              "snippet": "HumanEval Mini has become the standard for evaluating DeepSeek-V3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_plus_flash_attention_max_uses_tech",
          "source": "dataset:the_stack_v2_plus",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_codex_2_pro_depends_on",
          "source": "model:whisper_v4",
          "target": "model:codex_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_quantization_v2_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-11_hugging_face_bl_3405",
              "url": "https://huggingface.co/blog/2026/01/11/direct_preference_optimization",
              "published": "2026-01-11",
              "snippet": "Direct Preference Optimization Next leverages Quantization v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_mit_technology__5404",
              "url": "https://technologyreview.com/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "Direct Preference Optimization Next leverages Quantization v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_4833",
              "url": "https://huggingface.co/blog/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Direct Preference Optimization Next leverages Quantization v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_multimodal_fusion_uses_tech",
          "source": "repo:llamacpp_v2",
          "target": "tech:multimodal_fusion",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_the_stack_v2_pro_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:the_stack_v2_pro",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_tokenizer_bpe_ultra_uses_tech",
          "source": "model:gemma_3_plus",
          "target": "tech:tokenizer_bpe_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:gemma_3_v2_mixture_of_experts_lite_uses_tech",
          "source": "model:gemma_3_v2",
          "target": "tech:mixture_of_experts_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_speculative_decoding_lite_uses_tech",
          "source": "paper:direct_preference_optimization_pro",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-20_the_verge_7850",
              "url": "https://theverge.com/2026/01/20/direct_preference_optimization",
              "published": "2026-01-20",
              "snippet": "Technical details reveal Direct Preference Optimization Pro relies heavily on Speculative Decoding Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_pro_jamba_2_v2_integrates_with",
          "source": "tool:haystack_pro",
          "target": "model:jamba_2_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-24_the_gradient_2235",
              "url": "https://thegradient.pub/2026/01/24/haystack_pro_jamba_2_v2",
              "published": "2026-01-24",
              "snippet": "The latest release of Haystack Pro adds native Jamba 2 v2 integration..."
            },
            {
              "docId": "2026-01-24_techcrunch_6229",
              "url": "https://techcrunch.com/2026/01/24/haystack_pro_jamba_2_v2",
              "published": "2026-01-24",
              "snippet": "Haystack Pro now supports Jamba 2 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-24_mit_technology__5294",
              "url": "https://technologyreview.com/2026/01/24/haystack_pro_jamba_2_v2",
              "published": "2026-01-24",
              "snippet": "Haystack Pro now supports Jamba 2 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_8346",
              "url": "https://huggingface.co/blog/2026/01/24/haystack_pro_jamba_2_v2",
              "published": "2026-01-24",
              "snippet": "The latest release of Haystack Pro adds native Jamba 2 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mbpp_core_qwen_3_lite_measures",
          "source": "benchmark:mbpp_core",
          "target": "model:qwen_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-01_meta_ai_blog_4228",
              "url": "https://ai.meta.com/blog/2026/01/01/mbpp_core_qwen_3_lite",
              "published": "2026-01-01",
              "snippet": "MBPP Core provides standardized evaluation of Qwen-3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_lite_phi_4_edge_integrates_with",
          "source": "tool:tensorrt_llm_lite",
          "target": "model:phi_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-03_wired_3079",
              "url": "https://wired.com/2025/12/03/tensorrt_llm_lite_phi_4_edge",
              "published": "2025-12-03",
              "snippet": "The latest release of TensorRT-LLM Lite adds native Phi-4 Edge integration..."
            },
            {
              "docId": "2026-01-18_arxiv_2576",
              "url": "https://arxiv.org/abs/2026/01/18/tensorrt_llm_lite_phi_4_edge",
              "published": "2026-01-18",
              "snippet": "TensorRT-LLM Lite announced official support for Phi-4 Edge..."
            },
            {
              "docId": "2026-01-23_openai_blog_1755",
              "url": "https://openai.com/blog/2026/01/23/tensorrt_llm_lite_phi_4_edge",
              "published": "2026-01-23",
              "snippet": "TensorRT-LLM Lite announced official support for Phi-4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_core_rlhf_next_uses_tech",
          "source": "repo:gpt4all_core",
          "target": "tech:rlhf_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_red_teaming_ultra_uses_tech",
          "source": "dataset:redpajama_v2",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:falcon_3_v2_fineweb_plus_trained_on",
          "source": "model:falcon_3_v2",
          "target": "dataset:fineweb_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2025-12-23_weights_and_bia_2304",
              "url": "https://wandb.ai/articles/2025/12/23/falcon_3_v2_fineweb_plus",
              "published": "2025-12-23",
              "snippet": "Falcon 3 v2 was trained on FineWeb Plus comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_nemotron_5_depends_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "model:nemotron_5",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_claude_sonnet_4_v2_depends_on",
          "source": "model:yi_large_next",
          "target": "model:claude_sonnet_4_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_speculative_decoding_max_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-01_meta_ai_blog_2679",
              "url": "https://ai.meta.com/blog/2026/01/01/llm_agents:_a_survey_core_spec",
              "published": "2026-01-01",
              "snippet": "Technical details reveal LLM Agents: A Survey Core relies heavily on Speculative Decoding Max..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_4798",
              "url": "https://blogs.nvidia.com/2026/01/19/llm_agents:_a_survey_core_spec",
              "published": "2026-01-19",
              "snippet": "Under the hood, LLM Agents: A Survey Core implements Speculative Decoding Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__tool_use_max_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_deepseek_v3_integrates_with",
          "source": "repo:llamacpp_ultra",
          "target": "model:deepseek_v3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_humaneval_pro_evaluated_on",
          "source": "model:gemma_3_lite",
          "target": "benchmark:humaneval_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-20_hugging_face_bl_9727",
              "url": "https://huggingface.co/blog/2026/01/20/gemma_3_lite_humaneval_pro",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Gemma 3 Lite reaching 74% on HumanEval Pro..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_5848",
              "url": "https://anthropic.com/news/2026/01/25/gemma_3_lite_humaneval_pro",
              "published": "2026-01-25",
              "snippet": "On the HumanEval Pro benchmark, Gemma 3 Lite scored 78%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_mixture_of_experts_max_uses_tech",
          "source": "repo:text_generation_webui_max",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__tool_use_core_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-12_the_gradient_2516",
              "url": "https://thegradient.pub/2026/01/12/constitutional_ai:_harmlessnes",
              "published": "2026-01-12",
              "snippet": "Under the hood, Constitutional AI: Harmlessness from AI Feedback implements Tool Use Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_cody_ultra_integrates_with",
          "source": "tool:flowise_plus",
          "target": "tool:cody_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_nemotron_5_mini_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:nemotron_5_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_4601",
              "url": "https://wandb.ai/articles/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "On the Nemotron-5 Mini benchmark, Chain-of-Thought Prompting Elicits Reasoning scored 75%..."
            },
            {
              "docId": "2026-01-25_openai_blog_8147",
              "url": "https://openai.com/blog/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "On the Nemotron-5 Mini benchmark, Chain-of-Thought Prompting Elicits Reasoning scored 86%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_max_transformer_architecture_mini_uses_tech",
          "source": "dataset:mc4_max",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:localai_pro_codex_2_edge_integrates_with",
          "source": "tool:localai_pro",
          "target": "model:codex_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-08_weights_and_bia_4218",
              "url": "https://wandb.ai/articles/2026/01/08/localai_pro_codex_2_edge",
              "published": "2026-01-08",
              "snippet": "LocalAI Pro now supports Codex 2 Edge with full feature parity..."
            },
            {
              "docId": "2026-01-25_openai_blog_8241",
              "url": "https://openai.com/blog/2026/01/25/localai_pro_codex_2_edge",
              "published": "2026-01-25",
              "snippet": "The latest release of LocalAI Pro adds native Codex 2 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_rlhf_ultra_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__1384",
              "url": "https://technologyreview.com/2026/01/25/text_generation_webui_next_rlh",
              "published": "2026-01-25",
              "snippet": "Under the hood, text-generation-webui Next implements RLHF Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_techcrunch_9685",
              "url": "https://techcrunch.com/2026/01/25/text_generation_webui_next_rlh",
              "published": "2026-01-25",
              "snippet": "Under the hood, text-generation-webui Next implements RLHF Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_9785",
              "url": "https://blog.google/technology/ai/2026/01/25/text_generation_webui_next_rlh",
              "published": "2026-01-25",
              "snippet": "Under the hood, text-generation-webui Next implements RLHF Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_core_quantization_max_uses_tech",
          "source": "model:llama_4_core",
          "target": "tech:quantization_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-19_reuters_9766",
              "url": "https://reuters.com/technology/2026/01/19/llama_4_core_quantization_max",
              "published": "2026-01-19",
              "snippet": "Technical details reveal Llama 4 Core relies heavily on Quantization Max..."
            },
            {
              "docId": "2026-01-19_wired_8028",
              "url": "https://wired.com/2026/01/19/llama_4_core_quantization_max",
              "published": "2026-01-19",
              "snippet": "Under the hood, Llama 4 Core implements Quantization Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_lite_langchain_v2_integrates_with",
          "source": "tool:haystack_lite",
          "target": "tool:langchain_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:vllm_next_claude_sonnet_4_mini_integrates_with",
          "source": "tool:vllm_next",
          "target": "model:claude_sonnet_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-17_reuters_4628",
              "url": "https://reuters.com/technology/2026/01/17/vllm_next_claude_sonnet_4_mini",
              "published": "2026-01-17",
              "snippet": "vLLM Next now supports Claude Sonnet 4 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_chain_of_thought_edge_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:chain_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-06_ars_technica_5644",
              "url": "https://arstechnica.com/2026/01/06/text_generation_webui_next_cha",
              "published": "2026-01-06",
              "snippet": "Technical details reveal text-generation-webui Next relies heavily on Chain-of-Thought Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_mini_dolma_core_trained_on",
          "source": "model:dall_e_4_mini",
          "target": "dataset:dolma_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_lora_lite_uses_tech",
          "source": "model:sora_2_edge",
          "target": "tech:lora_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:jamba_2_phi_4_max_depends_on",
          "source": "model:jamba_2",
          "target": "model:phi_4_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_grok_3_ultra_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:grok_3_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_distillation_next_uses_tech",
          "source": "model:midjourney_v7_core",
          "target": "tech:distillation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_qwen_3_ultra_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:qwen_3_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-05_nextgov_6535",
              "url": "https://nextgov.com/2026/01/05/flash_attention:_fast_and_memo",
              "published": "2026-01-05",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 86% on Qwen-3 Ultra..."
            },
            {
              "docId": "2026-01-15_anthropic_blog_2557",
              "url": "https://anthropic.com/news/2026/01/15/flash_attention:_fast_and_memo",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 84% on Qwen-3 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_plus_gemma_3_max_depends_on",
          "source": "model:dall_e_4_plus",
          "target": "model:gemma_3_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:dify_edge_group_query_attention_uses_tech",
          "source": "tool:dify_edge",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:langchain_next_nemotron_5_plus_integrates_with",
          "source": "tool:langchain_next",
          "target": "model:nemotron_5_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_v2_grok_3_lite_depends_on",
          "source": "model:whisper_v4_v2",
          "target": "model:grok_3_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_gradio_plus_integrates_with",
          "source": "tool:mlflow_mini",
          "target": "tool:gradio_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_grok_3_plus_depends_on",
          "source": "model:command_r_plus_edge",
          "target": "model:grok_3_plus",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_phi_4_integrates_with",
          "source": "repo:gpt4all_v2",
          "target": "model:phi_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-24_hugging_face_bl_2108",
              "url": "https://huggingface.co/blog/2026/01/24/gpt4all_v2_phi_4",
              "published": "2026-01-24",
              "snippet": "gpt4all v2 announced official support for Phi-4..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_3988",
              "url": "https://huggingface.co/blog/2026/01/25/gpt4all_v2_phi_4",
              "published": "2026-01-25",
              "snippet": "gpt4all v2 now supports Phi-4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_plus_the_pile_core_trained_on",
          "source": "model:dall_e_4_plus",
          "target": "dataset:the_pile_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:slimpajama_mini_tree_of_thought_v2_uses_tech",
          "source": "dataset:slimpajama_mini",
          "target": "tech:tree_of_thought_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_v2_gemma_3_lite_measures",
          "source": "benchmark:alpacaeval_2_v2",
          "target": "model:gemma_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-16_the_gradient_3902",
              "url": "https://thegradient.pub/2026/01/16/alpacaeval_2_v2_gemma_3_lite",
              "published": "2026-01-16",
              "snippet": "The AlpacaEval 2 v2 benchmark measures Gemma 3 Lite across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_mc4_v2_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:mc4_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_ultra_dpo_v2_uses_tech",
          "source": "model:midjourney_v7_ultra",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_rotary_position_embedding_plus_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-22_mit_technology__1572",
              "url": "https://technologyreview.com/2026/01/22/llm_agents:_a_survey_core_rota",
              "published": "2026-01-22",
              "snippet": "Technical details reveal LLM Agents: A Survey Core relies heavily on Rotary Position Embedding Plus..."
            },
            {
              "docId": "2026-01-24_openai_blog_3374",
              "url": "https://openai.com/blog/2026/01/24/llm_agents:_a_survey_core_rota",
              "published": "2026-01-24",
              "snippet": "LLM Agents: A Survey Core leverages Rotary Position Embedding Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_core_nemotron_5_mini_integrates_with",
          "source": "tool:dify_core",
          "target": "model:nemotron_5_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:jamba_2_max_gguf_mini_uses_tech",
          "source": "model:jamba_2_max",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:langchain_core_mixtral_8x22b_mini_integrates_with",
          "source": "tool:langchain_core",
          "target": "model:mixtral_8x22b_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_lite_quantization_max_uses_tech",
          "source": "tool:tensorrt_llm_lite",
          "target": "tech:quantization_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_hellaswag_max_evaluated_on",
          "source": "model:grok_3_plus",
          "target": "benchmark:hellaswag_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-21_nvidia_blog_1179",
              "url": "https://blogs.nvidia.com/2026/01/21/grok_3_plus_hellaswag_max",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Grok-3 Plus reaching 80% on HellaSwag Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_edge_midjourney_v7_measures",
          "source": "benchmark:bigbench_hard_edge",
          "target": "model:midjourney_v7",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-30_reuters_4486",
              "url": "https://reuters.com/technology/2025/12/30/bigbench_hard_edge_midjourney_",
              "published": "2025-12-30",
              "snippet": "BigBench Hard Edge provides standardized evaluation of Midjourney V7..."
            },
            {
              "docId": "2026-01-16_openai_blog_4659",
              "url": "https://openai.com/blog/2026/01/16/bigbench_hard_edge_midjourney_",
              "published": "2026-01-16",
              "snippet": "The BigBench Hard Edge benchmark measures Midjourney V7 across multiple tasks..."
            },
            {
              "docId": "2026-01-20_nextgov_6032",
              "url": "https://nextgov.com/2026/01/20/bigbench_hard_edge_midjourney_",
              "published": "2026-01-20",
              "snippet": "BigBench Hard Edge provides standardized evaluation of Midjourney V7..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_dpo_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:dpo_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_gguf_mini_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-30_meta_ai_blog_5073",
              "url": "https://ai.meta.com/blog/2025/12/30/direct_preference_optimization",
              "published": "2025-12-30",
              "snippet": "Direct Preference Optimization Next leverages GGUF Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_falcon_3_next_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:falcon_3_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-22_nextgov_8353",
              "url": "https://nextgov.com/2026/01/22/llm_agents:_a_survey_falcon_3_",
              "published": "2026-01-22",
              "snippet": "On the Falcon 3 Next benchmark, LLM Agents: A Survey scored 97%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_core_the_stack_v2_trained_on",
          "source": "model:deepseek_v3_core",
          "target": "dataset:the_stack_v2",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_yi_large_ultra_integrates_with",
          "source": "repo:text_generation_webui",
          "target": "model:yi_large_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_gpt_5_ultra_depends_on",
          "source": "model:gemma_3_lite",
          "target": "model:gpt_5_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:langchain_max_deepseek_v3_lite_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:deepseek_v3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-02_langchain_blog_4749",
              "url": "https://blog.langchain.dev/2025/12/02/langchain_max_deepseek_v3_lite",
              "published": "2025-12-02",
              "snippet": "langchain Max now supports DeepSeek-V3 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_streamlit_integrates_with",
          "source": "tool:mlflow_v2",
          "target": "tool:streamlit",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-11_ars_technica_8948",
              "url": "https://arstechnica.com/2026/01/11/mlflow_v2_streamlit",
              "published": "2026-01-11",
              "snippet": "MLflow v2 announced official support for Streamlit..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_dpo_mini_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-17_google_ai_blog_6131",
              "url": "https://blog.google/technology/ai/2026/01/17/mixture_of_experts_meets_instr",
              "published": "2026-01-17",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements DPO Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-17_bloomberg_2472",
              "url": "https://bloomberg.com/technology/2026/01/17/mixture_of_experts_meets_instr",
              "published": "2026-01-17",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements DPO Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-20_anthropic_blog_8386",
              "url": "https://anthropic.com/news/2026/01/20/mixture_of_experts_meets_instr",
              "published": "2026-01-20",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements DPO Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_pro_speculative_decoding_lite_uses_tech",
          "source": "model:sora_2_pro",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_gpt_4o_mini_2_ultra_integrates_with",
          "source": "repo:open_interpreter_v2",
          "target": "model:gpt_4o_mini_2_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-05_meta_ai_blog_3485",
              "url": "https://ai.meta.com/blog/2026/01/05/open_interpreter_v2_gpt_4o_min",
              "published": "2026-01-05",
              "snippet": "open-interpreter v2 now supports GPT-4o Mini 2 Ultra with full feature parity..."
            },
            {
              "docId": "2026-01-21_microsoft_resea_1035",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/open_interpreter_v2_gpt_4o_min",
              "published": "2026-01-21",
              "snippet": "The latest release of open-interpreter v2 adds native GPT-4o Mini 2 Ultra integration..."
            },
            {
              "docId": "2026-01-24_langchain_blog_9177",
              "url": "https://blog.langchain.dev/2026/01/24/open_interpreter_v2_gpt_4o_min",
              "published": "2026-01-24",
              "snippet": "open-interpreter v2 now supports GPT-4o Mini 2 Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_flash_attention_edge_uses_tech",
          "source": "tool:litellm_mini",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-18_mit_technology__2149",
              "url": "https://technologyreview.com/2026/01/18/litellm_mini_flash_attention_e",
              "published": "2026-01-18",
              "snippet": "LiteLLM Mini leverages Flash Attention Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_flowise_plus_integrates_with",
          "source": "tool:cursor_v2",
          "target": "tool:flowise_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-20_nextgov_6522",
              "url": "https://nextgov.com/2026/01/20/cursor_v2_flowise_plus",
              "published": "2026-01-20",
              "snippet": "The latest release of Cursor v2 adds native Flowise Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_plus_tool_use_uses_tech",
          "source": "tool:copilot_plus",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:laion_5b_core_kv_cache_optimization_uses_tech",
          "source": "dataset:laion_5b_core",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_speculative_decoding_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-18_wired_1631",
              "url": "https://wired.com/2026/01/18/attention_is_all_you_need_v2_s",
              "published": "2026-01-18",
              "snippet": "Attention Is All You Need v2 leverages Speculative Decoding Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_transformer_architecture_core_uses_tech",
          "source": "model:palm_3_lite",
          "target": "tech:transformer_architecture_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:common_crawl_mini_transformer_architecture_plus_uses_tech",
          "source": "dataset:common_crawl_mini",
          "target": "tech:transformer_architecture_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:qwen_3_core_transformer_architecture_lite_uses_tech",
          "source": "model:qwen_3_core",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_4967",
              "url": "https://techcrunch.com/2026/01/23/qwen_3_core_transformer_archit",
              "published": "2026-01-23",
              "snippet": "Under the hood, Qwen-3 Core implements Transformer Architecture Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-23_nextgov_8101",
              "url": "https://nextgov.com/2026/01/23/qwen_3_core_transformer_archit",
              "published": "2026-01-23",
              "snippet": "Under the hood, Qwen-3 Core implements Transformer Architecture Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-23_microsoft_resea_4264",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/qwen_3_core_transformer_archit",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Qwen-3 Core relies heavily on Transformer Architecture Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_lite_gpqa_pro_evaluated_on",
          "source": "model:yi_large_lite",
          "target": "benchmark:gpqa_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-23_bloomberg_4193",
              "url": "https://bloomberg.com/technology/2026/01/23/yi_large_lite_gpqa_pro",
              "published": "2026-01-23",
              "snippet": "Yi-Large Lite achieves 81% on GPQA Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_v2_flash_attention_max_uses_tech",
          "source": "dataset:slimpajama_v2",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_c4_core_trained_on",
          "source": "model:grok_3_next",
          "target": "dataset:c4_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-12_meta_ai_blog_3377",
              "url": "https://ai.meta.com/blog/2026/01/12/grok_3_next_c4_core",
              "published": "2026-01-12",
              "snippet": "Grok-3 Next was trained on C4 Core comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_ultra_tensorrt_llm_lite_integrates_with",
          "source": "tool:copilot_ultra",
          "target": "tool:tensorrt_llm_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_codex_2_edge_depends_on",
          "source": "model:jamba_2_core",
          "target": "model:codex_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_red_teaming_core_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:red_teaming_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-02_the_gradient_3587",
              "url": "https://thegradient.pub/2026/01/02/toolformer:_language_models_ca",
              "published": "2026-01-02",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements Red Teaming Core for improved efficiency..."
            },
            {
              "docId": "2026-01-16_the_verge_6917",
              "url": "https://theverge.com/2026/01/16/toolformer:_language_models_ca",
              "published": "2026-01-16",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements Red Teaming Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_dall_e_4_plus_integrates_with",
          "source": "repo:text_generation_webui",
          "target": "model:dall_e_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_synthetic_data_generation_mini_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_group_query_attention_mini_uses_tech",
          "source": "repo:text_generation_webui",
          "target": "tech:group_query_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:mbpp_lite_phi_4_plus_measures",
          "source": "benchmark:mbpp_lite",
          "target": "model:phi_4_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-21_mit_technology__3147",
              "url": "https://technologyreview.com/2026/01/21/mbpp_lite_phi_4_plus",
              "published": "2026-01-21",
              "snippet": "MBPP Lite provides standardized evaluation of Phi-4 Plus..."
            },
            {
              "docId": "2026-01-23_microsoft_resea_7069",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/mbpp_lite_phi_4_plus",
              "published": "2026-01-23",
              "snippet": "The MBPP Lite benchmark measures Phi-4 Plus across multiple tasks..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_6784",
              "url": "https://huggingface.co/blog/2026/01/25/mbpp_lite_phi_4_plus",
              "published": "2026-01-25",
              "snippet": "MBPP Lite has become the standard for evaluating Phi-4 Plus..."
            },
            {
              "docId": "2026-01-25_nextgov_6825",
              "url": "https://nextgov.com/2026/01/25/mbpp_lite_phi_4_plus",
              "published": "2026-01-25",
              "snippet": "MBPP Lite provides standardized evaluation of Phi-4 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_tree_of_thought_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "tech:tree_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-19_hugging_face_bl_1232",
              "url": "https://huggingface.co/blog/2026/01/19/attention_is_all_you_need_v2_v",
              "published": "2026-01-19",
              "snippet": "Attention Is All You Need v2 v2 leverages Tree of Thought Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_v2_sora_2_plus_integrates_with",
          "source": "tool:langchain_v2",
          "target": "model:sora_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_red_teaming_lite_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_codex_2_next_evaluated_on",
          "source": "paper:direct_preference_optimization_edge",
          "target": "model:codex_2_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2025-12-23_microsoft_resea_1445",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/23/direct_preference_optimization",
              "published": "2025-12-23",
              "snippet": "Direct Preference Optimization Edge achieves 74% on Codex 2 Next, setting a new record..."
            },
            {
              "docId": "2026-01-23_arxiv_1347",
              "url": "https://arxiv.org/abs/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Direct Preference Optimization Edge reaching 72% on Codex 2 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_v2_speculative_decoding_core_uses_tech",
          "source": "dataset:laion_5b_v2",
          "target": "tech:speculative_decoding_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.4
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_gemini_ultra_2_lite_integrates_with",
          "source": "tool:ollama_lite",
          "target": "model:gemini_ultra_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-22_techcrunch_8301",
              "url": "https://techcrunch.com/2026/01/22/ollama_lite_gemini_ultra_2_lit",
              "published": "2026-01-22",
              "snippet": "Ollama Lite now supports Gemini Ultra 2 Lite with full feature parity..."
            },
            {
              "docId": "2026-01-24_nextgov_5420",
              "url": "https://nextgov.com/2026/01/24/ollama_lite_gemini_ultra_2_lit",
              "published": "2026-01-24",
              "snippet": "The latest release of Ollama Lite adds native Gemini Ultra 2 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_the_stack_v2_trained_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "dataset:the_stack_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-29_arxiv_9875",
              "url": "https://arxiv.org/abs/2025/12/29/stable_diffusion_4_v2_the_stac",
              "published": "2025-12-29",
              "snippet": "The training corpus for Stable Diffusion 4 v2 includes The Stack v2..."
            },
            {
              "docId": "2026-01-04_meta_ai_blog_4955",
              "url": "https://ai.meta.com/blog/2026/01/04/stable_diffusion_4_v2_the_stac",
              "published": "2026-01-04",
              "snippet": "Stable Diffusion 4 v2 was trained on The Stack v2 comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_pro_constitutional_ai_edge_uses_tech",
          "source": "tool:weights_and_biases_pro",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:cursor_core_cody_pro_integrates_with",
          "source": "tool:cursor_core",
          "target": "tool:cody_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_plus_gpt_5_next_measures",
          "source": "benchmark:alpacaeval_2_plus",
          "target": "model:gpt_5_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_7221",
              "url": "https://venturebeat.com/2026/01/23/alpacaeval_2_plus_gpt_5_next",
              "published": "2026-01-23",
              "snippet": "AlpacaEval 2 Plus has become the standard for evaluating GPT-5 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_flowise_integrates_with",
          "source": "tool:litellm_pro",
          "target": "tool:flowise",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-20_arxiv_3825",
              "url": "https://arxiv.org/abs/2026/01/20/litellm_pro_flowise",
              "published": "2026-01-20",
              "snippet": "LiteLLM Pro now supports Flowise with full feature parity..."
            },
            {
              "docId": "2026-01-20_the_gradient_6723",
              "url": "https://thegradient.pub/2026/01/20/litellm_pro_flowise",
              "published": "2026-01-20",
              "snippet": "The latest release of LiteLLM Pro adds native Flowise integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_plus_dpo_lite_uses_tech",
          "source": "model:phi_4_plus",
          "target": "tech:dpo_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:copilot_next_cody_integrates_with",
          "source": "tool:copilot_next",
          "target": "tool:cody",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-17_openai_blog_3495",
              "url": "https://openai.com/blog/2026/01/17/copilot_next_cody",
              "published": "2026-01-17",
              "snippet": "Copilot Next now supports Cody with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_tool_use_plus_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:tool_use_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_7510",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Mini leverages Tool Use Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_langchain_v2_integrates_with",
          "source": "tool:localai",
          "target": "tool:langchain_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:crewai_max_grok_3_max_integrates_with",
          "source": "tool:crewai_max",
          "target": "model:grok_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-01_bloomberg_9637",
              "url": "https://bloomberg.com/technology/2026/01/01/crewai_max_grok_3_max",
              "published": "2026-01-01",
              "snippet": "CrewAI Max announced official support for Grok-3 Max..."
            },
            {
              "docId": "2026-01-02_the_verge_3494",
              "url": "https://theverge.com/2026/01/02/crewai_max_grok_3_max",
              "published": "2026-01-02",
              "snippet": "CrewAI Max now supports Grok-3 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_ultra_gsm8k_core_evaluated_on",
          "source": "model:sora_2_ultra",
          "target": "benchmark:gsm8k_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_4015",
              "url": "https://openai.com/blog/2026/01/25/sora_2_ultra_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "On the GSM8K Core benchmark, Sora 2 Ultra scored 84%..."
            },
            {
              "docId": "2026-01-25_techcrunch_8642",
              "url": "https://techcrunch.com/2026/01/25/sora_2_ultra_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "Sora 2 Ultra achieves 84% on GSM8K Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_synthetic_data_generation_uses_tech",
          "source": "dataset:openwebtext2",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_multimodal_fusion_v2_uses_tech",
          "source": "tool:copilot_edge",
          "target": "tech:multimodal_fusion_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-22_mit_technology__2736",
              "url": "https://technologyreview.com/2026/01/22/copilot_edge_multimodal_fusion",
              "published": "2026-01-22",
              "snippet": "Under the hood, Copilot Edge implements Multimodal Fusion v2 for improved efficiency..."
            },
            {
              "docId": "2026-01-23_ars_technica_6566",
              "url": "https://arstechnica.com/2026/01/23/copilot_edge_multimodal_fusion",
              "published": "2026-01-23",
              "snippet": "Copilot Edge leverages Multimodal Fusion v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_reuters_2751",
              "url": "https://reuters.com/technology/2026/01/24/copilot_edge_multimodal_fusion",
              "published": "2026-01-24",
              "snippet": "Copilot Edge leverages Multimodal Fusion v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_7918",
              "url": "https://wandb.ai/articles/2026/01/25/copilot_edge_multimodal_fusion",
              "published": "2026-01-25",
              "snippet": "Copilot Edge leverages Multimodal Fusion v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_refinedweb_lite_trained_on",
          "source": "model:gemma_3_lite",
          "target": "dataset:refinedweb_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_mt_bench_pro_evaluated_on",
          "source": "model:whisper_v4_core",
          "target": "benchmark:mt_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_1374",
              "url": "https://venturebeat.com/2026/01/23/whisper_v4_core_mt_bench_pro",
              "published": "2026-01-23",
              "snippet": "Whisper v4 Core achieves 96% on MT-Bench Pro, setting a new record..."
            },
            {
              "docId": "2026-01-24_ars_technica_5803",
              "url": "https://arstechnica.com/2026/01/24/whisper_v4_core_mt_bench_pro",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Whisper v4 Core reaching 87% on MT-Bench Pro..."
            },
            {
              "docId": "2026-01-24_openai_blog_9293",
              "url": "https://openai.com/blog/2026/01/24/whisper_v4_core_mt_bench_pro",
              "published": "2026-01-24",
              "snippet": "On the MT-Bench Pro benchmark, Whisper v4 Core scored 88%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:roots_v2_rlhf_core_uses_tech",
          "source": "dataset:roots_v2",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:gpt_5_next_gemini_ultra_2_depends_on",
          "source": "model:gpt_5_next",
          "target": "model:gemini_ultra_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:codex_2_mini_hellaswag_mini_evaluated_on",
          "source": "model:codex_2_mini",
          "target": "benchmark:hellaswag_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-20_meta_ai_blog_4045",
              "url": "https://ai.meta.com/blog/2026/01/20/codex_2_mini_hellaswag_mini",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Codex 2 Mini reaching 94% on HellaSwag Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_transformer_architecture_lite_uses_tech",
          "source": "model:gpt_5_v2",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:mbpp_jamba_2_next_measures",
          "source": "benchmark:mbpp",
          "target": "model:jamba_2_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_9511",
              "url": "https://arxiv.org/abs/2026/01/24/mbpp_jamba_2_next",
              "published": "2026-01-24",
              "snippet": "MBPP provides standardized evaluation of Jamba 2 Next..."
            },
            {
              "docId": "2026-01-24_openai_blog_9410",
              "url": "https://openai.com/blog/2026/01/24/mbpp_jamba_2_next",
              "published": "2026-01-24",
              "snippet": "The MBPP benchmark measures Jamba 2 Next across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_rotary_position_embedding_ultra_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-24_bloomberg_8581",
              "url": "https://bloomberg.com/technology/2026/01/24/attention_is_all_you_need_v2_r",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Attention Is All You Need v2 relies heavily on Rotary Position Embedding Ultra..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_3359",
              "url": "https://wandb.ai/articles/2026/01/25/attention_is_all_you_need_v2_r",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 relies heavily on Rotary Position Embedding Ultra..."
            },
            {
              "docId": "2026-01-25_techcrunch_1933",
              "url": "https://techcrunch.com/2026/01/25/attention_is_all_you_need_v2_r",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 relies heavily on Rotary Position Embedding Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_core_sora_2_mini_integrates_with",
          "source": "tool:copilot_core",
          "target": "model:sora_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-23_nextgov_3798",
              "url": "https://nextgov.com/2026/01/23/copilot_core_sora_2_mini",
              "published": "2026-01-23",
              "snippet": "The latest release of Copilot Core adds native Sora 2 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_claude_sonnet_4_next_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:claude_sonnet_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-20_ars_technica_6082",
              "url": "https://arstechnica.com/2026/01/20/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Tree of Thoughts: Deliberate Problem Solving with LLMs reaching 85% on Claude Sonnet 4 Next..."
            },
            {
              "docId": "2026-01-21_wired_9869",
              "url": "https://wired.com/2026/01/21/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-21",
              "snippet": "Tree of Thoughts: Deliberate Problem Solving with LLMs achieves 79% on Claude Sonnet 4 Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_tree_of_thought_next_uses_tech",
          "source": "model:llama_4_plus",
          "target": "tech:tree_of_thought_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_lite_dpo_uses_tech",
          "source": "tool:tensorrt_llm_lite",
          "target": "tech:dpo",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:sora_2_next_humaneval_pro_evaluated_on",
          "source": "model:sora_2_next",
          "target": "benchmark:humaneval_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-17_hugging_face_bl_3519",
              "url": "https://huggingface.co/blog/2026/01/17/sora_2_next_humaneval_pro",
              "published": "2026-01-17",
              "snippet": "Sora 2 Next achieves 70% on HumanEval Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_mini_quantization_v2_uses_tech",
          "source": "tool:semantic_kernel_mini",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2025-12-31_venturebeat_5283",
              "url": "https://venturebeat.com/2025/12/31/semantic_kernel_mini_quantizat",
              "published": "2025-12-31",
              "snippet": "Semantic Kernel Mini leverages Quantization v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_5734",
              "url": "https://blogs.nvidia.com/2026/01/22/semantic_kernel_mini_quantizat",
              "published": "2026-01-22",
              "snippet": "Semantic Kernel Mini leverages Quantization v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_lite_gguf_mini_uses_tech",
          "source": "dataset:common_crawl_lite",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_claude_sonnet_4_v2_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:claude_sonnet_4_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-26_techcrunch_4943",
              "url": "https://techcrunch.com/2025/12/26/self_play_fine_tuning_for_lang",
              "published": "2025-12-26",
              "snippet": "On the Claude Sonnet 4 v2 benchmark, Self-Play Fine-Tuning for Language Models scored 73%..."
            },
            {
              "docId": "2025-12-28_ars_technica_2417",
              "url": "https://arstechnica.com/2025/12/28/self_play_fine_tuning_for_lang",
              "published": "2025-12-28",
              "snippet": "Self-Play Fine-Tuning for Language Models achieves 92% on Claude Sonnet 4 v2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_kv_cache_optimization_pro_uses_tech",
          "source": "repo:open_interpreter_pro",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-13_anthropic_blog_2414",
              "url": "https://anthropic.com/news/2026/01/13/open_interpreter_pro_kv_cache_",
              "published": "2026-01-13",
              "snippet": "Under the hood, open-interpreter Pro implements KV Cache Optimization Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-15_openai_blog_5569",
              "url": "https://openai.com/blog/2026/01/15/open_interpreter_pro_kv_cache_",
              "published": "2026-01-15",
              "snippet": "Technical details reveal open-interpreter Pro relies heavily on KV Cache Optimization Pro..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_1523",
              "url": "https://huggingface.co/blog/2026/01/25/open_interpreter_pro_kv_cache_",
              "published": "2026-01-25",
              "snippet": "Under the hood, open-interpreter Pro implements KV Cache Optimization Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_gpqa_evaluated_on",
          "source": "model:stable_diffusion_4_next",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-14_bloomberg_7033",
              "url": "https://bloomberg.com/technology/2026/01/14/stable_diffusion_4_next_gpqa",
              "published": "2026-01-14",
              "snippet": "Stable Diffusion 4 Next achieves 99% on GPQA, setting a new record..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_5115",
              "url": "https://blog.google/technology/ai/2026/01/24/stable_diffusion_4_next_gpqa",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Stable Diffusion 4 Next reaching 85% on GPQA..."
            },
            {
              "docId": "2026-01-24_venturebeat_1698",
              "url": "https://venturebeat.com/2026/01/24/stable_diffusion_4_next_gpqa",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Stable Diffusion 4 Next reaching 95% on GPQA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_sora_2_plus_measures",
          "source": "benchmark:gpqa",
          "target": "model:sora_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_7177",
              "url": "https://techcrunch.com/2026/01/23/gpqa_sora_2_plus",
              "published": "2026-01-23",
              "snippet": "The GPQA benchmark measures Sora 2 Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_plus_flash_attention_lite_uses_tech",
          "source": "dataset:fineweb_plus",
          "target": "tech:flash_attention_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_tool_use_core_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-23_nvidia_blog_9042",
              "url": "https://blogs.nvidia.com/2026/01/23/text_generation_webui_next_too",
              "published": "2026-01-23",
              "snippet": "Under the hood, text-generation-webui Next implements Tool Use Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_group_query_attention_plus_uses_tech",
          "source": "tool:llamaindex_pro",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-14_hugging_face_bl_7716",
              "url": "https://huggingface.co/blog/2026/01/14/llamaindex_pro_group_query_att",
              "published": "2026-01-14",
              "snippet": "LlamaIndex Pro leverages Group Query Attention Plus to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_techcrunch_5338",
              "url": "https://techcrunch.com/2026/01/25/llamaindex_pro_group_query_att",
              "published": "2026-01-25",
              "snippet": "LlamaIndex Pro leverages Group Query Attention Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_next_tool_use_uses_tech",
          "source": "model:deepseek_v3_next",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-15_mit_technology__4016",
              "url": "https://technologyreview.com/2026/01/15/deepseek_v3_next_tool_use",
              "published": "2026-01-15",
              "snippet": "Technical details reveal DeepSeek-V3 Next relies heavily on Tool Use..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_mini_sliding_window_attention_max_uses_tech",
          "source": "dataset:the_stack_v2_mini",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_dall_e_4_edge_depends_on",
          "source": "model:midjourney_v7",
          "target": "model:dall_e_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_quantization_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:slimpajama_v2_tool_use_v2_uses_tech",
          "source": "dataset:slimpajama_v2",
          "target": "tech:tool_use_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_sliding_window_attention_next_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:palm_3_max_sora_2_edge_depends_on",
          "source": "model:palm_3_max",
          "target": "model:sora_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:common_crawl_edge_multimodal_fusion_lite_uses_tech",
          "source": "dataset:common_crawl_edge",
          "target": "tech:multimodal_fusion_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:mmlu_next_llama_4_edge_measures",
          "source": "benchmark:mmlu_next",
          "target": "model:llama_4_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-24_microsoft_resea_7865",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/mmlu_next_llama_4_edge",
              "published": "2026-01-24",
              "snippet": "MMLU Next has become the standard for evaluating Llama 4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_quantization_ultra_uses_tech",
          "source": "repo:ollama_core",
          "target": "tech:quantization_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-22_mit_technology__7824",
              "url": "https://technologyreview.com/2026/01/22/ollama_core_quantization_ultra",
              "published": "2026-01-22",
              "snippet": "Under the hood, ollama Core implements Quantization Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_3100",
              "url": "https://wandb.ai/articles/2026/01/24/ollama_core_quantization_ultra",
              "published": "2026-01-24",
              "snippet": "Under the hood, ollama Core implements Quantization Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_chain_of_thought_ultra_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_group_query_attention_core_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-19_ars_technica_1022",
              "url": "https://arstechnica.com/2025/12/19/textbooks_are_all_you_need_min",
              "published": "2025-12-19",
              "snippet": "Textbooks Are All You Need Mini leverages Group Query Attention Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_arc_agi_edge_evaluated_on",
          "source": "model:gpt_5",
          "target": "benchmark:arc_agi_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-13_wired_3987",
              "url": "https://wired.com/2026/01/13/gpt_5_arc_agi_edge",
              "published": "2026-01-13",
              "snippet": "Evaluation results show GPT-5 reaching 81% on ARC-AGI Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_core_codex_2_core_measures",
          "source": "benchmark:alpacaeval_2_core",
          "target": "model:codex_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-12_anthropic_blog_1068",
              "url": "https://anthropic.com/news/2026/01/12/alpacaeval_2_core_codex_2_core",
              "published": "2026-01-12",
              "snippet": "AlpacaEval 2 Core provides standardized evaluation of Codex 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_max_qwen_3_measures",
          "source": "benchmark:hellaswag_max",
          "target": "model:qwen_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_2844",
              "url": "https://nextgov.com/2026/01/25/hellaswag_max_qwen_3",
              "published": "2026-01-25",
              "snippet": "HellaSwag Max has become the standard for evaluating Qwen-3..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_6958",
              "url": "https://blog.google/technology/ai/2026/01/25/hellaswag_max_qwen_3",
              "published": "2026-01-25",
              "snippet": "HellaSwag Max provides standardized evaluation of Qwen-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_tensorrt_llm_lite_integrates_with",
          "source": "tool:ollama_lite",
          "target": "tool:tensorrt_llm_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:llamacpp_gpt_5_ultra_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:gpt_5_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-21_the_gradient_1881",
              "url": "https://thegradient.pub/2026/01/21/llamacpp_gpt_5_ultra",
              "published": "2026-01-21",
              "snippet": "llama.cpp now supports GPT-5 Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_plus_gpt_4o_mini_2_lite_integrates_with",
          "source": "tool:cody_plus",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-07_reuters_7283",
              "url": "https://reuters.com/technology/2025/12/07/cody_plus_gpt_4o_mini_2_lite",
              "published": "2025-12-07",
              "snippet": "The latest release of Cody Plus adds native GPT-4o Mini 2 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_max_mixtral_8x22b_ultra_integrates_with",
          "source": "tool:semantic_kernel_max",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-24_langchain_blog_6554",
              "url": "https://blog.langchain.dev/2025/12/24/semantic_kernel_max_mixtral_8x",
              "published": "2025-12-24",
              "snippet": "Semantic Kernel Max announced official support for Mixtral 8x22B Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_gguf_plus_uses_tech",
          "source": "repo:langchain",
          "target": "tech:gguf_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-01_reuters_8717",
              "url": "https://reuters.com/technology/2026/01/01/langchain_gguf_plus",
              "published": "2026-01-01",
              "snippet": "Technical details reveal langchain relies heavily on GGUF Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_ollama_lite_integrates_with",
          "source": "tool:gradio_plus",
          "target": "tool:ollama_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-15_weights_and_bia_2055",
              "url": "https://wandb.ai/articles/2026/01/15/gradio_plus_ollama_lite",
              "published": "2026-01-15",
              "snippet": "The latest release of Gradio Plus adds native Ollama Lite integration..."
            },
            {
              "docId": "2026-01-16_techcrunch_4020",
              "url": "https://techcrunch.com/2026/01/16/gradio_plus_ollama_lite",
              "published": "2026-01-16",
              "snippet": "Gradio Plus now supports Ollama Lite with full feature parity..."
            },
            {
              "docId": "2026-01-19_wired_5578",
              "url": "https://wired.com/2026/01/19/gradio_plus_ollama_lite",
              "published": "2026-01-19",
              "snippet": "The latest release of Gradio Plus adds native Ollama Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_lite_midjourney_v7_ultra_integrates_with",
          "source": "tool:semantic_kernel_lite",
          "target": "model:midjourney_v7_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_pro_common_crawl_v2_trained_on",
          "source": "model:nemotron_5_pro",
          "target": "dataset:common_crawl_v2",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:yi_large_pro_bigbench_hard_core_evaluated_on",
          "source": "model:yi_large_pro",
          "target": "benchmark:bigbench_hard_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-18_venturebeat_8577",
              "url": "https://venturebeat.com/2026/01/18/yi_large_pro_bigbench_hard_cor",
              "published": "2026-01-18",
              "snippet": "Yi-Large Pro achieves 94% on BigBench Hard Core, setting a new record..."
            },
            {
              "docId": "2026-01-22_reuters_4014",
              "url": "https://reuters.com/technology/2026/01/22/yi_large_pro_bigbench_hard_cor",
              "published": "2026-01-22",
              "snippet": "Yi-Large Pro achieves 95% on BigBench Hard Core, setting a new record..."
            },
            {
              "docId": "2026-01-24_mit_technology__8198",
              "url": "https://technologyreview.com/2026/01/24/yi_large_pro_bigbench_hard_cor",
              "published": "2026-01-24",
              "snippet": "On the BigBench Hard Core benchmark, Yi-Large Pro scored 89%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_gpt_4o_mini_2_measures",
          "source": "benchmark:math",
          "target": "model:gpt_4o_mini_2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-25_meta_ai_blog_1567",
              "url": "https://ai.meta.com/blog/2026/01/25/math_gpt_4o_mini_2",
              "published": "2026-01-25",
              "snippet": "MATH has become the standard for evaluating GPT-4o Mini 2..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_3527",
              "url": "https://blog.google/technology/ai/2026/01/25/math_gpt_4o_mini_2",
              "published": "2026-01-25",
              "snippet": "The MATH benchmark measures GPT-4o Mini 2 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_pro_langchain_integrates_with",
          "source": "tool:localai_pro",
          "target": "tool:langchain",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_kv_cache_optimization_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_the_gradient_4874",
              "url": "https://thegradient.pub/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 Mini relies heavily on KV Cache Optimization..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_ultra_tool_use_v2_uses_tech",
          "source": "dataset:c4_ultra",
          "target": "tech:tool_use_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:haystack_plus_ollama_lite_integrates_with",
          "source": "tool:haystack_plus",
          "target": "tool:ollama_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gpt4all_core_constitutional_ai_mini_uses_tech",
          "source": "repo:gpt4all_core",
          "target": "tech:constitutional_ai_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_9186",
              "url": "https://reuters.com/technology/2026/01/24/gpt4all_core_constitutional_ai",
              "published": "2026-01-24",
              "snippet": "Under the hood, gpt4all Core implements Constitutional AI Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-25_langchain_blog_6175",
              "url": "https://blog.langchain.dev/2026/01/25/gpt4all_core_constitutional_ai",
              "published": "2026-01-25",
              "snippet": "Under the hood, gpt4all Core implements Constitutional AI Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_mbpp_core_evaluated_on",
          "source": "model:deepseek_v3_mini",
          "target": "benchmark:mbpp_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:litellm_plus_weights_and_biases_next_integrates_with",
          "source": "tool:litellm_plus",
          "target": "tool:weights_and_biases_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:c4_max_multimodal_fusion_ultra_uses_tech",
          "source": "dataset:c4_max",
          "target": "tech:multimodal_fusion_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_next_hellaswag_edge_evaluated_on",
          "source": "model:claude_opus_45_next",
          "target": "benchmark:hellaswag_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-10_the_gradient_4550",
              "url": "https://thegradient.pub/2026/01/10/claude_opus_45_next_hellaswag_",
              "published": "2026-01-10",
              "snippet": "On the HellaSwag Edge benchmark, Claude Opus 4.5 Next scored 80%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_speculative_decoding_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:localai_next_cursor_integrates_with",
          "source": "tool:localai_next",
          "target": "tool:cursor",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_9049",
              "url": "https://wandb.ai/articles/2026/01/25/localai_next_cursor",
              "published": "2026-01-25",
              "snippet": "LocalAI Next announced official support for Cursor..."
            },
            {
              "docId": "2026-01-25_mit_technology__5279",
              "url": "https://technologyreview.com/2026/01/25/localai_next_cursor",
              "published": "2026-01-25",
              "snippet": "LocalAI Next now supports Cursor with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_dall_e_4_mini_integrates_with",
          "source": "repo:vllm_edge",
          "target": "model:dall_e_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-18_mit_technology__9247",
              "url": "https://technologyreview.com/2026/01/18/vllm_edge_dall_e_4_mini",
              "published": "2026-01-18",
              "snippet": "vllm Edge announced official support for DALL-E 4 Mini..."
            },
            {
              "docId": "2026-01-19_venturebeat_1628",
              "url": "https://venturebeat.com/2026/01/19/vllm_edge_dall_e_4_mini",
              "published": "2026-01-19",
              "snippet": "vllm Edge now supports DALL-E 4 Mini with full feature parity..."
            },
            {
              "docId": "2026-01-21_microsoft_resea_7990",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/vllm_edge_dall_e_4_mini",
              "published": "2026-01-21",
              "snippet": "vllm Edge announced official support for DALL-E 4 Mini..."
            },
            {
              "docId": "2026-01-22_microsoft_resea_5253",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/22/vllm_edge_dall_e_4_mini",
              "published": "2026-01-22",
              "snippet": "The latest release of vllm Edge adds native DALL-E 4 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_lite_multimodal_fusion_edge_uses_tech",
          "source": "tool:tensorrt_llm_lite",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_max_deepseek_v3_core_measures",
          "source": "benchmark:alpacaeval_2_max",
          "target": "model:deepseek_v3_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-16_anthropic_blog_2004",
              "url": "https://anthropic.com/news/2026/01/16/alpacaeval_2_max_deepseek_v3_c",
              "published": "2026-01-16",
              "snippet": "AlpacaEval 2 Max has become the standard for evaluating DeepSeek-V3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_group_query_attention_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_8324",
              "url": "https://blog.google/technology/ai/2026/01/25/mlflow_max_group_query_attenti",
              "published": "2026-01-25",
              "snippet": "Technical details reveal MLflow Max relies heavily on Group Query Attention..."
            },
            {
              "docId": "2026-01-25_langchain_blog_1034",
              "url": "https://blog.langchain.dev/2026/01/25/mlflow_max_group_query_attenti",
              "published": "2026-01-25",
              "snippet": "MLflow Max leverages Group Query Attention to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_ultra_dall_e_4_core_measures",
          "source": "benchmark:gsm8k_ultra",
          "target": "model:dall_e_4_core",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.96
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_roots_edge_trained_on",
          "source": "model:stable_diffusion_4_next",
          "target": "dataset:roots_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:gradio_core_streamlit_max_integrates_with",
          "source": "tool:gradio_core",
          "target": "tool:streamlit_max",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_edge_common_crawl_lite_trained_on",
          "source": "model:midjourney_v7_edge",
          "target": "dataset:common_crawl_lite",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_gpt_4o_mini_2_pro_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:gpt_4o_mini_2_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:flowise_next_streamlit_lite_integrates_with",
          "source": "tool:flowise_next",
          "target": "tool:streamlit_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2025-11-20_langchain_blog_1258",
              "url": "https://blog.langchain.dev/2025/11/20/flowise_next_streamlit_lite",
              "published": "2025-11-20",
              "snippet": "The latest release of Flowise Next adds native Streamlit Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_pro_mmlu_next_evaluated_on",
          "source": "model:deepseek_v3_pro",
          "target": "benchmark:mmlu_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-12-20_the_verge_5898",
              "url": "https://theverge.com/2025/12/20/deepseek_v3_pro_mmlu_next",
              "published": "2025-12-20",
              "snippet": "DeepSeek-V3 Pro achieves 75% on MMLU Next, setting a new record..."
            },
            {
              "docId": "2026-01-18_ars_technica_8984",
              "url": "https://arstechnica.com/2026/01/18/deepseek_v3_pro_mmlu_next",
              "published": "2026-01-18",
              "snippet": "DeepSeek-V3 Pro achieves 77% on MMLU Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_mixtral_8x22b_edge_integrates_with",
          "source": "repo:llamacpp_ultra",
          "target": "model:mixtral_8x22b_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-21_nvidia_blog_1462",
              "url": "https://blogs.nvidia.com/2026/01/21/llamacpp_ultra_mixtral_8x22b_e",
              "published": "2026-01-21",
              "snippet": "The latest release of llama.cpp Ultra adds native Mixtral 8x22B Edge integration..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_7594",
              "url": "https://wandb.ai/articles/2026/01/24/llamacpp_ultra_mixtral_8x22b_e",
              "published": "2026-01-24",
              "snippet": "llama.cpp Ultra now supports Mixtral 8x22B Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_v2_whisper_v4_edge_measures",
          "source": "benchmark:mt_bench_v2",
          "target": "model:whisper_v4_edge",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_flash_attention_mini_uses_tech",
          "source": "paper:direct_preference_optimization_edge",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-11_reuters_3260",
              "url": "https://reuters.com/technology/2026/01/11/direct_preference_optimization",
              "published": "2026-01-11",
              "snippet": "Technical details reveal Direct Preference Optimization Edge relies heavily on Flash Attention Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_red_teaming_edge_uses_tech",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tech:red_teaming_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_flash_attention_edge_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:haystack_next_gpt_4o_mini_2_ultra_integrates_with",
          "source": "tool:haystack_next",
          "target": "model:gpt_4o_mini_2_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-21_the_verge_8681",
              "url": "https://theverge.com/2026/01/21/haystack_next_gpt_4o_mini_2_ul",
              "published": "2026-01-21",
              "snippet": "The latest release of Haystack Next adds native GPT-4o Mini 2 Ultra integration..."
            },
            {
              "docId": "2026-01-23_wired_7760",
              "url": "https://wired.com/2026/01/23/haystack_next_gpt_4o_mini_2_ul",
              "published": "2026-01-23",
              "snippet": "The latest release of Haystack Next adds native GPT-4o Mini 2 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_edge_tool_use_uses_tech",
          "source": "tool:weights_and_biases_edge",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_lora_core_uses_tech",
          "source": "tool:copilot_edge",
          "target": "tech:lora_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_ultra_stable_diffusion_4_lite_depends_on",
          "source": "model:gpt_4o_mini_2_ultra",
          "target": "model:stable_diffusion_4_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:laion_5b_tokenizer_bpe_uses_tech",
          "source": "dataset:laion_5b",
          "target": "tech:tokenizer_bpe",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:falcon_3_v2_alpacaeval_2_lite_evaluated_on",
          "source": "model:falcon_3_v2",
          "target": "benchmark:alpacaeval_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2025-12-24_arxiv_3684",
              "url": "https://arxiv.org/abs/2025/12/24/falcon_3_v2_alpacaeval_2_lite",
              "published": "2025-12-24",
              "snippet": "Evaluation results show Falcon 3 v2 reaching 85% on AlpacaEval 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_plus_llamaindex_ultra_integrates_with",
          "source": "tool:ollama_plus",
          "target": "tool:llamaindex_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_mbpp_core_evaluated_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "benchmark:mbpp_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-17_anthropic_blog_5438",
              "url": "https://anthropic.com/news/2026/01/17/stable_diffusion_4_plus_mbpp_c",
              "published": "2026-01-17",
              "snippet": "Evaluation results show Stable Diffusion 4 Plus reaching 96% on MBPP Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_gsm8k_edge_evaluated_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "benchmark:gsm8k_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-27_nextgov_2414",
              "url": "https://nextgov.com/2025/12/27/stable_diffusion_4_v2_gsm8k_ed",
              "published": "2025-12-27",
              "snippet": "On the GSM8K Edge benchmark, Stable Diffusion 4 v2 scored 76%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_c4_edge_trained_on",
          "source": "model:qwen_3_next",
          "target": "dataset:c4_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_falcon_3_next_integrates_with",
          "source": "repo:gpt4all_v2",
          "target": "model:falcon_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_mixture_of_experts_uses_tech",
          "source": "tool:langchain_plus",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_autogpt_ultra_integrates_with",
          "source": "tool:semantic_kernel_v2",
          "target": "tool:autogpt_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mt_bench_pro_evaluated_on",
          "source": "model:deepseek_v3",
          "target": "benchmark:mt_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_max_dall_e_4_edge_integrates_with",
          "source": "tool:semantic_kernel_max",
          "target": "model:dall_e_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_gpt_5_core_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:gpt_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-16_google_ai_blog_9885",
              "url": "https://blog.google/technology/ai/2026/01/16/llamacpp_mini_gpt_5_core",
              "published": "2026-01-16",
              "snippet": "The latest release of llama.cpp Mini adds native GPT-5 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_yi_large_depends_on",
          "source": "model:grok_3_next",
          "target": "model:yi_large",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_mt_bench_plus_evaluated_on",
          "source": "model:mixtral_8x22b_mini",
          "target": "benchmark:mt_bench_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-18_ars_technica_5014",
              "url": "https://arstechnica.com/2026/01/18/mixtral_8x22b_mini_mt_bench_pl",
              "published": "2026-01-18",
              "snippet": "Mixtral 8x22B Mini achieves 73% on MT-Bench Plus, setting a new record..."
            },
            {
              "docId": "2026-01-18_mit_technology__4546",
              "url": "https://technologyreview.com/2026/01/18/mixtral_8x22b_mini_mt_bench_pl",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Mixtral 8x22B Mini reaching 77% on MT-Bench Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_core_gpt_4o_mini_2_max_integrates_with",
          "source": "tool:localai_core",
          "target": "model:gpt_4o_mini_2_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-04_anthropic_blog_6921",
              "url": "https://anthropic.com/news/2026/01/04/localai_core_gpt_4o_mini_2_max",
              "published": "2026-01-04",
              "snippet": "LocalAI Core now supports GPT-4o Mini 2 Max with full feature parity..."
            },
            {
              "docId": "2026-01-19_ars_technica_8170",
              "url": "https://arstechnica.com/2026/01/19/localai_core_gpt_4o_mini_2_max",
              "published": "2026-01-19",
              "snippet": "LocalAI Core now supports GPT-4o Mini 2 Max with full feature parity..."
            },
            {
              "docId": "2026-01-25_nextgov_6532",
              "url": "https://nextgov.com/2026/01/25/localai_core_gpt_4o_mini_2_max",
              "published": "2026-01-25",
              "snippet": "LocalAI Core now supports GPT-4o Mini 2 Max with full feature parity..."
            },
            {
              "docId": "2026-01-25_openai_blog_9097",
              "url": "https://openai.com/blog/2026/01/25/localai_core_gpt_4o_mini_2_max",
              "published": "2026-01-25",
              "snippet": "LocalAI Core announced official support for GPT-4o Mini 2 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_ultra_redpajama_v2_edge_trained_on",
          "source": "model:falcon_3_ultra",
          "target": "dataset:redpajama_v2_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:mlflow_lite_dall_e_4_ultra_integrates_with",
          "source": "tool:mlflow_lite",
          "target": "model:dall_e_4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:sora_2_core_transformer_architecture_core_uses_tech",
          "source": "model:sora_2_core",
          "target": "tech:transformer_architecture_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_jamba_2_edge_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:jamba_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_deepseek_v3_max_depends_on",
          "source": "model:deepseek_v3",
          "target": "model:deepseek_v3_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:cody_claude_sonnet_4_mini_integrates_with",
          "source": "tool:cody",
          "target": "model:claude_sonnet_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:sora_2_the_stack_v2_core_trained_on",
          "source": "model:sora_2",
          "target": "dataset:the_stack_v2_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-11_venturebeat_3156",
              "url": "https://venturebeat.com/2026/01/11/sora_2_the_stack_v2_core",
              "published": "2026-01-11",
              "snippet": "Sora 2 was trained on The Stack v2 Core comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_deepseek_v3_v2_integrates_with",
          "source": "tool:litellm",
          "target": "model:deepseek_v3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-13_weights_and_bia_7573",
              "url": "https://wandb.ai/articles/2025/12/13/litellm_deepseek_v3_v2",
              "published": "2025-12-13",
              "snippet": "The latest release of LiteLLM adds native DeepSeek-V3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_edge_midjourney_v7_mini_integrates_with",
          "source": "tool:llamaindex_edge",
          "target": "model:midjourney_v7_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_3193",
              "url": "https://arxiv.org/abs/2026/01/24/llamaindex_edge_midjourney_v7_",
              "published": "2026-01-24",
              "snippet": "LlamaIndex Edge announced official support for Midjourney V7 Mini..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_6494",
              "url": "https://ai.meta.com/blog/2026/01/24/llamaindex_edge_midjourney_v7_",
              "published": "2026-01-24",
              "snippet": "LlamaIndex Edge now supports Midjourney V7 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_deepseek_v3_core_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:deepseek_v3_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-20_venturebeat_6955",
              "url": "https://venturebeat.com/2026/01/20/textbooks_are_all_you_need_max",
              "published": "2026-01-20",
              "snippet": "Textbooks Are All You Need Max achieves 77% on DeepSeek-V3 Core, setting a new record..."
            },
            {
              "docId": "2026-01-20_microsoft_resea_5936",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/textbooks_are_all_you_need_max",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Textbooks Are All You Need Max reaching 98% on DeepSeek-V3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_dall_e_4_core_evaluated_on",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "model:dall_e_4_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_multimodal_fusion_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:multimodal_fusion",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_redpajama_v2_next_trained_on",
          "source": "model:grok_3_next",
          "target": "dataset:redpajama_v2_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_retrieval_augmented_generation_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_1818",
              "url": "https://theverge.com/2026/01/25/llamacpp_mini_retrieval_augmen",
              "published": "2026-01-25",
              "snippet": "llama.cpp Mini leverages Retrieval-Augmented Generation to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_mit_technology__6859",
              "url": "https://technologyreview.com/2026/01/25/llamacpp_mini_retrieval_augmen",
              "published": "2026-01-25",
              "snippet": "Under the hood, llama.cpp Mini implements Retrieval-Augmented Generation for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_mixture_of_experts_plus_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_copilot_core_integrates_with",
          "source": "tool:langchain_plus",
          "target": "tool:copilot_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:localai_sparse_attention_pro_uses_tech",
          "source": "repo:localai",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-21_bloomberg_3417",
              "url": "https://bloomberg.com/technology/2026/01/21/localai_sparse_attention_pro",
              "published": "2026-01-21",
              "snippet": "LocalAI leverages Sparse Attention Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_falcon_3_pro_integrates_with",
          "source": "tool:cursor",
          "target": "model:falcon_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-20_venturebeat_4589",
              "url": "https://venturebeat.com/2026/01/20/cursor_falcon_3_pro",
              "published": "2026-01-20",
              "snippet": "Cursor announced official support for Falcon 3 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_mini_mixture_of_experts_next_uses_tech",
          "source": "repo:localai_mini",
          "target": "tech:mixture_of_experts_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:swe_bench_pro_mixtral_8x22b_ultra_measures",
          "source": "benchmark:swe_bench_pro",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:arc_agi_max_nemotron_5_measures",
          "source": "benchmark:arc_agi_max",
          "target": "model:nemotron_5",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-14_nvidia_blog_9083",
              "url": "https://blogs.nvidia.com/2026/01/14/arc_agi_max_nemotron_5",
              "published": "2026-01-14",
              "snippet": "ARC-AGI Max has become the standard for evaluating Nemotron-5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_flash_attention_edge_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_8567",
              "url": "https://huggingface.co/blog/2026/01/25/llamacpp_mini_flash_attention_",
              "published": "2026-01-25",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Flash Attention Edge..."
            },
            {
              "docId": "2026-01-25_openai_blog_5028",
              "url": "https://openai.com/blog/2026/01/25/llamacpp_mini_flash_attention_",
              "published": "2026-01-25",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Flash Attention Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_plus_midjourney_v7_v2_integrates_with",
          "source": "tool:cody_plus",
          "target": "model:midjourney_v7_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-23_google_ai_blog_4083",
              "url": "https://blog.google/technology/ai/2026/01/23/cody_plus_midjourney_v7_v2",
              "published": "2026-01-23",
              "snippet": "The latest release of Cody Plus adds native Midjourney V7 v2 integration..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_6977",
              "url": "https://ai.meta.com/blog/2026/01/24/cody_plus_midjourney_v7_v2",
              "published": "2026-01-24",
              "snippet": "Cody Plus now supports Midjourney V7 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-24_techcrunch_6683",
              "url": "https://techcrunch.com/2026/01/24/cody_plus_midjourney_v7_v2",
              "published": "2026-01-24",
              "snippet": "Cody Plus now supports Midjourney V7 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_multimodal_fusion_ultra_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:multimodal_fusion_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-23_microsoft_resea_3331",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/mlflow_max_multimodal_fusion_u",
              "published": "2026-01-23",
              "snippet": "Technical details reveal MLflow Max relies heavily on Multimodal Fusion Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_core_jamba_2_mini_measures",
          "source": "benchmark:lmsys_chatbot_arena_core",
          "target": "model:jamba_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-21_techcrunch_9912",
              "url": "https://techcrunch.com/2026/01/21/lmsys_chatbot_arena_core_jamba",
              "published": "2026-01-21",
              "snippet": "LMSYS Chatbot Arena Core has become the standard for evaluating Jamba 2 Mini..."
            },
            {
              "docId": "2026-01-24_reuters_1398",
              "url": "https://reuters.com/technology/2026/01/24/lmsys_chatbot_arena_core_jamba",
              "published": "2026-01-24",
              "snippet": "LMSYS Chatbot Arena Core has become the standard for evaluating Jamba 2 Mini..."
            },
            {
              "docId": "2026-01-24_langchain_blog_3471",
              "url": "https://blog.langchain.dev/2026/01/24/lmsys_chatbot_arena_core_jamba",
              "published": "2026-01-24",
              "snippet": "LMSYS Chatbot Arena Core has become the standard for evaluating Jamba 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_mixture_of_experts_max_uses_tech",
          "source": "tool:copilot_edge",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:langchain_max_qwen_3_lite_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:qwen_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_gemma_3_edge_evaluated_on",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "model:gemma_3_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-21_langchain_blog_5708",
              "url": "https://blog.langchain.dev/2026/01/21/llm_agents:_a_survey_core_gemm",
              "published": "2026-01-21",
              "snippet": "LLM Agents: A Survey Core achieves 93% on Gemma 3 Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_plus_deepseek_v3_ultra_integrates_with",
          "source": "tool:ollama_plus",
          "target": "model:deepseek_v3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-18_techcrunch_3278",
              "url": "https://techcrunch.com/2026/01/18/ollama_plus_deepseek_v3_ultra",
              "published": "2026-01-18",
              "snippet": "The latest release of Ollama Plus adds native DeepSeek-V3 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_max_slimpajama_pro_trained_on",
          "source": "model:dall_e_4_max",
          "target": "dataset:slimpajama_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_gpt_5_v2_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:gpt_5_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_edge_transformer_architecture_uses_tech",
          "source": "dataset:the_stack_v2_edge",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_distillation_core_uses_tech",
          "source": "repo:transformers_pro",
          "target": "tech:distillation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__whisper_v4_lite_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:whisper_v4_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-21_arxiv_1859",
              "url": "https://arxiv.org/abs/2026/01/21/scaling_laws_for_neural_langua",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Scaling Laws for Neural Language Models (2025) reaching 74% on Whisper v4 Lite..."
            },
            {
              "docId": "2026-01-25_venturebeat_9374",
              "url": "https://venturebeat.com/2026/01/25/scaling_laws_for_neural_langua",
              "published": "2026-01-25",
              "snippet": "Scaling Laws for Neural Language Models (2025) achieves 90% on Whisper v4 Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_rlhf_plus_uses_tech",
          "source": "repo:ollama_core",
          "target": "tech:rlhf_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-23_langchain_blog_3413",
              "url": "https://blog.langchain.dev/2026/01/23/ollama_core_rlhf_plus",
              "published": "2026-01-23",
              "snippet": "ollama Core leverages RLHF Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_redpajama_v2_next_trained_on",
          "source": "model:codex_2_core",
          "target": "dataset:redpajama_v2_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_tree_of_thought_core_uses_tech",
          "source": "repo:transformers_pro",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_pro_lmsys_chatbot_arena_core_evaluated_on",
          "source": "model:mixtral_8x22b_pro",
          "target": "benchmark:lmsys_chatbot_arena_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-11_reuters_1432",
              "url": "https://reuters.com/technology/2026/01/11/mixtral_8x22b_pro_lmsys_chatbo",
              "published": "2026-01-11",
              "snippet": "On the LMSYS Chatbot Arena Core benchmark, Mixtral 8x22B Pro scored 77%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_max_speculative_decoding_ultra_uses_tech",
          "source": "dataset:wikipedia_dump_2025_max",
          "target": "tech:speculative_decoding_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_max_mt_bench_plus_evaluated_on",
          "source": "model:dall_e_4_max",
          "target": "benchmark:mt_bench_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-20_google_ai_blog_2940",
              "url": "https://blog.google/technology/ai/2026/01/20/dall_e_4_max_mt_bench_plus",
              "published": "2026-01-20",
              "snippet": "DALL-E 4 Max achieves 77% on MT-Bench Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_pro_falcon_3_v2_integrates_with",
          "source": "repo:llamacpp_pro",
          "target": "model:falcon_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-20_anthropic_blog_5500",
              "url": "https://anthropic.com/news/2026/01/20/llamacpp_pro_falcon_3_v2",
              "published": "2026-01-20",
              "snippet": "The latest release of llama.cpp Pro adds native Falcon 3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_edge_kv_cache_optimization_next_uses_tech",
          "source": "tool:crewai_edge",
          "target": "tech:kv_cache_optimization_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:ollama_plus_haystack_pro_integrates_with",
          "source": "tool:ollama_plus",
          "target": "tool:haystack_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-07_hugging_face_bl_9381",
              "url": "https://huggingface.co/blog/2026/01/07/ollama_plus_haystack_pro",
              "published": "2026-01-07",
              "snippet": "The latest release of Ollama Plus adds native Haystack Pro integration..."
            },
            {
              "docId": "2026-01-13_hugging_face_bl_5062",
              "url": "https://huggingface.co/blog/2026/01/13/ollama_plus_haystack_pro",
              "published": "2026-01-13",
              "snippet": "Ollama Plus announced official support for Haystack Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_max_dolma_pro_trained_on",
          "source": "model:jamba_2_max",
          "target": "dataset:dolma_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_slimpajama_pro_trained_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "dataset:slimpajama_pro",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-25_bloomberg_5705",
              "url": "https://bloomberg.com/technology/2025/12/25/gemini_ultra_2_pro_slimpajama_",
              "published": "2025-12-25",
              "snippet": "Gemini Ultra 2 Pro utilized SlimPajama Pro as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_v2_red_teaming_lite_uses_tech",
          "source": "dataset:mc4_v2",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:codex_2_plus_gemma_3_edge_depends_on",
          "source": "model:codex_2_plus",
          "target": "model:gemma_3_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_nemotron_5_mini_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "model:nemotron_5_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-13_the_verge_4406",
              "url": "https://theverge.com/2026/01/13/attention_is_all_you_need_v2_m",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Attention Is All You Need v2 Max reaching 87% on Nemotron-5 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_pro_rotary_position_embedding_pro_uses_tech",
          "source": "model:phi_4_pro",
          "target": "tech:rotary_position_embedding_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_quantization_ultra_uses_tech",
          "source": "model:sora_2_edge",
          "target": "tech:quantization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_swe_bench_pro_evaluated_on",
          "source": "model:yi_large_next",
          "target": "benchmark:swe_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_midjourney_v7_edge_depends_on",
          "source": "model:stable_diffusion_4",
          "target": "model:midjourney_v7_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_multimodal_fusion_lite_uses_tech",
          "source": "model:claude_opus_45",
          "target": "tech:multimodal_fusion_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_core_distillation_next_uses_tech",
          "source": "dataset:openwebtext2_core",
          "target": "tech:distillation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_ultra_qwen_3_max_depends_on",
          "source": "model:midjourney_v7_ultra",
          "target": "model:qwen_3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_yi_large_mini_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:yi_large_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_next_sora_2_edge_measures",
          "source": "benchmark:bigbench_hard_next",
          "target": "model:sora_2_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-18_bloomberg_7007",
              "url": "https://bloomberg.com/technology/2026/01/18/bigbench_hard_next_sora_2_edge",
              "published": "2026-01-18",
              "snippet": "BigBench Hard Next provides standardized evaluation of Sora 2 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_yi_large_max_depends_on",
          "source": "model:gemma_3_lite",
          "target": "model:yi_large_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:vllm_v2_jamba_2_plus_integrates_with",
          "source": "repo:vllm_v2",
          "target": "model:jamba_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_v2_stable_diffusion_4_depends_on",
          "source": "model:claude_sonnet_4_v2",
          "target": "model:stable_diffusion_4",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:hellaswag_mini_sora_2_core_measures",
          "source": "benchmark:hellaswag_mini",
          "target": "model:sora_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-09_openai_blog_7688",
              "url": "https://openai.com/blog/2026/01/09/hellaswag_mini_sora_2_core",
              "published": "2026-01-09",
              "snippet": "HellaSwag Mini provides standardized evaluation of Sora 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_core_claude_opus_45_pro_integrates_with",
          "source": "repo:gpt4all_core",
          "target": "model:claude_opus_45_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_v2_gpqa_max_evaluated_on",
          "source": "model:gemini_ultra_2_v2",
          "target": "benchmark:gpqa_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_1674",
              "url": "https://nextgov.com/2026/01/24/gemini_ultra_2_v2_gpqa_max",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Gemini Ultra 2 v2 reaching 76% on GPQA Max..."
            },
            {
              "docId": "2026-01-25_the_verge_5716",
              "url": "https://theverge.com/2026/01/25/gemini_ultra_2_v2_gpqa_max",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Gemini Ultra 2 v2 reaching 93% on GPQA Max..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_6969",
              "url": "https://blogs.nvidia.com/2026/01/25/gemini_ultra_2_v2_gpqa_max",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Gemini Ultra 2 v2 reaching 81% on GPQA Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_plus_llama_4_ultra_integrates_with",
          "source": "tool:haystack_plus",
          "target": "model:llama_4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-24_microsoft_resea_5007",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/haystack_plus_llama_4_ultra",
              "published": "2026-01-24",
              "snippet": "Haystack Plus now supports Llama 4 Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_v2_yi_large_measures",
          "source": "benchmark:math_v2",
          "target": "model:yi_large",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-11_openai_blog_6800",
              "url": "https://openai.com/blog/2026/01/11/math_v2_yi_large",
              "published": "2026-01-11",
              "snippet": "The MATH v2 benchmark measures Yi-Large across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_tree_of_thought_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_next_aya_3_integrates_with",
          "source": "tool:weights_and_biases_next",
          "target": "model:aya_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-07_hugging_face_bl_7598",
              "url": "https://huggingface.co/blog/2026/01/07/weights_and_biases_next_aya_3",
              "published": "2026-01-07",
              "snippet": "The latest release of Weights & Biases Next adds native Aya 3 integration..."
            },
            {
              "docId": "2026-01-23_langchain_blog_4789",
              "url": "https://blog.langchain.dev/2026/01/23/weights_and_biases_next_aya_3",
              "published": "2026-01-23",
              "snippet": "The latest release of Weights & Biases Next adds native Aya 3 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_edge_whisper_v4_lite_measures",
          "source": "benchmark:winogrande_edge",
          "target": "model:whisper_v4_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-25_openai_blog_8377",
              "url": "https://openai.com/blog/2025/12/25/winogrande_edge_whisper_v4_lit",
              "published": "2025-12-25",
              "snippet": "WinoGrande Edge provides standardized evaluation of Whisper v4 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_openwebtext2_trained_on",
          "source": "model:deepseek_v3",
          "target": "dataset:openwebtext2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:dify_lite_gemini_ultra_2_lite_integrates_with",
          "source": "tool:dify_lite",
          "target": "model:gemini_ultra_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-13_langchain_blog_8250",
              "url": "https://blog.langchain.dev/2026/01/13/dify_lite_gemini_ultra_2_lite",
              "published": "2026-01-13",
              "snippet": "The latest release of Dify Lite adds native Gemini Ultra 2 Lite integration..."
            },
            {
              "docId": "2026-01-16_anthropic_blog_2342",
              "url": "https://anthropic.com/news/2026/01/16/dify_lite_gemini_ultra_2_lite",
              "published": "2026-01-16",
              "snippet": "Dify Lite now supports Gemini Ultra 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_gpt_5_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:gpt_5",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_mixture_of_experts_max_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_8711",
              "url": "https://techcrunch.com/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 v2 leverages Mixture of Experts Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_ars_technica_1579",
              "url": "https://arstechnica.com/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 v2 relies heavily on Mixture of Experts Max..."
            },
            {
              "docId": "2026-01-25_bloomberg_6990",
              "url": "https://bloomberg.com/technology/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 v2 leverages Mixture of Experts Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_reuters_9184",
              "url": "https://reuters.com/technology/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 v2 leverages Mixture of Experts Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_edge_aya_3_pro_depends_on",
          "source": "model:gemini_ultra_2_edge",
          "target": "model:aya_3_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_next_fineweb_pro_trained_on",
          "source": "model:claude_opus_45_next",
          "target": "dataset:fineweb_pro",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2026-01-08_google_ai_blog_4675",
              "url": "https://blog.google/technology/ai/2026/01/08/claude_opus_45_next_fineweb_pr",
              "published": "2026-01-08",
              "snippet": "Claude Opus 4.5 Next utilized FineWeb Pro as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-16_openai_blog_8938",
              "url": "https://openai.com/blog/2026/01/16/claude_opus_45_next_fineweb_pr",
              "published": "2026-01-16",
              "snippet": "The training corpus for Claude Opus 4.5 Next includes FineWeb Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_stable_diffusion_4_edge_integrates_with",
          "source": "tool:cursor_mini",
          "target": "model:stable_diffusion_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-22_venturebeat_1560",
              "url": "https://venturebeat.com/2026/01/22/cursor_mini_stable_diffusion_4",
              "published": "2026-01-22",
              "snippet": "The latest release of Cursor Mini adds native Stable Diffusion 4 Edge integration..."
            },
            {
              "docId": "2026-01-22_meta_ai_blog_7231",
              "url": "https://ai.meta.com/blog/2026/01/22/cursor_mini_stable_diffusion_4",
              "published": "2026-01-22",
              "snippet": "The latest release of Cursor Mini adds native Stable Diffusion 4 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_stable_diffusion_4_lite_evaluated_on",
          "source": "paper:direct_preference_optimization_next",
          "target": "model:stable_diffusion_4_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-14_techcrunch_4857",
              "url": "https://techcrunch.com/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "On the Stable Diffusion 4 Lite benchmark, Direct Preference Optimization Next scored 92%..."
            },
            {
              "docId": "2026-01-22_the_gradient_7859",
              "url": "https://thegradient.pub/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "Direct Preference Optimization Next achieves 83% on Stable Diffusion 4 Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_core_codex_2_mini_depends_on",
          "source": "model:qwen_3_core",
          "target": "model:codex_2_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:haystack_next_autogpt_next_integrates_with",
          "source": "tool:haystack_next",
          "target": "tool:autogpt_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_9004",
              "url": "https://nextgov.com/2026/01/24/haystack_next_autogpt_next",
              "published": "2026-01-24",
              "snippet": "The latest release of Haystack Next adds native AutoGPT Next integration..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_4430",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/haystack_next_autogpt_next",
              "published": "2026-01-25",
              "snippet": "The latest release of Haystack Next adds native AutoGPT Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_flash_attention_lite_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:flash_attention_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-15_venturebeat_1118",
              "url": "https://venturebeat.com/2026/01/15/mixture_of_experts_meets_instr",
              "published": "2026-01-15",
              "snippet": "Technical details reveal Mixture of Experts Meets Instruction Tuning relies heavily on Flash Attention Lite..."
            },
            {
              "docId": "2026-01-22_microsoft_resea_2502",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/22/mixture_of_experts_meets_instr",
              "published": "2026-01-22",
              "snippet": "Mixture of Experts Meets Instruction Tuning leverages Flash Attention Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_nemotron_5_lite_depends_on",
          "source": "model:yi_large_next",
          "target": "model:nemotron_5_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_next_mbpp_v2_evaluated_on",
          "source": "model:midjourney_v7_next",
          "target": "benchmark:mbpp_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-13_mit_technology__3167",
              "url": "https://technologyreview.com/2026/01/13/midjourney_v7_next_mbpp_v2",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Midjourney V7 Next reaching 88% on MBPP v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_nemotron_5_lite_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "model:nemotron_5_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__sora_2_next_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:sora_2_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-23_hugging_face_bl_8590",
              "url": "https://huggingface.co/blog/2026/01/23/constitutional_ai:_harmlessnes",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Constitutional AI: Harmlessness from AI Feedback reaching 97% on Sora 2 Next..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_6512",
              "url": "https://wandb.ai/articles/2026/01/25/constitutional_ai:_harmlessnes",
              "published": "2026-01-25",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 71% on Sora 2 Next, setting a new record..."
            },
            {
              "docId": "2026-01-25_reuters_6307",
              "url": "https://reuters.com/technology/2026/01/25/constitutional_ai:_harmlessnes",
              "published": "2026-01-25",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 75% on Sora 2 Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_lite_deepseek_v3_lite_measures",
          "source": "benchmark:lmsys_chatbot_arena_lite",
          "target": "model:deepseek_v3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-14_weights_and_bia_9012",
              "url": "https://wandb.ai/articles/2026/01/14/lmsys_chatbot_arena_lite_deeps",
              "published": "2026-01-14",
              "snippet": "LMSYS Chatbot Arena Lite has become the standard for evaluating DeepSeek-V3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_mixture_of_experts_ultra_uses_tech",
          "source": "repo:text_generation_webui_pro",
          "target": "tech:mixture_of_experts_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-14_weights_and_bia_2899",
              "url": "https://wandb.ai/articles/2026/01/14/text_generation_webui_pro_mixt",
              "published": "2026-01-14",
              "snippet": "text-generation-webui Pro leverages Mixture of Experts Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_mit_technology__6037",
              "url": "https://technologyreview.com/2026/01/25/text_generation_webui_pro_mixt",
              "published": "2026-01-25",
              "snippet": "Technical details reveal text-generation-webui Pro relies heavily on Mixture of Experts Ultra..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_9081",
              "url": "https://wandb.ai/articles/2026/01/25/text_generation_webui_pro_mixt",
              "published": "2026-01-25",
              "snippet": "Technical details reveal text-generation-webui Pro relies heavily on Mixture of Experts Ultra..."
            },
            {
              "docId": "2026-01-25_langchain_blog_9212",
              "url": "https://blog.langchain.dev/2026/01/25/text_generation_webui_pro_mixt",
              "published": "2026-01-25",
              "snippet": "Technical details reveal text-generation-webui Pro relies heavily on Mixture of Experts Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_claude_sonnet_4_ultra_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:claude_sonnet_4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-14_the_verge_3933",
              "url": "https://theverge.com/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "Evaluation results show Direct Preference Optimization Pro reaching 97% on Claude Sonnet 4 Ultra..."
            },
            {
              "docId": "2026-01-14_langchain_blog_4097",
              "url": "https://blog.langchain.dev/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "Direct Preference Optimization Pro achieves 85% on Claude Sonnet 4 Ultra, setting a new record..."
            },
            {
              "docId": "2026-01-18_reuters_9318",
              "url": "https://reuters.com/technology/2026/01/18/direct_preference_optimization",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Direct Preference Optimization Pro reaching 80% on Claude Sonnet 4 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_aya_3_plus_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:aya_3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-27_venturebeat_9871",
              "url": "https://venturebeat.com/2025/12/27/transformers_pro_aya_3_plus",
              "published": "2025-12-27",
              "snippet": "transformers Pro announced official support for Aya 3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_wikipedia_dump_2025_trained_on",
          "source": "model:gpt_5_v2",
          "target": "dataset:wikipedia_dump_2025",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:hellaswag_gemini_ultra_2_edge_measures",
          "source": "benchmark:hellaswag",
          "target": "model:gemini_ultra_2_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-09_nextgov_9769",
              "url": "https://nextgov.com/2025/12/09/hellaswag_gemini_ultra_2_edge",
              "published": "2025-12-09",
              "snippet": "HellaSwag has become the standard for evaluating Gemini Ultra 2 Edge..."
            },
            {
              "docId": "2025-12-22_openai_blog_5828",
              "url": "https://openai.com/blog/2025/12/22/hellaswag_gemini_ultra_2_edge",
              "published": "2025-12-22",
              "snippet": "HellaSwag provides standardized evaluation of Gemini Ultra 2 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_plus_winogrande_plus_evaluated_on",
          "source": "model:dall_e_4_plus",
          "target": "benchmark:winogrande_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-10_langchain_blog_9784",
              "url": "https://blog.langchain.dev/2026/01/10/dall_e_4_plus_winogrande_plus",
              "published": "2026-01-10",
              "snippet": "DALL-E 4 Plus achieves 90% on WinoGrande Plus, setting a new record..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_9046",
              "url": "https://huggingface.co/blog/2026/01/25/dall_e_4_plus_winogrande_plus",
              "published": "2026-01-25",
              "snippet": "Evaluation results show DALL-E 4 Plus reaching 93% on WinoGrande Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_lite_llama_4_plus_measures",
          "source": "benchmark:arc_agi_lite",
          "target": "model:llama_4_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:slimpajama_plus_qlora_mini_uses_tech",
          "source": "dataset:slimpajama_plus",
          "target": "tech:qlora_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_retrieval_augmented_generation_next_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:retrieval_augmented_generation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:vllm_core_llama_4_core_integrates_with",
          "source": "tool:vllm_core",
          "target": "model:llama_4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-05_the_verge_7876",
              "url": "https://theverge.com/2026/01/05/vllm_core_llama_4_core",
              "published": "2026-01-05",
              "snippet": "vLLM Core announced official support for Llama 4 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_midjourney_v7_next_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:midjourney_v7_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:fineweb_next_lora_lite_uses_tech",
          "source": "dataset:fineweb_next",
          "target": "tech:lora_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_tokenizer_bpe_lite_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-17_the_gradient_3195",
              "url": "https://thegradient.pub/2026/01/17/langchain_ultra_tokenizer_bpe_",
              "published": "2026-01-17",
              "snippet": "Under the hood, langchain Ultra implements Tokenizer BPE Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_chain_of_thought_core_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:chain_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:localai_core_mixtral_8x22b_core_integrates_with",
          "source": "tool:localai_core",
          "target": "model:mixtral_8x22b_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-02_anthropic_blog_4302",
              "url": "https://anthropic.com/news/2026/01/02/localai_core_mixtral_8x22b_cor",
              "published": "2026-01-02",
              "snippet": "LocalAI Core announced official support for Mixtral 8x22B Core..."
            },
            {
              "docId": "2026-01-18_arxiv_4486",
              "url": "https://arxiv.org/abs/2026/01/18/localai_core_mixtral_8x22b_cor",
              "published": "2026-01-18",
              "snippet": "The latest release of LocalAI Core adds native Mixtral 8x22B Core integration..."
            },
            {
              "docId": "2026-01-22_techcrunch_2756",
              "url": "https://techcrunch.com/2026/01/22/localai_core_mixtral_8x22b_cor",
              "published": "2026-01-22",
              "snippet": "The latest release of LocalAI Core adds native Mixtral 8x22B Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_jamba_2_lite_integrates_with",
          "source": "tool:tensorrt_llm_ultra",
          "target": "model:jamba_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_3347",
              "url": "https://blogs.nvidia.com/2026/01/25/tensorrt_llm_ultra_jamba_2_lit",
              "published": "2026-01-25",
              "snippet": "The latest release of TensorRT-LLM Ultra adds native Jamba 2 Lite integration..."
            },
            {
              "docId": "2026-01-25_reuters_6205",
              "url": "https://reuters.com/technology/2026/01/25/tensorrt_llm_ultra_jamba_2_lit",
              "published": "2026-01-25",
              "snippet": "TensorRT-LLM Ultra now supports Jamba 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_plus_palm_3_next_measures",
          "source": "benchmark:humaneval_plus",
          "target": "model:palm_3_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-07_mit_technology__5332",
              "url": "https://technologyreview.com/2026/01/07/humaneval_plus_palm_3_next",
              "published": "2026-01-07",
              "snippet": "HumanEval Plus provides standardized evaluation of PaLM 3 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_gsm8k_mini_evaluated_on",
          "source": "model:whisper_v4_core",
          "target": "benchmark:gsm8k_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_8095",
              "url": "https://openai.com/blog/2026/01/25/whisper_v4_core_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "On the GSM8K Mini benchmark, Whisper v4 Core scored 90%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_max_mixture_of_experts_ultra_uses_tech",
          "source": "tool:dify_max",
          "target": "tech:mixture_of_experts_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_tree_of_thought_next_uses_tech",
          "source": "paper:llm_agents:_a_survey",
          "target": "tech:tree_of_thought_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-20_the_verge_4440",
              "url": "https://theverge.com/2025/12/20/llm_agents:_a_survey_tree_of_t",
              "published": "2025-12-20",
              "snippet": "Technical details reveal LLM Agents: A Survey relies heavily on Tree of Thought Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_sparse_attention_mini_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:sparse_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:gsm8k_ultra_claude_opus_45_lite_measures",
          "source": "benchmark:gsm8k_ultra",
          "target": "model:claude_opus_45_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-04_anthropic_blog_5708",
              "url": "https://anthropic.com/news/2025/12/04/gsm8k_ultra_claude_opus_45_lit",
              "published": "2025-12-04",
              "snippet": "GSM8K Ultra provides standardized evaluation of Claude Opus 4.5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_max_flash_attention_edge_uses_tech",
          "source": "dataset:c4_max",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_sora_2_plus_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:sora_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-10_weights_and_bia_2035",
              "url": "https://wandb.ai/articles/2026/01/10/transformers_pro_sora_2_plus",
              "published": "2026-01-10",
              "snippet": "The latest release of transformers Pro adds native Sora 2 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_starcoder_data_core_trained_on",
          "source": "model:command_r_plus",
          "target": "dataset:starcoder_data_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_ultra_midjourney_v7_mini_depends_on",
          "source": "model:midjourney_v7_ultra",
          "target": "model:midjourney_v7_mini",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:gpt4all_gguf_edge_uses_tech",
          "source": "repo:gpt4all",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_edge_deepseek_v3_ultra_measures",
          "source": "benchmark:bigbench_hard_edge",
          "target": "model:deepseek_v3_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_2093",
              "url": "https://blogs.nvidia.com/2026/01/25/bigbench_hard_edge_deepseek_v3",
              "published": "2026-01-25",
              "snippet": "The BigBench Hard Edge benchmark measures DeepSeek-V3 Ultra across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_alpacaeval_2_mini_evaluated_on",
          "source": "model:yi_large_next",
          "target": "benchmark:alpacaeval_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-27_meta_ai_blog_1458",
              "url": "https://ai.meta.com/blog/2025/12/27/yi_large_next_alpacaeval_2_min",
              "published": "2025-12-27",
              "snippet": "Evaluation results show Yi-Large Next reaching 81% on AlpacaEval 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_refinedweb_trained_on",
          "source": "model:claude_opus_45_core",
          "target": "dataset:refinedweb",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:winogrande_pro_sora_2_next_measures",
          "source": "benchmark:winogrande_pro",
          "target": "model:sora_2_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_edge_mixtral_8x22b_max_measures",
          "source": "benchmark:lmsys_chatbot_arena_edge",
          "target": "model:mixtral_8x22b_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-11-30_nvidia_blog_1056",
              "url": "https://blogs.nvidia.com/2025/11/30/lmsys_chatbot_arena_edge_mixtr",
              "published": "2025-11-30",
              "snippet": "LMSYS Chatbot Arena Edge has become the standard for evaluating Mixtral 8x22B Max..."
            },
            {
              "docId": "2025-12-11_meta_ai_blog_3576",
              "url": "https://ai.meta.com/blog/2025/12/11/lmsys_chatbot_arena_edge_mixtr",
              "published": "2025-12-11",
              "snippet": "LMSYS Chatbot Arena Edge provides standardized evaluation of Mixtral 8x22B Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_core_dpo_uses_tech",
          "source": "dataset:starcoder_data_core",
          "target": "tech:dpo",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_rotary_position_embedding_uses_tech",
          "source": "model:gpt_4o_mini_2",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_edge_grok_3_max_depends_on",
          "source": "model:stable_diffusion_4_edge",
          "target": "model:grok_3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:refinedweb_max_dpo_lite_uses_tech",
          "source": "dataset:refinedweb_max",
          "target": "tech:dpo_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:sora_2_core_truthfulqa_edge_evaluated_on",
          "source": "model:sora_2_core",
          "target": "benchmark:truthfulqa_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-17_the_gradient_4367",
              "url": "https://thegradient.pub/2026/01/17/sora_2_core_truthfulqa_edge",
              "published": "2026-01-17",
              "snippet": "Sora 2 Core achieves 75% on TruthfulQA Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_codex_2_core_integrates_with",
          "source": "tool:llamaindex_pro",
          "target": "model:codex_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:mt_bench_plus_stable_diffusion_4_plus_measures",
          "source": "benchmark:mt_bench_plus",
          "target": "model:stable_diffusion_4_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:llamaindex_core_llama_4_v2_integrates_with",
          "source": "tool:llamaindex_core",
          "target": "model:llama_4_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:swe_bench_plus_nemotron_5_lite_measures",
          "source": "benchmark:swe_bench_plus",
          "target": "model:nemotron_5_lite",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__gemini_ultra_2_core_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:gemini_ultra_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-24_weights_and_bia_5275",
              "url": "https://wandb.ai/articles/2026/01/24/constitutional_ai:_harmlessnes",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Constitutional AI: Harmlessness from AI Feedback reaching 86% on Gemini Ultra 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_mini_gemma_3_mini_integrates_with",
          "source": "repo:localai_mini",
          "target": "model:gemma_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-25_arxiv_4126",
              "url": "https://arxiv.org/abs/2026/01/25/localai_mini_gemma_3_mini",
              "published": "2026-01-25",
              "snippet": "LocalAI Mini now supports Gemma 3 Mini with full feature parity..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_8347",
              "url": "https://blogs.nvidia.com/2026/01/25/localai_mini_gemma_3_mini",
              "published": "2026-01-25",
              "snippet": "LocalAI Mini now supports Gemma 3 Mini with full feature parity..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_5553",
              "url": "https://blogs.nvidia.com/2026/01/25/localai_mini_gemma_3_mini",
              "published": "2026-01-25",
              "snippet": "LocalAI Mini now supports Gemma 3 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_autogpt_pro_integrates_with",
          "source": "tool:cursor_v2",
          "target": "tool:autogpt_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_wired_2446",
              "url": "https://wired.com/2026/01/25/cursor_v2_autogpt_pro",
              "published": "2026-01-25",
              "snippet": "Cursor v2 announced official support for AutoGPT Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_midjourney_v7_core_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:midjourney_v7_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-15_openai_blog_5635",
              "url": "https://openai.com/blog/2026/01/15/flash_attention:_fast_and_memo",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 91% on Midjourney V7 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_core_llamaindex_integrates_with",
          "source": "tool:cursor_core",
          "target": "tool:llamaindex",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:mt_bench_plus_gpt_5_lite_measures",
          "source": "benchmark:mt_bench_plus",
          "target": "model:gpt_5_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-25_google_ai_blog_1792",
              "url": "https://blog.google/technology/ai/2025/12/25/mt_bench_plus_gpt_5_lite",
              "published": "2025-12-25",
              "snippet": "MT-Bench Plus has become the standard for evaluating GPT-5 Lite..."
            },
            {
              "docId": "2026-01-18_nvidia_blog_3335",
              "url": "https://blogs.nvidia.com/2026/01/18/mt_bench_plus_gpt_5_lite",
              "published": "2026-01-18",
              "snippet": "MT-Bench Plus has become the standard for evaluating GPT-5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_max_tensorrt_llm_edge_integrates_with",
          "source": "tool:flowise_max",
          "target": "tool:tensorrt_llm_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-14_nextgov_3859",
              "url": "https://nextgov.com/2026/01/14/flowise_max_tensorrt_llm_edge",
              "published": "2026-01-14",
              "snippet": "The latest release of Flowise Max adds native TensorRT-LLM Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_laion_5b_ultra_trained_on",
          "source": "model:claude_sonnet_4",
          "target": "dataset:laion_5b_ultra",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_6252",
              "url": "https://blog.langchain.dev/2026/01/17/claude_sonnet_4_laion_5b_ultra",
              "published": "2026-01-17",
              "snippet": "Claude Sonnet 4 was trained on LAION-5B Ultra comprising billions of tokens..."
            },
            {
              "docId": "2026-01-24_techcrunch_7149",
              "url": "https://techcrunch.com/2026/01/24/claude_sonnet_4_laion_5b_ultra",
              "published": "2026-01-24",
              "snippet": "The training corpus for Claude Sonnet 4 includes LAION-5B Ultra..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_7181",
              "url": "https://huggingface.co/blog/2026/01/24/claude_sonnet_4_laion_5b_ultra",
              "published": "2026-01-24",
              "snippet": "Claude Sonnet 4 was trained on LAION-5B Ultra comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_next_palm_3_v2_measures",
          "source": "benchmark:mt_bench_next",
          "target": "model:palm_3_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-23_the_gradient_2485",
              "url": "https://thegradient.pub/2025/12/23/mt_bench_next_palm_3_v2",
              "published": "2025-12-23",
              "snippet": "The MT-Bench Next benchmark measures PaLM 3 v2 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_semantic_kernel_max_integrates_with",
          "source": "tool:vllm_pro",
          "target": "tool:semantic_kernel_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_mini_command_r_plus_edge_integrates_with",
          "source": "tool:tensorrt_llm_mini",
          "target": "model:command_r_plus_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:sora_2_pro_codex_2_edge_depends_on",
          "source": "model:sora_2_pro",
          "target": "model:codex_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_lite_hellaswag_evaluated_on",
          "source": "model:command_r_plus_lite",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-15_venturebeat_8494",
              "url": "https://venturebeat.com/2026/01/15/command_r_plus_lite_hellaswag",
              "published": "2026-01-15",
              "snippet": "Command R+ Lite achieves 79% on HellaSwag, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_plus_jamba_2_integrates_with",
          "source": "tool:vllm_plus",
          "target": "model:jamba_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_edge_weights_and_biases_integrates_with",
          "source": "tool:tensorrt_llm_edge",
          "target": "tool:weights_and_biases",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_alpacaeval_2_plus_evaluated_on",
          "source": "model:command_r_plus",
          "target": "benchmark:alpacaeval_2_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-19_meta_ai_blog_2836",
              "url": "https://ai.meta.com/blog/2026/01/19/command_r_plus_alpacaeval_2_pl",
              "published": "2026-01-19",
              "snippet": "Command R+ achieves 82% on AlpacaEval 2 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_mixtral_8x22b_core_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:mixtral_8x22b_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-08_anthropic_blog_4573",
              "url": "https://anthropic.com/news/2026/01/08/gpt4all_mixtral_8x22b_core",
              "published": "2026-01-08",
              "snippet": "gpt4all announced official support for Mixtral 8x22B Core..."
            },
            {
              "docId": "2026-01-25_ars_technica_8359",
              "url": "https://arstechnica.com/2026/01/25/gpt4all_mixtral_8x22b_core",
              "published": "2026-01-25",
              "snippet": "gpt4all now supports Mixtral 8x22B Core with full feature parity..."
            },
            {
              "docId": "2026-01-25_bloomberg_8117",
              "url": "https://bloomberg.com/technology/2026/01/25/gpt4all_mixtral_8x22b_core",
              "published": "2026-01-25",
              "snippet": "gpt4all now supports Mixtral 8x22B Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_rlhf_plus_uses_tech",
          "source": "repo:transformers",
          "target": "tech:rlhf_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_flash_attention_mini_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-23_the_gradient_5067",
              "url": "https://thegradient.pub/2026/01/23/langchain_ultra_flash_attentio",
              "published": "2026-01-23",
              "snippet": "Technical details reveal langchain Ultra relies heavily on Flash Attention Mini..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_8367",
              "url": "https://wandb.ai/articles/2026/01/25/langchain_ultra_flash_attentio",
              "published": "2026-01-25",
              "snippet": "langchain Ultra leverages Flash Attention Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_edge_gguf_edge_uses_tech",
          "source": "dataset:openwebtext2_edge",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:ollama_localai_max_integrates_with",
          "source": "tool:ollama",
          "target": "tool:localai_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_mt_bench_evaluated_on",
          "source": "model:claude_opus_45_core",
          "target": "benchmark:mt_bench",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:localai_mini_nemotron_5_core_integrates_with",
          "source": "repo:localai_mini",
          "target": "model:nemotron_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_sora_2_ultra_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:sora_2_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_lite_winogrande_plus_evaluated_on",
          "source": "model:gpt_4o_mini_2_lite",
          "target": "benchmark:winogrande_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_reuters_2644",
              "url": "https://reuters.com/technology/2026/01/25/gpt_4o_mini_2_lite_winogrande_",
              "published": "2026-01-25",
              "snippet": "Evaluation results show GPT-4o Mini 2 Lite reaching 96% on WinoGrande Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_v2_command_r_plus_lite_integrates_with",
          "source": "tool:crewai_v2",
          "target": "model:command_r_plus_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-20_nextgov_3329",
              "url": "https://nextgov.com/2026/01/20/crewai_v2_command_r_plus_lite",
              "published": "2026-01-20",
              "snippet": "The latest release of CrewAI v2 adds native Command R+ Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_core_qlora_mini_uses_tech",
          "source": "dataset:redpajama_v2_core",
          "target": "tech:qlora_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_mini_yi_large_pro_depends_on",
          "source": "model:whisper_v4_mini",
          "target": "model:yi_large_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:winogrande_aya_3_pro_measures",
          "source": "benchmark:winogrande",
          "target": "model:aya_3_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-23_ars_technica_6331",
              "url": "https://arstechnica.com/2026/01/23/winogrande_aya_3_pro",
              "published": "2026-01-23",
              "snippet": "The WinoGrande benchmark measures Aya 3 Pro across multiple tasks..."
            },
            {
              "docId": "2026-01-23_the_verge_1948",
              "url": "https://theverge.com/2026/01/23/winogrande_aya_3_pro",
              "published": "2026-01-23",
              "snippet": "WinoGrande has become the standard for evaluating Aya 3 Pro..."
            },
            {
              "docId": "2026-01-23_google_ai_blog_7934",
              "url": "https://blog.google/technology/ai/2026/01/23/winogrande_aya_3_pro",
              "published": "2026-01-23",
              "snippet": "The WinoGrande benchmark measures Aya 3 Pro across multiple tasks..."
            },
            {
              "docId": "2026-01-23_bloomberg_8683",
              "url": "https://bloomberg.com/technology/2026/01/23/winogrande_aya_3_pro",
              "published": "2026-01-23",
              "snippet": "The WinoGrande benchmark measures Aya 3 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_crewai_max_integrates_with",
          "source": "tool:mlflow_v2",
          "target": "tool:crewai_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-12_langchain_blog_6778",
              "url": "https://blog.langchain.dev/2026/01/12/mlflow_v2_crewai_max",
              "published": "2026-01-12",
              "snippet": "The latest release of MLflow v2 adds native CrewAI Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_lite_jamba_2_core_integrates_with",
          "source": "tool:weights_and_biases_lite",
          "target": "model:jamba_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_claude_sonnet_4_next_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:claude_sonnet_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_6059",
              "url": "https://arxiv.org/abs/2026/01/24/textbooks_are_all_you_need_max",
              "published": "2026-01-24",
              "snippet": "Textbooks Are All You Need Max achieves 97% on Claude Sonnet 4 Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_tool_use_plus_uses_tech",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "tech:tool_use_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_speculative_decoding_max_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:streamlit_max_copilot_edge_integrates_with",
          "source": "tool:streamlit_max",
          "target": "tool:copilot_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_qwen_3_mini_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "model:qwen_3_mini",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:gsm8k_plus_stable_diffusion_4_ultra_measures",
          "source": "benchmark:gsm8k_plus",
          "target": "model:stable_diffusion_4_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-19_google_ai_blog_8994",
              "url": "https://blog.google/technology/ai/2026/01/19/gsm8k_plus_stable_diffusion_4_",
              "published": "2026-01-19",
              "snippet": "The GSM8K Plus benchmark measures Stable Diffusion 4 Ultra across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_next_sparse_attention_pro_uses_tech",
          "source": "dataset:openwebtext2_next",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_codex_2_v2_depends_on",
          "source": "model:midjourney_v7_core",
          "target": "model:codex_2_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:gpt4all_claude_opus_45_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:claude_opus_45",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_9114",
              "url": "https://huggingface.co/blog/2026/01/25/gpt4all_claude_opus_45",
              "published": "2026-01-25",
              "snippet": "gpt4all announced official support for Claude Opus 4.5..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_9768",
              "url": "https://blog.google/technology/ai/2026/01/25/gpt4all_claude_opus_45",
              "published": "2026-01-25",
              "snippet": "The latest release of gpt4all adds native Claude Opus 4.5 integration..."
            },
            {
              "docId": "2026-01-25_wired_8956",
              "url": "https://wired.com/2026/01/25/gpt4all_claude_opus_45",
              "published": "2026-01-25",
              "snippet": "The latest release of gpt4all adds native Claude Opus 4.5 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_stable_diffusion_4_integrates_with",
          "source": "tool:vllm_pro",
          "target": "model:stable_diffusion_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-20_arxiv_1039",
              "url": "https://arxiv.org/abs/2026/01/20/vllm_pro_stable_diffusion_4",
              "published": "2026-01-20",
              "snippet": "vLLM Pro now supports Stable Diffusion 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_next_mixture_of_experts_max_uses_tech",
          "source": "tool:streamlit_next",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-17_meta_ai_blog_2081",
              "url": "https://ai.meta.com/blog/2026/01/17/streamlit_next_mixture_of_expe",
              "published": "2026-01-17",
              "snippet": "Streamlit Next leverages Mixture of Experts Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_v2_gsm8k_plus_evaluated_on",
          "source": "model:palm_3_v2",
          "target": "benchmark:gsm8k_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-03_wired_3695",
              "url": "https://wired.com/2025/12/03/palm_3_v2_gsm8k_plus",
              "published": "2025-12-03",
              "snippet": "On the GSM8K Plus benchmark, PaLM 3 v2 scored 78%..."
            },
            {
              "docId": "2025-12-15_the_verge_9364",
              "url": "https://theverge.com/2025/12/15/palm_3_v2_gsm8k_plus",
              "published": "2025-12-15",
              "snippet": "PaLM 3 v2 achieves 93% on GSM8K Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_edge_starcoder_data_mini_trained_on",
          "source": "model:mixtral_8x22b_edge",
          "target": "dataset:starcoder_data_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:gpt4all_core_codex_2_plus_integrates_with",
          "source": "repo:gpt4all_core",
          "target": "model:codex_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_edge_gpt_5_lite_measures",
          "source": "benchmark:lmsys_chatbot_arena_edge",
          "target": "model:gpt_5_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-01_nextgov_7802",
              "url": "https://nextgov.com/2026/01/01/lmsys_chatbot_arena_edge_gpt_5",
              "published": "2026-01-01",
              "snippet": "LMSYS Chatbot Arena Edge has become the standard for evaluating GPT-5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_pro_qwen_3_pro_integrates_with",
          "source": "tool:haystack_pro",
          "target": "model:qwen_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_plus_grok_3_ultra_measures",
          "source": "benchmark:lmsys_chatbot_arena_plus",
          "target": "model:grok_3_ultra",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:qwen_3_mini_fineweb_lite_trained_on",
          "source": "model:qwen_3_mini",
          "target": "dataset:fineweb_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_flash_attention_edge_uses_tech",
          "source": "repo:text_generation_webui_pro",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-04_hugging_face_bl_7335",
              "url": "https://huggingface.co/blog/2026/01/04/text_generation_webui_pro_flas",
              "published": "2026-01-04",
              "snippet": "Technical details reveal text-generation-webui Pro relies heavily on Flash Attention Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_max_gguf_lite_uses_tech",
          "source": "tool:cursor_max",
          "target": "tech:gguf_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_redpajama_v2_pro_trained_on",
          "source": "model:sora_2_plus",
          "target": "dataset:redpajama_v2_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_whisper_v4_core_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:whisper_v4_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:phi_4_plus_tool_use_v2_uses_tech",
          "source": "model:phi_4_plus",
          "target": "tech:tool_use_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-26_anthropic_blog_3043",
              "url": "https://anthropic.com/news/2025/12/26/phi_4_plus_tool_use_v2",
              "published": "2025-12-26",
              "snippet": "Technical details reveal Phi-4 Plus relies heavily on Tool Use v2..."
            },
            {
              "docId": "2026-01-25_ars_technica_2037",
              "url": "https://arstechnica.com/2026/01/25/phi_4_plus_tool_use_v2",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Plus relies heavily on Tool Use v2..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_5776",
              "url": "https://huggingface.co/blog/2026/01/25/phi_4_plus_tool_use_v2",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Plus relies heavily on Tool Use v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_tree_of_thought_core_uses_tech",
          "source": "repo:ollama_ultra",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-21_venturebeat_8126",
              "url": "https://venturebeat.com/2026/01/21/ollama_ultra_tree_of_thought_c",
              "published": "2026-01-21",
              "snippet": "Under the hood, ollama Ultra implements Tree of Thought Core for improved efficiency..."
            },
            {
              "docId": "2026-01-22_ars_technica_5667",
              "url": "https://arstechnica.com/2026/01/22/ollama_ultra_tree_of_thought_c",
              "published": "2026-01-22",
              "snippet": "Under the hood, ollama Ultra implements Tree of Thought Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_whisper_v4_core_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:whisper_v4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-09_anthropic_blog_9037",
              "url": "https://anthropic.com/news/2026/01/09/vllm_pro_whisper_v4_core",
              "published": "2026-01-09",
              "snippet": "vllm Pro announced official support for Whisper v4 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_claude_sonnet_4_next_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:claude_sonnet_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-20_the_gradient_8531",
              "url": "https://thegradient.pub/2026/01/20/llm_agents:_a_survey_next_clau",
              "published": "2026-01-20",
              "snippet": "On the Claude Sonnet 4 Next benchmark, LLM Agents: A Survey Next scored 82%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_kv_cache_optimization_uses_tech",
          "source": "repo:autogpt_plus",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-10_bloomberg_3583",
              "url": "https://bloomberg.com/technology/2026/01/10/autogpt_plus_kv_cache_optimiza",
              "published": "2026-01-10",
              "snippet": "Technical details reveal AutoGPT Plus relies heavily on KV Cache Optimization..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_lite_sora_2_mini_integrates_with",
          "source": "tool:semantic_kernel_lite",
          "target": "model:sora_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_8638",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/semantic_kernel_lite_sora_2_mi",
              "published": "2026-01-25",
              "snippet": "The latest release of Semantic Kernel Lite adds native Sora 2 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_fineweb_ultra_trained_on",
          "source": "model:phi_4",
          "target": "dataset:fineweb_ultra",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_palm_3_edge_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:palm_3_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-20_meta_ai_blog_4212",
              "url": "https://ai.meta.com/blog/2026/01/20/langchain_plus_palm_3_edge",
              "published": "2026-01-20",
              "snippet": "The latest release of langchain Plus adds native PaLM 3 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_core_llama_4_ultra_depends_on",
          "source": "model:llama_4_core",
          "target": "model:llama_4_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_chain_of_thought_ultra_uses_tech",
          "source": "model:gemini_ultra_2",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2025-12-27_google_ai_blog_1695",
              "url": "https://blog.google/technology/ai/2025/12/27/gemini_ultra_2_chain_of_though",
              "published": "2025-12-27",
              "snippet": "Under the hood, Gemini Ultra 2 implements Chain-of-Thought Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_sora_2_ultra_integrates_with",
          "source": "repo:autogpt_plus",
          "target": "model:sora_2_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_the_gradient_4831",
              "url": "https://thegradient.pub/2026/01/25/autogpt_plus_sora_2_ultra",
              "published": "2026-01-25",
              "snippet": "The latest release of AutoGPT Plus adds native Sora 2 Ultra integration..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_2080",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/autogpt_plus_sora_2_ultra",
              "published": "2026-01-25",
              "snippet": "AutoGPT Plus announced official support for Sora 2 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_lora_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-08_the_gradient_5478",
              "url": "https://thegradient.pub/2026/01/08/gpt4all_pro_lora",
              "published": "2026-01-08",
              "snippet": "Under the hood, gpt4all Pro implements LoRA for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_chain_of_thought_lite_uses_tech",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "tech:chain_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-06_microsoft_resea_9628",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/06/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-06",
              "snippet": "Under the hood, Tree of Thoughts: Deliberate Problem Solving with LLMs implements Chain-of-Thought Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_core_gguf_mini_uses_tech",
          "source": "dataset:common_crawl_core",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_palm_3_v2_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:palm_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-30_nextgov_9472",
              "url": "https://nextgov.com/2025/12/30/vllm_pro_palm_3_v2",
              "published": "2025-12-30",
              "snippet": "The latest release of vllm Pro adds native PaLM 3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_max_hellaswag_next_evaluated_on",
          "source": "model:claude_sonnet_4_max",
          "target": "benchmark:hellaswag_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-02_the_gradient_6945",
              "url": "https://thegradient.pub/2026/01/02/claude_sonnet_4_max_hellaswag_",
              "published": "2026-01-02",
              "snippet": "Evaluation results show Claude Sonnet 4 Max reaching 98% on HellaSwag Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_tool_use_max_uses_tech",
          "source": "model:qwen_3_next",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_edge_lora_core_uses_tech",
          "source": "model:stable_diffusion_4_edge",
          "target": "tech:lora_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:winogrande_v2_deepseek_v3_v2_measures",
          "source": "benchmark:winogrande_v2",
          "target": "model:deepseek_v3_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-21_nextgov_1040",
              "url": "https://nextgov.com/2025/12/21/winogrande_v2_deepseek_v3_v2",
              "published": "2025-12-21",
              "snippet": "WinoGrande v2 has become the standard for evaluating DeepSeek-V3 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_multimodal_fusion_edge_uses_tech",
          "source": "repo:transformers_edge",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:hellaswag_mini_gpt_5_next_measures",
          "source": "benchmark:hellaswag_mini",
          "target": "model:gpt_5_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-01_nextgov_3628",
              "url": "https://nextgov.com/2026/01/01/hellaswag_mini_gpt_5_next",
              "published": "2026-01-01",
              "snippet": "The HellaSwag Mini benchmark measures GPT-5 Next across multiple tasks..."
            },
            {
              "docId": "2026-01-15_mit_technology__7835",
              "url": "https://technologyreview.com/2026/01/15/hellaswag_mini_gpt_5_next",
              "published": "2026-01-15",
              "snippet": "The HellaSwag Mini benchmark measures GPT-5 Next across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_group_query_attention_mini_uses_tech",
          "source": "repo:open_interpreter_v2",
          "target": "tech:group_query_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-11_nvidia_blog_6740",
              "url": "https://blogs.nvidia.com/2026/01/11/open_interpreter_v2_group_quer",
              "published": "2026-01-11",
              "snippet": "open-interpreter v2 leverages Group Query Attention Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-16_hugging_face_bl_4419",
              "url": "https://huggingface.co/blog/2026/01/16/open_interpreter_v2_group_quer",
              "published": "2026-01-16",
              "snippet": "open-interpreter v2 leverages Group Query Attention Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_next_deepseek_v3_mini_measures",
          "source": "benchmark:humaneval_next",
          "target": "model:deepseek_v3_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_gguf_plus_uses_tech",
          "source": "model:gpt_5_v2",
          "target": "tech:gguf_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_jamba_2_lite_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:jamba_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_gpt_5_core_evaluated_on",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "model:gpt_5_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_mmlu_evaluated_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_bloomberg_8572",
              "url": "https://bloomberg.com/technology/2026/01/22/stable_diffusion_4_v2_mmlu",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Stable Diffusion 4 v2 reaching 90% on MMLU..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_gemini_ultra_2_integrates_with",
          "source": "tool:ollama_lite",
          "target": "model:gemini_ultra_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-01_ars_technica_1230",
              "url": "https://arstechnica.com/2026/01/01/ollama_lite_gemini_ultra_2",
              "published": "2026-01-01",
              "snippet": "Ollama Lite now supports Gemini Ultra 2 with full feature parity..."
            },
            {
              "docId": "2026-01-18_arxiv_4033",
              "url": "https://arxiv.org/abs/2026/01/18/ollama_lite_gemini_ultra_2",
              "published": "2026-01-18",
              "snippet": "Ollama Lite announced official support for Gemini Ultra 2..."
            },
            {
              "docId": "2026-01-21_nvidia_blog_9607",
              "url": "https://blogs.nvidia.com/2026/01/21/ollama_lite_gemini_ultra_2",
              "published": "2026-01-21",
              "snippet": "Ollama Lite now supports Gemini Ultra 2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_llama_4_measures",
          "source": "benchmark:alpacaeval_2",
          "target": "model:llama_4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-19_anthropic_blog_9833",
              "url": "https://anthropic.com/news/2026/01/19/alpacaeval_2_llama_4",
              "published": "2026-01-19",
              "snippet": "The AlpacaEval 2 benchmark measures Llama 4 across multiple tasks..."
            },
            {
              "docId": "2026-01-21_the_verge_4146",
              "url": "https://theverge.com/2026/01/21/alpacaeval_2_llama_4",
              "published": "2026-01-21",
              "snippet": "AlpacaEval 2 provides standardized evaluation of Llama 4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_rlhf_next_uses_tech",
          "source": "paper:direct_preference_optimization_edge",
          "target": "tech:rlhf_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-06_meta_ai_blog_3607",
              "url": "https://ai.meta.com/blog/2025/12/06/direct_preference_optimization",
              "published": "2025-12-06",
              "snippet": "Direct Preference Optimization Edge leverages RLHF Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_qwen_3_lite_integrates_with",
          "source": "tool:semantic_kernel",
          "target": "model:qwen_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-13_anthropic_blog_2644",
              "url": "https://anthropic.com/news/2026/01/13/semantic_kernel_qwen_3_lite",
              "published": "2026-01-13",
              "snippet": "The latest release of Semantic Kernel adds native Qwen-3 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_plus_quantization_edge_uses_tech",
          "source": "repo:llamacpp_plus",
          "target": "tech:quantization_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:gpqa_claude_sonnet_4_plus_measures",
          "source": "benchmark:gpqa",
          "target": "model:claude_sonnet_4_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-15_nextgov_6128",
              "url": "https://nextgov.com/2026/01/15/gpqa_claude_sonnet_4_plus",
              "published": "2026-01-15",
              "snippet": "GPQA has become the standard for evaluating Claude Sonnet 4 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_sliding_window_attention_next_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:codex_2_plus_dall_e_4_edge_depends_on",
          "source": "model:codex_2_plus",
          "target": "model:dall_e_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:phi_4_slimpajama_core_trained_on",
          "source": "model:phi_4",
          "target": "dataset:slimpajama_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.47,
          "evidence": [
            {
              "docId": "2025-12-30_nextgov_9067",
              "url": "https://nextgov.com/2025/12/30/phi_4_slimpajama_core",
              "published": "2025-12-30",
              "snippet": "The training corpus for Phi-4 includes SlimPajama Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_multimodal_fusion_max_uses_tech",
          "source": "paper:direct_preference_optimization_edge",
          "target": "tech:multimodal_fusion_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-09_techcrunch_2495",
              "url": "https://techcrunch.com/2026/01/09/direct_preference_optimization",
              "published": "2026-01-09",
              "snippet": "Direct Preference Optimization Edge leverages Multimodal Fusion Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-12_langchain_blog_8052",
              "url": "https://blog.langchain.dev/2026/01/12/direct_preference_optimization",
              "published": "2026-01-12",
              "snippet": "Under the hood, Direct Preference Optimization Edge implements Multimodal Fusion Max for improved efficiency..."
            },
            {
              "docId": "2026-01-18_arxiv_6961",
              "url": "https://arxiv.org/abs/2026/01/18/direct_preference_optimization",
              "published": "2026-01-18",
              "snippet": "Under the hood, Direct Preference Optimization Edge implements Multimodal Fusion Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_ultra_red_teaming_core_uses_tech",
          "source": "dataset:laion_5b_ultra",
          "target": "tech:red_teaming_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:vllm_max_chain_of_thought_next_uses_tech",
          "source": "tool:vllm_max",
          "target": "tech:chain_of_thought_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:cody_ultra_rotary_position_embedding_ultra_uses_tech",
          "source": "tool:cody_ultra",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-21_the_gradient_7733",
              "url": "https://thegradient.pub/2026/01/21/cody_ultra_rotary_position_emb",
              "published": "2026-01-21",
              "snippet": "Cody Ultra leverages Rotary Position Embedding Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dolma_next_tool_use_max_uses_tech",
          "source": "dataset:dolma_next",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_claude_sonnet_4_v2_depends_on",
          "source": "model:mixtral_8x22b_mini",
          "target": "model:claude_sonnet_4_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gpt_5_max_starcoder_data_trained_on",
          "source": "model:gpt_5_max",
          "target": "dataset:starcoder_data",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_lite_stable_diffusion_4_ultra_measures",
          "source": "benchmark:alpacaeval_2_lite",
          "target": "model:stable_diffusion_4_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-13_bloomberg_2471",
              "url": "https://bloomberg.com/technology/2026/01/13/alpacaeval_2_lite_stable_diffu",
              "published": "2026-01-13",
              "snippet": "AlpacaEval 2 Lite has become the standard for evaluating Stable Diffusion 4 Ultra..."
            },
            {
              "docId": "2026-01-22_google_ai_blog_3941",
              "url": "https://blog.google/technology/ai/2026/01/22/alpacaeval_2_lite_stable_diffu",
              "published": "2026-01-22",
              "snippet": "The AlpacaEval 2 Lite benchmark measures Stable Diffusion 4 Ultra across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_falcon_3_core_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "model:falcon_3_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-16_reuters_1639",
              "url": "https://reuters.com/technology/2026/01/16/textbooks_are_all_you_need_nex",
              "published": "2026-01-16",
              "snippet": "Textbooks Are All You Need Next achieves 90% on Falcon 3 Core, setting a new record..."
            },
            {
              "docId": "2026-01-19_google_ai_blog_5672",
              "url": "https://blog.google/technology/ai/2026/01/19/textbooks_are_all_you_need_nex",
              "published": "2026-01-19",
              "snippet": "Textbooks Are All You Need Next achieves 87% on Falcon 3 Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_lite_distillation_uses_tech",
          "source": "model:gpt_4o_mini_2_lite",
          "target": "tech:distillation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-21_nvidia_blog_6503",
              "url": "https://blogs.nvidia.com/2026/01/21/gpt_4o_mini_2_lite_distillatio",
              "published": "2026-01-21",
              "snippet": "Under the hood, GPT-4o Mini 2 Lite implements Distillation for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_aya_3_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:aya_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-16_google_ai_blog_3137",
              "url": "https://blog.google/technology/ai/2025/12/16/attention_is_all_you_need_v2_u",
              "published": "2025-12-16",
              "snippet": "Evaluation results show Attention Is All You Need v2 Ultra reaching 96% on Aya 3..."
            },
            {
              "docId": "2026-01-20_openai_blog_1567",
              "url": "https://openai.com/blog/2026/01/20/attention_is_all_you_need_v2_u",
              "published": "2026-01-20",
              "snippet": "Attention Is All You Need v2 Ultra achieves 88% on Aya 3, setting a new record..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_1203",
              "url": "https://blogs.nvidia.com/2026/01/22/attention_is_all_you_need_v2_u",
              "published": "2026-01-22",
              "snippet": "Attention Is All You Need v2 Ultra achieves 88% on Aya 3, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_synthetic_data_generation_core_uses_tech",
          "source": "repo:transformers_edge",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-10_reuters_6527",
              "url": "https://reuters.com/technology/2026/01/10/transformers_edge_synthetic_da",
              "published": "2026-01-10",
              "snippet": "Under the hood, transformers Edge implements Synthetic Data Generation Core for improved efficiency..."
            },
            {
              "docId": "2026-01-17_microsoft_resea_6642",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/17/transformers_edge_synthetic_da",
              "published": "2026-01-17",
              "snippet": "Under the hood, transformers Edge implements Synthetic Data Generation Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_roots_next_trained_on",
          "source": "model:deepseek_v3",
          "target": "dataset:roots_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.49,
          "evidence": [
            {
              "docId": "2026-01-04_anthropic_blog_4809",
              "url": "https://anthropic.com/news/2026/01/04/deepseek_v3_roots_next",
              "published": "2026-01-04",
              "snippet": "DeepSeek-V3 was trained on ROOTS Next comprising billions of tokens..."
            },
            {
              "docId": "2026-01-10_the_verge_8745",
              "url": "https://theverge.com/2026/01/10/deepseek_v3_roots_next",
              "published": "2026-01-10",
              "snippet": "DeepSeek-V3 was trained on ROOTS Next comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_cursor_max_integrates_with",
          "source": "tool:llamaindex_pro",
          "target": "tool:cursor_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-24_bloomberg_8173",
              "url": "https://bloomberg.com/technology/2026/01/24/llamaindex_pro_cursor_max",
              "published": "2026-01-24",
              "snippet": "LlamaIndex Pro announced official support for Cursor Max..."
            },
            {
              "docId": "2026-01-25_venturebeat_5362",
              "url": "https://venturebeat.com/2026/01/25/llamaindex_pro_cursor_max",
              "published": "2026-01-25",
              "snippet": "The latest release of LlamaIndex Pro adds native Cursor Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_lite_whisper_v4_integrates_with",
          "source": "tool:mlflow_lite",
          "target": "model:whisper_v4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-27_ars_technica_4159",
              "url": "https://arstechnica.com/2025/12/27/mlflow_lite_whisper_v4",
              "published": "2025-12-27",
              "snippet": "The latest release of MLflow Lite adds native Whisper v4 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_core_mixtral_8x22b_ultra_integrates_with",
          "source": "tool:gradio_core",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-16_langchain_blog_2474",
              "url": "https://blog.langchain.dev/2026/01/16/gradio_core_mixtral_8x22b_ultr",
              "published": "2026-01-16",
              "snippet": "Gradio Core now supports Mixtral 8x22B Ultra with full feature parity..."
            },
            {
              "docId": "2026-01-21_nextgov_6610",
              "url": "https://nextgov.com/2026/01/21/gradio_core_mixtral_8x22b_ultr",
              "published": "2026-01-21",
              "snippet": "Gradio Core announced official support for Mixtral 8x22B Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_deepseek_v3_v2_depends_on",
          "source": "model:gpt_5_v2",
          "target": "model:deepseek_v3_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_chain_of_thought_core_uses_tech",
          "source": "repo:ollama_edge",
          "target": "tech:chain_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:gpqa_ultra_nemotron_5_v2_measures",
          "source": "benchmark:gpqa_ultra",
          "target": "model:nemotron_5_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-19_bloomberg_9220",
              "url": "https://bloomberg.com/technology/2026/01/19/gpqa_ultra_nemotron_5_v2",
              "published": "2026-01-19",
              "snippet": "GPQA Ultra provides standardized evaluation of Nemotron-5 v2..."
            },
            {
              "docId": "2026-01-20_mit_technology__9639",
              "url": "https://technologyreview.com/2026/01/20/gpqa_ultra_nemotron_5_v2",
              "published": "2026-01-20",
              "snippet": "GPQA Ultra provides standardized evaluation of Nemotron-5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_plus_litellm_pro_integrates_with",
          "source": "tool:llamaindex_plus",
          "target": "tool:litellm_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_dolma_pro_trained_on",
          "source": "model:gpt_4o_mini_2",
          "target": "dataset:dolma_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:llama_4_sparse_attention_lite_uses_tech",
          "source": "model:llama_4",
          "target": "tech:sparse_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:mt_bench_v2_phi_4_ultra_measures",
          "source": "benchmark:mt_bench_v2",
          "target": "model:phi_4_ultra",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.97
        }
      },
      {
        "data": {
          "id": "e:yi_large_pro_llama_4_core_depends_on",
          "source": "model:yi_large_pro",
          "target": "model:llama_4_core",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:slimpajama_mini_rotary_position_embedding_plus_uses_tech",
          "source": "dataset:slimpajama_mini",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:langchain_rotary_position_embedding_pro_uses_tech",
          "source": "repo:langchain",
          "target": "tech:rotary_position_embedding_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_techcrunch_5859",
              "url": "https://techcrunch.com/2026/01/24/langchain_rotary_position_embe",
              "published": "2026-01-24",
              "snippet": "langchain leverages Rotary Position Embedding Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_retrieval_augmented_generation_ultra_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:retrieval_augmented_generation_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_ultra_whisper_v4_ultra_depends_on",
          "source": "model:deepseek_v3_ultra",
          "target": "model:whisper_v4_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_deepseek_v3_pro_integrates_with",
          "source": "repo:autogpt_v2",
          "target": "model:deepseek_v3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:dify_v2_grok_3_v2_integrates_with",
          "source": "tool:dify_v2",
          "target": "model:grok_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-19_venturebeat_1138",
              "url": "https://venturebeat.com/2026/01/19/dify_v2_grok_3_v2",
              "published": "2026-01-19",
              "snippet": "The latest release of Dify v2 adds native Grok-3 v2 integration..."
            },
            {
              "docId": "2026-01-23_arxiv_4629",
              "url": "https://arxiv.org/abs/2026/01/23/dify_v2_grok_3_v2",
              "published": "2026-01-23",
              "snippet": "The latest release of Dify v2 adds native Grok-3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_max_mc4_max_trained_on",
          "source": "model:midjourney_v7_max",
          "target": "dataset:mc4_max",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-15_nvidia_blog_8714",
              "url": "https://blogs.nvidia.com/2026/01/15/midjourney_v7_max_mc4_max",
              "published": "2026-01-15",
              "snippet": "The training corpus for Midjourney V7 Max includes mC4 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_edge_flash_attention_max_uses_tech",
          "source": "dataset:c4_edge",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_claude_sonnet_4_next_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:claude_sonnet_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-22_bloomberg_5106",
              "url": "https://bloomberg.com/technology/2026/01/22/llm_agents:_a_survey_claude_so",
              "published": "2026-01-22",
              "snippet": "Evaluation results show LLM Agents: A Survey reaching 85% on Claude Sonnet 4 Next..."
            },
            {
              "docId": "2026-01-22_ars_technica_1583",
              "url": "https://arstechnica.com/2026/01/22/llm_agents:_a_survey_claude_so",
              "published": "2026-01-22",
              "snippet": "LLM Agents: A Survey achieves 93% on Claude Sonnet 4 Next, setting a new record..."
            },
            {
              "docId": "2026-01-25_nextgov_7218",
              "url": "https://nextgov.com/2026/01/25/llm_agents:_a_survey_claude_so",
              "published": "2026-01-25",
              "snippet": "LLM Agents: A Survey achieves 87% on Claude Sonnet 4 Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_roots_v2_trained_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "dataset:roots_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_gemma_3_max_depends_on",
          "source": "model:command_r_plus",
          "target": "model:gemma_3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:streamlit_group_query_attention_ultra_uses_tech",
          "source": "tool:streamlit",
          "target": "tech:group_query_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_group_query_attention_plus_uses_tech",
          "source": "model:stable_diffusion_4_max",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-24_microsoft_resea_5522",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/24/stable_diffusion_4_max_group_q",
              "published": "2025-12-24",
              "snippet": "Stable Diffusion 4 Max leverages Group Query Attention Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_gemma_3_edge_evaluated_on",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "model:gemma_3_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_1898",
              "url": "https://anthropic.com/news/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "On the Gemma 3 Edge benchmark, Direct Preference Optimization Ultra scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_the_stack_v2_mini_trained_on",
          "source": "model:codex_2_core",
          "target": "dataset:the_stack_v2_mini",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-21_google_ai_blog_9641",
              "url": "https://blog.google/technology/ai/2026/01/21/codex_2_core_the_stack_v2_mini",
              "published": "2026-01-21",
              "snippet": "Codex 2 Core was trained on The Stack v2 Mini comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_mini_midjourney_v7_v2_depends_on",
          "source": "model:codex_2_mini",
          "target": "model:midjourney_v7_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_max_fineweb_lite_trained_on",
          "source": "model:midjourney_v7_max",
          "target": "dataset:fineweb_lite",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_jamba_2_ultra_depends_on",
          "source": "model:claude_sonnet_4_next",
          "target": "model:jamba_2_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:gpt4all_grok_3_next_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:grok_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_c4_ultra_trained_on",
          "source": "model:claude_sonnet_4_next",
          "target": "dataset:c4_ultra",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_lite_kv_cache_optimization_max_uses_tech",
          "source": "model:whisper_v4_lite",
          "target": "tech:kv_cache_optimization_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-14_ars_technica_6843",
              "url": "https://arstechnica.com/2025/12/14/whisper_v4_lite_kv_cache_optim",
              "published": "2025-12-14",
              "snippet": "Whisper v4 Lite leverages KV Cache Optimization Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_3369",
              "url": "https://anthropic.com/news/2026/01/25/whisper_v4_lite_kv_cache_optim",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Whisper v4 Lite relies heavily on KV Cache Optimization Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_llama_4_plus_integrates_with",
          "source": "repo:open_interpreter_v2",
          "target": "model:llama_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-15_the_verge_9673",
              "url": "https://theverge.com/2026/01/15/open_interpreter_v2_llama_4_pl",
              "published": "2026-01-15",
              "snippet": "The latest release of open-interpreter v2 adds native Llama 4 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_v2_jamba_2_lite_integrates_with",
          "source": "repo:open_interpreter_v2",
          "target": "model:jamba_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_refinedweb_next_trained_on",
          "source": "model:jamba_2_lite",
          "target": "dataset:refinedweb_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_mt_bench_v2_evaluated_on",
          "source": "model:jamba_2_mini",
          "target": "benchmark:mt_bench_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_synthetic_data_generation_max_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_lora_max_uses_tech",
          "source": "repo:text_generation_webui_max",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-19_openai_blog_9858",
              "url": "https://openai.com/blog/2026/01/19/text_generation_webui_max_lora",
              "published": "2026-01-19",
              "snippet": "Under the hood, text-generation-webui Max implements LoRA Max for improved efficiency..."
            },
            {
              "docId": "2026-01-25_langchain_blog_8395",
              "url": "https://blog.langchain.dev/2026/01/25/text_generation_webui_max_lora",
              "published": "2026-01-25",
              "snippet": "text-generation-webui Max leverages LoRA Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_falcon_3_pro_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:falcon_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_mixtral_8x22b_mini_evaluated_on",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "model:mixtral_8x22b_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-18_wired_5868",
              "url": "https://wired.com/2026/01/18/llm_agents:_a_survey_pro_mixtr",
              "published": "2026-01-18",
              "snippet": "On the Mixtral 8x22B Mini benchmark, LLM Agents: A Survey Pro scored 82%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_next_qwen_3_mini_depends_on",
          "source": "model:sora_2_next",
          "target": "model:qwen_3_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:vllm_core_flash_attention_uses_tech",
          "source": "repo:vllm_core",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-12_google_ai_blog_3836",
              "url": "https://blog.google/technology/ai/2026/01/12/vllm_core_flash_attention",
              "published": "2026-01-12",
              "snippet": "vllm Core leverages Flash Attention to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_core_chain_of_thought_max_uses_tech",
          "source": "dataset:the_pile_core",
          "target": "tech:chain_of_thought_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_tree_of_thought_plus_uses_tech",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tech:tree_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-12_nvidia_blog_5204",
              "url": "https://blogs.nvidia.com/2026/01/12/tensorrt_llm_ultra_tree_of_tho",
              "published": "2026-01-12",
              "snippet": "TensorRT-LLM Ultra leverages Tree of Thought Plus to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-15_nvidia_blog_7874",
              "url": "https://blogs.nvidia.com/2026/01/15/tensorrt_llm_ultra_tree_of_tho",
              "published": "2026-01-15",
              "snippet": "Technical details reveal TensorRT-LLM Ultra relies heavily on Tree of Thought Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_gemma_3_lite_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:gemma_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-07_bloomberg_2218",
              "url": "https://bloomberg.com/technology/2026/01/07/tensorrt_llm_gemma_3_lite",
              "published": "2026-01-07",
              "snippet": "TensorRT-LLM announced official support for Gemma 3 Lite..."
            },
            {
              "docId": "2026-01-13_bloomberg_4366",
              "url": "https://bloomberg.com/technology/2026/01/13/tensorrt_llm_gemma_3_lite",
              "published": "2026-01-13",
              "snippet": "TensorRT-LLM announced official support for Gemma 3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_pro_yi_large_next_depends_on",
          "source": "model:mixtral_8x22b_pro",
          "target": "model:yi_large_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:flowise_v2_copilot_integrates_with",
          "source": "tool:flowise_v2",
          "target": "tool:copilot",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-21_microsoft_resea_7609",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/flowise_v2_copilot",
              "published": "2026-01-21",
              "snippet": "Flowise v2 now supports Copilot with full feature parity..."
            },
            {
              "docId": "2026-01-23_bloomberg_1607",
              "url": "https://bloomberg.com/technology/2026/01/23/flowise_v2_copilot",
              "published": "2026-01-23",
              "snippet": "Flowise v2 announced official support for Copilot..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_chain_of_thought_lite_uses_tech",
          "source": "model:gpt_5_v2",
          "target": "tech:chain_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-21_meta_ai_blog_4319",
              "url": "https://ai.meta.com/blog/2026/01/21/gpt_5_v2_chain_of_thought_lite",
              "published": "2026-01-21",
              "snippet": "Under the hood, GPT-5 v2 implements Chain-of-Thought Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-24_techcrunch_2795",
              "url": "https://techcrunch.com/2026/01/24/gpt_5_v2_chain_of_thought_lite",
              "published": "2026-01-24",
              "snippet": "GPT-5 v2 leverages Chain-of-Thought Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_6450",
              "url": "https://ai.meta.com/blog/2026/01/24/gpt_5_v2_chain_of_thought_lite",
              "published": "2026-01-24",
              "snippet": "Technical details reveal GPT-5 v2 relies heavily on Chain-of-Thought Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_next_redpajama_v2_plus_trained_on",
          "source": "model:gpt_5_next",
          "target": "dataset:redpajama_v2_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_constitutional_ai_max_uses_tech",
          "source": "repo:transformers_edge",
          "target": "tech:constitutional_ai_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-24_weights_and_bia_9470",
              "url": "https://wandb.ai/articles/2026/01/24/transformers_edge_constitution",
              "published": "2026-01-24",
              "snippet": "Under the hood, transformers Edge implements Constitutional AI Max for improved efficiency..."
            },
            {
              "docId": "2026-01-24_anthropic_blog_3070",
              "url": "https://anthropic.com/news/2026/01/24/transformers_edge_constitution",
              "published": "2026-01-24",
              "snippet": "Under the hood, transformers Edge implements Constitutional AI Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_kv_cache_optimization_pro_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-15_the_verge_4077",
              "url": "https://theverge.com/2026/01/15/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-15",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models leverages KV Cache Optimization Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_the_verge_9987",
              "url": "https://theverge.com/2026/01/23/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-23",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models leverages KV Cache Optimization Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_mit_technology__6204",
              "url": "https://technologyreview.com/2026/01/25/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-25",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models leverages KV Cache Optimization Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__midjourney_v7_mini_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:midjourney_v7_mini",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_the_stack_v2_mini_trained_on",
          "source": "model:claude_opus_45_core",
          "target": "dataset:the_stack_v2_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:jamba_2_edge_mmlu_edge_evaluated_on",
          "source": "model:jamba_2_edge",
          "target": "benchmark:mmlu_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-13_mit_technology__7849",
              "url": "https://technologyreview.com/2026/01/13/jamba_2_edge_mmlu_edge",
              "published": "2026-01-13",
              "snippet": "Jamba 2 Edge achieves 82% on MMLU Edge, setting a new record..."
            },
            {
              "docId": "2026-01-15_reuters_3118",
              "url": "https://reuters.com/technology/2026/01/15/jamba_2_edge_mmlu_edge",
              "published": "2026-01-15",
              "snippet": "Jamba 2 Edge achieves 72% on MMLU Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_gemini_ultra_2_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:gemini_ultra_2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_synthetic_data_generation_lite_uses_tech",
          "source": "repo:langchain_lite",
          "target": "tech:synthetic_data_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-21_anthropic_blog_9525",
              "url": "https://anthropic.com/news/2026/01/21/langchain_lite_synthetic_data_",
              "published": "2026-01-21",
              "snippet": "Technical details reveal langchain Lite relies heavily on Synthetic Data Generation Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_max_copilot_plus_integrates_with",
          "source": "tool:localai_max",
          "target": "tool:copilot_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_qlora_plus_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:qlora_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_lite_mc4_edge_trained_on",
          "source": "model:deepseek_v3_lite",
          "target": "dataset:mc4_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-21_anthropic_blog_3870",
              "url": "https://anthropic.com/news/2026/01/21/deepseek_v3_lite_mc4_edge",
              "published": "2026-01-21",
              "snippet": "DeepSeek-V3 Lite was trained on mC4 Edge comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_red_teaming_v2_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:red_teaming_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_1391",
              "url": "https://openai.com/blog/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning leverages Red Teaming v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_red_teaming_ultra_uses_tech",
          "source": "tool:streamlit",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:falcon_3_core_qwen_3_core_depends_on",
          "source": "model:falcon_3_core",
          "target": "model:qwen_3_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_retrieval_augmented_generation_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-21_bloomberg_2633",
              "url": "https://bloomberg.com/technology/2026/01/21/flash_attention:_fast_and_memo",
              "published": "2026-01-21",
              "snippet": "Under the hood, Flash Attention: Fast and Memory-Efficient Attention implements Retrieval-Augmented Generation for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_tree_of_thought_mini_uses_tech",
          "source": "repo:autogpt_plus",
          "target": "tech:tree_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_palm_3_lite_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:palm_3_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-25_meta_ai_blog_3351",
              "url": "https://ai.meta.com/blog/2026/01/25/textbooks_are_all_you_need_min",
              "published": "2026-01-25",
              "snippet": "Textbooks Are All You Need Mini achieves 75% on PaLM 3 Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_max_jamba_2_core_measures",
          "source": "benchmark:alpacaeval_2_max",
          "target": "model:jamba_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2025-12-28_venturebeat_7295",
              "url": "https://venturebeat.com/2025/12/28/alpacaeval_2_max_jamba_2_core",
              "published": "2025-12-28",
              "snippet": "The AlpacaEval 2 Max benchmark measures Jamba 2 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_lite_common_crawl_edge_trained_on",
          "source": "model:falcon_3_lite",
          "target": "dataset:common_crawl_edge",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_slimpajama_trained_on",
          "source": "model:command_r_plus",
          "target": "dataset:slimpajama",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_claude_opus_45_max_depends_on",
          "source": "model:sora_2_edge",
          "target": "model:claude_opus_45_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_nemotron_5_pro_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:nemotron_5_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-14_meta_ai_blog_2255",
              "url": "https://ai.meta.com/blog/2026/01/14/attention_is_all_you_need_v2_u",
              "published": "2026-01-14",
              "snippet": "On the Nemotron-5 Pro benchmark, Attention Is All You Need v2 Ultra scored 73%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_claude_sonnet_4_v2_depends_on",
          "source": "model:jamba_2_lite",
          "target": "model:claude_sonnet_4_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_gpt_5_pro_integrates_with",
          "source": "repo:langchain_ultra",
          "target": "model:gpt_5_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-06_techcrunch_4529",
              "url": "https://techcrunch.com/2026/01/06/langchain_ultra_gpt_5_pro",
              "published": "2026-01-06",
              "snippet": "langchain Ultra announced official support for GPT-5 Pro..."
            },
            {
              "docId": "2026-01-22_hugging_face_bl_1016",
              "url": "https://huggingface.co/blog/2026/01/22/langchain_ultra_gpt_5_pro",
              "published": "2026-01-22",
              "snippet": "The latest release of langchain Ultra adds native GPT-5 Pro integration..."
            },
            {
              "docId": "2026-01-23_the_verge_3304",
              "url": "https://theverge.com/2026/01/23/langchain_ultra_gpt_5_pro",
              "published": "2026-01-23",
              "snippet": "langchain Ultra now supports GPT-5 Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_gpt_4o_mini_2_pro_integrates_with",
          "source": "repo:localai",
          "target": "model:gpt_4o_mini_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-18_arxiv_2883",
              "url": "https://arxiv.org/abs/2026/01/18/localai_gpt_4o_mini_2_pro",
              "published": "2026-01-18",
              "snippet": "LocalAI announced official support for GPT-4o Mini 2 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_plus_copilot_max_integrates_with",
          "source": "tool:cursor_plus",
          "target": "tool:copilot_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_speculative_decoding_lite_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_ollama_pro_integrates_with",
          "source": "tool:litellm_mini",
          "target": "tool:ollama_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_rotary_position_embedding_core_uses_tech",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "tech:rotary_position_embedding_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_1175",
              "url": "https://theverge.com/2026/01/25/textbooks_are_all_you_need_max",
              "published": "2026-01-25",
              "snippet": "Under the hood, Textbooks Are All You Need Max implements Rotary Position Embedding Core for improved efficiency..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_3758",
              "url": "https://ai.meta.com/blog/2026/01/25/textbooks_are_all_you_need_max",
              "published": "2026-01-25",
              "snippet": "Under the hood, Textbooks Are All You Need Max implements Rotary Position Embedding Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_claude_opus_45_plus_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:claude_opus_45_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-19_mit_technology__9665",
              "url": "https://technologyreview.com/2026/01/19/llm_agents:_a_survey_next_clau",
              "published": "2026-01-19",
              "snippet": "Evaluation results show LLM Agents: A Survey Next reaching 79% on Claude Opus 4.5 Plus..."
            },
            {
              "docId": "2026-01-19_venturebeat_7757",
              "url": "https://venturebeat.com/2026/01/19/llm_agents:_a_survey_next_clau",
              "published": "2026-01-19",
              "snippet": "LLM Agents: A Survey Next achieves 87% on Claude Opus 4.5 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_v2_mixture_of_experts_plus_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:dolma_core_transformer_architecture_plus_uses_tech",
          "source": "dataset:dolma_core",
          "target": "tech:transformer_architecture_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:cody_pro_distillation_edge_uses_tech",
          "source": "tool:cody_pro",
          "target": "tech:distillation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:localai_lite_gemini_ultra_2_core_integrates_with",
          "source": "repo:localai_lite",
          "target": "model:gemini_ultra_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-19_venturebeat_7117",
              "url": "https://venturebeat.com/2026/01/19/localai_lite_gemini_ultra_2_co",
              "published": "2026-01-19",
              "snippet": "LocalAI Lite now supports Gemini Ultra 2 Core with full feature parity..."
            },
            {
              "docId": "2026-01-25_mit_technology__3382",
              "url": "https://technologyreview.com/2026/01/25/localai_lite_gemini_ultra_2_co",
              "published": "2026-01-25",
              "snippet": "LocalAI Lite now supports Gemini Ultra 2 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_plus_claude_opus_45_edge_integrates_with",
          "source": "tool:cody_plus",
          "target": "model:claude_opus_45_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:autogpt_multimodal_fusion_ultra_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:multimodal_fusion_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-12_arxiv_9334",
              "url": "https://arxiv.org/abs/2026/01/12/autogpt_multimodal_fusion_ultr",
              "published": "2026-01-12",
              "snippet": "Technical details reveal AutoGPT relies heavily on Multimodal Fusion Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_whisper_v4_core_depends_on",
          "source": "model:jamba_2_core",
          "target": "model:whisper_v4_core",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:qwen_3_max_tree_of_thought_v2_uses_tech",
          "source": "model:qwen_3_max",
          "target": "tech:tree_of_thought_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:gradio_v2_cursor_v2_integrates_with",
          "source": "tool:gradio_v2",
          "target": "tool:cursor_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-21_microsoft_resea_6504",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/gradio_v2_cursor_v2",
              "published": "2026-01-21",
              "snippet": "Gradio v2 now supports Cursor v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_lite_claude_opus_45_lite_integrates_with",
          "source": "repo:localai_lite",
          "target": "model:claude_opus_45_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-15_anthropic_blog_9796",
              "url": "https://anthropic.com/news/2026/01/15/localai_lite_claude_opus_45_li",
              "published": "2026-01-15",
              "snippet": "LocalAI Lite announced official support for Claude Opus 4.5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_core_gpt_5_pro_depends_on",
          "source": "model:palm_3_core",
          "target": "model:gpt_5_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_flash_attention_ultra_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:flash_attention_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_7545",
              "url": "https://blog.google/technology/ai/2026/01/22/langchain_ultra_flash_attentio",
              "published": "2026-01-22",
              "snippet": "Under the hood, langchain Ultra implements Flash Attention Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-22_ars_technica_2764",
              "url": "https://arstechnica.com/2026/01/22/langchain_ultra_flash_attentio",
              "published": "2026-01-22",
              "snippet": "Technical details reveal langchain Ultra relies heavily on Flash Attention Ultra..."
            },
            {
              "docId": "2026-01-25_the_gradient_3452",
              "url": "https://thegradient.pub/2026/01/25/langchain_ultra_flash_attentio",
              "published": "2026-01-25",
              "snippet": "Under the hood, langchain Ultra implements Flash Attention Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_the_verge_3430",
              "url": "https://theverge.com/2026/01/25/langchain_ultra_flash_attentio",
              "published": "2026-01-25",
              "snippet": "Technical details reveal langchain Ultra relies heavily on Flash Attention Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_the_pile_plus_trained_on",
          "source": "model:claude_opus_45_lite",
          "target": "dataset:the_pile_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-14_the_verge_5665",
              "url": "https://theverge.com/2025/12/14/claude_opus_45_lite_the_pile_p",
              "published": "2025-12-14",
              "snippet": "Claude Opus 4.5 Lite utilized The Pile Plus as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-22_google_ai_blog_7520",
              "url": "https://blog.google/technology/ai/2026/01/22/claude_opus_45_lite_the_pile_p",
              "published": "2026-01-22",
              "snippet": "Claude Opus 4.5 Lite was trained on The Pile Plus comprising billions of tokens..."
            },
            {
              "docId": "2026-01-23_the_verge_6028",
              "url": "https://theverge.com/2026/01/23/claude_opus_45_lite_the_pile_p",
              "published": "2026-01-23",
              "snippet": "Claude Opus 4.5 Lite was trained on The Pile Plus comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_qlora_v2_uses_tech",
          "source": "dataset:mc4",
          "target": "tech:qlora_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_falcon_3_next_depends_on",
          "source": "model:dall_e_4_core",
          "target": "model:falcon_3_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_edge_gpt_5_pro_measures",
          "source": "benchmark:truthfulqa_edge",
          "target": "model:gpt_5_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-07_the_verge_7769",
              "url": "https://theverge.com/2026/01/07/truthfulqa_edge_gpt_5_pro",
              "published": "2026-01-07",
              "snippet": "The TruthfulQA Edge benchmark measures GPT-5 Pro across multiple tasks..."
            },
            {
              "docId": "2026-01-08_venturebeat_4841",
              "url": "https://venturebeat.com/2026/01/08/truthfulqa_edge_gpt_5_pro",
              "published": "2026-01-08",
              "snippet": "TruthfulQA Edge provides standardized evaluation of GPT-5 Pro..."
            },
            {
              "docId": "2026-01-18_venturebeat_6002",
              "url": "https://venturebeat.com/2026/01/18/truthfulqa_edge_gpt_5_pro",
              "published": "2026-01-18",
              "snippet": "The TruthfulQA Edge benchmark measures GPT-5 Pro across multiple tasks..."
            },
            {
              "docId": "2026-01-21_the_verge_5426",
              "url": "https://theverge.com/2026/01/21/truthfulqa_edge_gpt_5_pro",
              "published": "2026-01-21",
              "snippet": "TruthfulQA Edge provides standardized evaluation of GPT-5 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_stable_diffusion_4_depends_on",
          "source": "model:whisper_v4_core",
          "target": "model:stable_diffusion_4",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_llama_4_plus_integrates_with",
          "source": "repo:gpt4all_v2",
          "target": "model:llama_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_6029",
              "url": "https://theverge.com/2026/01/25/gpt4all_v2_llama_4_plus",
              "published": "2026-01-25",
              "snippet": "gpt4all v2 now supports Llama 4 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_lite_dall_e_4_max_depends_on",
          "source": "model:whisper_v4_lite",
          "target": "model:dall_e_4_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_lite_the_pile_max_trained_on",
          "source": "model:nemotron_5_lite",
          "target": "dataset:the_pile_max",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_transformer_architecture_lite_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_aya_3_max_evaluated_on",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "model:aya_3_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2025-12-29_google_ai_blog_1861",
              "url": "https://blog.google/technology/ai/2025/12/29/llm_agents:_a_survey_core_aya_",
              "published": "2025-12-29",
              "snippet": "Evaluation results show LLM Agents: A Survey Core reaching 95% on Aya 3 Max..."
            },
            {
              "docId": "2026-01-11_wired_8378",
              "url": "https://wired.com/2026/01/11/llm_agents:_a_survey_core_aya_",
              "published": "2026-01-11",
              "snippet": "LLM Agents: A Survey Core achieves 92% on Aya 3 Max, setting a new record..."
            },
            {
              "docId": "2026-01-15_nvidia_blog_2336",
              "url": "https://blogs.nvidia.com/2026/01/15/llm_agents:_a_survey_core_aya_",
              "published": "2026-01-15",
              "snippet": "LLM Agents: A Survey Core achieves 81% on Aya 3 Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_gemma_3_max_depends_on",
          "source": "model:sora_2_plus",
          "target": "model:gemma_3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gradio_edge_palm_3_next_integrates_with",
          "source": "tool:gradio_edge",
          "target": "model:palm_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-12-17_the_gradient_9336",
              "url": "https://thegradient.pub/2025/12/17/gradio_edge_palm_3_next",
              "published": "2025-12-17",
              "snippet": "The latest release of Gradio Edge adds native PaLM 3 Next integration..."
            },
            {
              "docId": "2026-01-22_google_ai_blog_3278",
              "url": "https://blog.google/technology/ai/2026/01/22/gradio_edge_palm_3_next",
              "published": "2026-01-22",
              "snippet": "The latest release of Gradio Edge adds native PaLM 3 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_arc_agi_evaluated_on",
          "source": "model:midjourney_v7_core",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-19_microsoft_resea_1996",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/19/midjourney_v7_core_arc_agi",
              "published": "2026-01-19",
              "snippet": "Midjourney V7 Core achieves 94% on ARC-AGI, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_speculative_decoding_pro_uses_tech",
          "source": "model:midjourney_v7",
          "target": "tech:speculative_decoding_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_codex_2_mini_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:codex_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_v2_tool_use_plus_uses_tech",
          "source": "model:claude_sonnet_4_v2",
          "target": "tech:tool_use_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-24_langchain_blog_5366",
              "url": "https://blog.langchain.dev/2026/01/24/claude_sonnet_4_v2_tool_use_pl",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Claude Sonnet 4 v2 relies heavily on Tool Use Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_mixtral_8x22b_ultra_integrates_with",
          "source": "tool:cursor_v2",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_lite_grok_3_core_depends_on",
          "source": "model:gemini_ultra_2_lite",
          "target": "model:grok_3_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_max_synthetic_data_generation_edge_uses_tech",
          "source": "dataset:redpajama_v2_max",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_distillation_mini_uses_tech",
          "source": "tool:litellm_mini",
          "target": "tech:distillation_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_max_synthetic_data_generation_edge_uses_tech",
          "source": "model:mixtral_8x22b_max",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:vllm_max_llama_4_pro_integrates_with",
          "source": "repo:vllm_max",
          "target": "model:llama_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-10_wired_1032",
              "url": "https://wired.com/2025/12/10/vllm_max_llama_4_pro",
              "published": "2025-12-10",
              "snippet": "vllm Max announced official support for Llama 4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_yi_large_max_depends_on",
          "source": "model:gemma_3_plus",
          "target": "model:yi_large_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_aya_3_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:aya_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-29_reuters_4784",
              "url": "https://reuters.com/technology/2025/12/29/flash_attention:_fast_and_memo",
              "published": "2025-12-29",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention achieves 90% on Aya 3, setting a new record..."
            },
            {
              "docId": "2026-01-02_arxiv_1101",
              "url": "https://arxiv.org/abs/2026/01/02/flash_attention:_fast_and_memo",
              "published": "2026-01-02",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 97% on Aya 3..."
            },
            {
              "docId": "2026-01-25_mit_technology__3438",
              "url": "https://technologyreview.com/2026/01/25/flash_attention:_fast_and_memo",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 75% on Aya 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_constitutional_ai_max_uses_tech",
          "source": "tool:vllm_core",
          "target": "tech:constitutional_ai_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:codex_2_next_deepseek_v3_v2_depends_on",
          "source": "model:codex_2_next",
          "target": "model:deepseek_v3_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_flash_attention_mini_uses_tech",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_wired_8834",
              "url": "https://wired.com/2026/01/25/llm_agents:_a_survey_next_flas",
              "published": "2026-01-25",
              "snippet": "LLM Agents: A Survey Next leverages Flash Attention Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_qlora_pro_uses_tech",
          "source": "repo:open_interpreter_core",
          "target": "tech:qlora_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-08_microsoft_resea_1959",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/08/open_interpreter_core_qlora_pr",
              "published": "2026-01-08",
              "snippet": "Under the hood, open-interpreter Core implements QLoRA Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_flash_attention_ultra_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:flash_attention_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-09_nvidia_blog_3990",
              "url": "https://blogs.nvidia.com/2026/01/09/transformers_next_flash_attent",
              "published": "2026-01-09",
              "snippet": "Technical details reveal transformers Next relies heavily on Flash Attention Ultra..."
            },
            {
              "docId": "2026-01-24_arxiv_5017",
              "url": "https://arxiv.org/abs/2026/01/24/transformers_next_flash_attent",
              "published": "2026-01-24",
              "snippet": "Under the hood, transformers Next implements Flash Attention Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_nemotron_5_next_depends_on",
          "source": "model:yi_large_next",
          "target": "model:nemotron_5_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_gpqa_evaluated_on",
          "source": "model:palm_3_lite",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__9375",
              "url": "https://technologyreview.com/2026/01/24/palm_3_lite_gpqa",
              "published": "2026-01-24",
              "snippet": "PaLM 3 Lite achieves 89% on GPQA, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_transformer_architecture_lite_uses_tech",
          "source": "repo:transformers",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_claude_opus_45_max_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:claude_opus_45_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-29_the_gradient_6392",
              "url": "https://thegradient.pub/2025/12/29/self_play_fine_tuning_for_lang",
              "published": "2025-12-29",
              "snippet": "Self-Play Fine-Tuning for Language Models achieves 86% on Claude Opus 4.5 Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_quantization_mini_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:quantization_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-24_nvidia_blog_7021",
              "url": "https://blogs.nvidia.com/2026/01/24/attention_is_all_you_need_v2_q",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Attention Is All You Need v2 relies heavily on Quantization Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_multimodal_fusion_max_uses_tech",
          "source": "model:mixtral_8x22b_mini",
          "target": "tech:multimodal_fusion_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_pro_codex_2_mini_integrates_with",
          "source": "tool:semantic_kernel_pro",
          "target": "model:codex_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_distillation_uses_tech",
          "source": "dataset:starcoder_data",
          "target": "tech:distillation",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:localai_core_qlora_ultra_uses_tech",
          "source": "tool:localai_core",
          "target": "tech:qlora_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_tree_of_thought_mini_uses_tech",
          "source": "repo:langchain_lite",
          "target": "tech:tree_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:langchain_v2_weights_and_biases_core_integrates_with",
          "source": "tool:langchain_v2",
          "target": "tool:weights_and_biases_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-09_mit_technology__8514",
              "url": "https://technologyreview.com/2026/01/09/langchain_v2_weights_and_biase",
              "published": "2026-01-09",
              "snippet": "The latest release of LangChain v2 adds native Weights & Biases Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_tool_use_max_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-05_anthropic_blog_1852",
              "url": "https://anthropic.com/news/2026/01/05/direct_preference_optimization",
              "published": "2026-01-05",
              "snippet": "Under the hood, Direct Preference Optimization Next implements Tool Use Max for improved efficiency..."
            },
            {
              "docId": "2026-01-09_google_ai_blog_1427",
              "url": "https://blog.google/technology/ai/2026/01/09/direct_preference_optimization",
              "published": "2026-01-09",
              "snippet": "Direct Preference Optimization Next leverages Tool Use Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_rotary_position_embedding_v2_uses_tech",
          "source": "model:gemma_3_plus",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:swe_bench_pro_phi_4_measures",
          "source": "benchmark:swe_bench_pro",
          "target": "model:phi_4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-04_ars_technica_4298",
              "url": "https://arstechnica.com/2026/01/04/swe_bench_pro_phi_4",
              "published": "2026-01-04",
              "snippet": "The SWE-bench Pro benchmark measures Phi-4 across multiple tasks..."
            },
            {
              "docId": "2026-01-20_the_gradient_1203",
              "url": "https://thegradient.pub/2026/01/20/swe_bench_pro_phi_4",
              "published": "2026-01-20",
              "snippet": "SWE-bench Pro has become the standard for evaluating Phi-4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_next_qwen_3_plus_depends_on",
          "source": "model:midjourney_v7_next",
          "target": "model:qwen_3_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:phi_4_plus_slimpajama_core_trained_on",
          "source": "model:phi_4_plus",
          "target": "dataset:slimpajama_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_max_gguf_core_uses_tech",
          "source": "tool:semantic_kernel_max",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_lite_claude_sonnet_4_next_integrates_with",
          "source": "tool:weights_and_biases_lite",
          "target": "model:claude_sonnet_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:falcon_3_lite_claude_sonnet_4_max_depends_on",
          "source": "model:falcon_3_lite",
          "target": "model:claude_sonnet_4_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:vllm_next_mlflow_mini_integrates_with",
          "source": "tool:vllm_next",
          "target": "tool:mlflow_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:codex_2_edge_retrieval_augmented_generation_lite_uses_tech",
          "source": "model:codex_2_edge",
          "target": "tech:retrieval_augmented_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-23_the_verge_3131",
              "url": "https://theverge.com/2026/01/23/codex_2_edge_retrieval_augment",
              "published": "2026-01-23",
              "snippet": "Under the hood, Codex 2 Edge implements Retrieval-Augmented Generation Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_constitutional_ai_pro_uses_tech",
          "source": "repo:text_generation_webui_max",
          "target": "tech:constitutional_ai_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__synthetic_data_generation_mini_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-23_nvidia_blog_9633",
              "url": "https://blogs.nvidia.com/2026/01/23/scaling_laws_for_neural_langua",
              "published": "2026-01-23",
              "snippet": "Scaling Laws for Neural Language Models (2025) leverages Synthetic Data Generation Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_anthropic_blog_5526",
              "url": "https://anthropic.com/news/2026/01/24/scaling_laws_for_neural_langua",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Scaling Laws for Neural Language Models (2025) relies heavily on Synthetic Data Generation Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_qlora_core_uses_tech",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "tech:qlora_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_retrieval_augmented_generation_lite_uses_tech",
          "source": "repo:text_generation_webui_max",
          "target": "tech:retrieval_augmented_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_8768",
              "url": "https://nextgov.com/2026/01/25/text_generation_webui_max_retr",
              "published": "2026-01-25",
              "snippet": "text-generation-webui Max leverages Retrieval-Augmented Generation Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_langchain_blog_3788",
              "url": "https://blog.langchain.dev/2026/01/25/text_generation_webui_max_retr",
              "published": "2026-01-25",
              "snippet": "Under the hood, text-generation-webui Max implements Retrieval-Augmented Generation Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_v2_bigbench_hard_max_evaluated_on",
          "source": "model:gpt_4o_mini_2_v2",
          "target": "benchmark:bigbench_hard_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-22_anthropic_blog_8350",
              "url": "https://anthropic.com/news/2026/01/22/gpt_4o_mini_2_v2_bigbench_hard",
              "published": "2026-01-22",
              "snippet": "On the BigBench Hard Max benchmark, GPT-4o Mini 2 v2 scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_rlhf_ultra_uses_tech",
          "source": "tool:mlflow_v2",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-08_venturebeat_4877",
              "url": "https://venturebeat.com/2026/01/08/mlflow_v2_rlhf_ultra",
              "published": "2026-01-08",
              "snippet": "Under the hood, MLflow v2 implements RLHF Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_wikipedia_dump_2025_core_trained_on",
          "source": "model:sora_2_edge",
          "target": "dataset:wikipedia_dump_2025_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_qwen_3_v2_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:qwen_3_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-25_google_ai_blog_5108",
              "url": "https://blog.google/technology/ai/2025/12/25/textbooks_are_all_you_need_min",
              "published": "2025-12-25",
              "snippet": "On the Qwen-3 v2 benchmark, Textbooks Are All You Need Mini scored 98%..."
            },
            {
              "docId": "2026-01-22_hugging_face_bl_9439",
              "url": "https://huggingface.co/blog/2026/01/22/textbooks_are_all_you_need_min",
              "published": "2026-01-22",
              "snippet": "On the Qwen-3 v2 benchmark, Textbooks Are All You Need Mini scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_distillation_uses_tech",
          "source": "tool:haystack",
          "target": "tech:distillation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:vllm_synthetic_data_generation_core_uses_tech",
          "source": "repo:vllm",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-22_weights_and_bia_4779",
              "url": "https://wandb.ai/articles/2026/01/22/vllm_synthetic_data_generation",
              "published": "2026-01-22",
              "snippet": "Under the hood, vllm implements Synthetic Data Generation Core for improved efficiency..."
            },
            {
              "docId": "2026-01-24_ars_technica_6627",
              "url": "https://arstechnica.com/2026/01/24/vllm_synthetic_data_generation",
              "published": "2026-01-24",
              "snippet": "vllm leverages Synthetic Data Generation Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_rotary_position_embedding_ultra_uses_tech",
          "source": "tool:cursor_v2",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:codex_2_starcoder_data_mini_trained_on",
          "source": "model:codex_2",
          "target": "dataset:starcoder_data_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:mc4_constitutional_ai_plus_uses_tech",
          "source": "dataset:mc4",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_llama_4_core_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:llama_4_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-21_wired_8499",
              "url": "https://wired.com/2025/12/21/tree_of_thoughts:_deliberate_p",
              "published": "2025-12-21",
              "snippet": "On the Llama 4 Core benchmark, Tree of Thoughts: Deliberate Problem Solving with LLMs scored 89%..."
            },
            {
              "docId": "2025-12-21_google_ai_blog_9301",
              "url": "https://blog.google/technology/ai/2025/12/21/tree_of_thoughts:_deliberate_p",
              "published": "2025-12-21",
              "snippet": "Tree of Thoughts: Deliberate Problem Solving with LLMs achieves 97% on Llama 4 Core, setting a new record..."
            },
            {
              "docId": "2026-01-10_meta_ai_blog_3880",
              "url": "https://ai.meta.com/blog/2026/01/10/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-10",
              "snippet": "Evaluation results show Tree of Thoughts: Deliberate Problem Solving with LLMs reaching 80% on Llama 4 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_gpt_4o_mini_2_evaluated_on",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "model:gpt_4o_mini_2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_kv_cache_optimization_v2_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:kv_cache_optimization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:haystack_multimodal_fusion_pro_uses_tech",
          "source": "tool:haystack",
          "target": "tech:multimodal_fusion_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-07_venturebeat_1031",
              "url": "https://venturebeat.com/2026/01/07/haystack_multimodal_fusion_pro",
              "published": "2026-01-07",
              "snippet": "Technical details reveal Haystack relies heavily on Multimodal Fusion Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_laion_5b_v2_trained_on",
          "source": "model:gpt_5_v2",
          "target": "dataset:laion_5b_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_tree_of_thought_plus_uses_tech",
          "source": "tool:tensorrt_llm",
          "target": "tech:tree_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-06_nvidia_blog_4901",
              "url": "https://blogs.nvidia.com/2026/01/06/tensorrt_llm_tree_of_thought_p",
              "published": "2026-01-06",
              "snippet": "TensorRT-LLM leverages Tree of Thought Plus to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-16_anthropic_blog_4613",
              "url": "https://anthropic.com/news/2026/01/16/tensorrt_llm_tree_of_thought_p",
              "published": "2026-01-16",
              "snippet": "Under the hood, TensorRT-LLM implements Tree of Thought Plus for improved efficiency..."
            },
            {
              "docId": "2026-01-19_mit_technology__5851",
              "url": "https://technologyreview.com/2026/01/19/tensorrt_llm_tree_of_thought_p",
              "published": "2026-01-19",
              "snippet": "Technical details reveal TensorRT-LLM relies heavily on Tree of Thought Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_gemini_ultra_2_plus_depends_on",
          "source": "model:qwen_3_next",
          "target": "model:gemini_ultra_2_plus",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_gguf_edge_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_6218",
              "url": "https://blogs.nvidia.com/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Mini leverages GGUF Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_core_tree_of_thought_uses_tech",
          "source": "dataset:starcoder_data_core",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_next_deepseek_v3_depends_on",
          "source": "model:mixtral_8x22b_next",
          "target": "model:deepseek_v3",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:arc_agi_core_sora_2_plus_measures",
          "source": "benchmark:arc_agi_core",
          "target": "model:sora_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_6162",
              "url": "https://arstechnica.com/2026/01/25/arc_agi_core_sora_2_plus",
              "published": "2026-01-25",
              "snippet": "ARC-AGI Core has become the standard for evaluating Sora 2 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_weights_and_biases_lite_integrates_with",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tool:weights_and_biases_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2025-12-22_arxiv_9438",
              "url": "https://arxiv.org/abs/2025/12/22/tensorrt_llm_ultra_weights_and",
              "published": "2025-12-22",
              "snippet": "TensorRT-LLM Ultra now supports Weights & Biases Lite with full feature parity..."
            },
            {
              "docId": "2026-01-22_ars_technica_2307",
              "url": "https://arstechnica.com/2026/01/22/tensorrt_llm_ultra_weights_and",
              "published": "2026-01-22",
              "snippet": "The latest release of TensorRT-LLM Ultra adds native Weights & Biases Lite integration..."
            },
            {
              "docId": "2026-01-23_reuters_3912",
              "url": "https://reuters.com/technology/2026/01/23/tensorrt_llm_ultra_weights_and",
              "published": "2026-01-23",
              "snippet": "TensorRT-LLM Ultra now supports Weights & Biases Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_aya_3_max_integrates_with",
          "source": "repo:localai_plus",
          "target": "model:aya_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-29_langchain_blog_6419",
              "url": "https://blog.langchain.dev/2025/12/29/localai_plus_aya_3_max",
              "published": "2025-12-29",
              "snippet": "The latest release of LocalAI Plus adds native Aya 3 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_multimodal_fusion_ultra_uses_tech",
          "source": "paper:direct_preference_optimization_pro",
          "target": "tech:multimodal_fusion_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-25_bloomberg_9219",
              "url": "https://bloomberg.com/technology/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Direct Preference Optimization Pro leverages Multimodal Fusion Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_1345",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Under the hood, Direct Preference Optimization Pro implements Multimodal Fusion Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_speculative_decoding_uses_tech",
          "source": "model:deepseek_v3_mini",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:ollama_transformer_architecture_ultra_uses_tech",
          "source": "repo:ollama",
          "target": "tech:transformer_architecture_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:yi_large_lite_dolma_lite_trained_on",
          "source": "model:yi_large_lite",
          "target": "dataset:dolma_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_qwen_3_core_integrates_with",
          "source": "repo:autogpt_plus",
          "target": "model:qwen_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-12_reuters_2448",
              "url": "https://reuters.com/technology/2026/01/12/autogpt_plus_qwen_3_core",
              "published": "2026-01-12",
              "snippet": "AutoGPT Plus announced official support for Qwen-3 Core..."
            },
            {
              "docId": "2026-01-14_openai_blog_8945",
              "url": "https://openai.com/blog/2026/01/14/autogpt_plus_qwen_3_core",
              "published": "2026-01-14",
              "snippet": "The latest release of AutoGPT Plus adds native Qwen-3 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_stable_diffusion_4_max_integrates_with",
          "source": "tool:mlflow_mini",
          "target": "model:stable_diffusion_4_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-13_google_ai_blog_5720",
              "url": "https://blog.google/technology/ai/2026/01/13/mlflow_mini_stable_diffusion_4",
              "published": "2026-01-13",
              "snippet": "MLflow Mini announced official support for Stable Diffusion 4 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_constitutional_ai_mini_uses_tech",
          "source": "paper:llm_agents:_a_survey",
          "target": "tech:constitutional_ai_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-12-31_the_gradient_6874",
              "url": "https://thegradient.pub/2025/12/31/llm_agents:_a_survey_constitut",
              "published": "2025-12-31",
              "snippet": "Under the hood, LLM Agents: A Survey implements Constitutional AI Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_core_mixture_of_experts_plus_uses_tech",
          "source": "tool:copilot_core",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_qlora_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:qlora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-24_hugging_face_bl_3131",
              "url": "https://huggingface.co/blog/2026/01/24/chain_of_thought_prompting_eli",
              "published": "2026-01-24",
              "snippet": "Under the hood, Chain-of-Thought Prompting Elicits Reasoning implements QLoRA for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_core_common_crawl_mini_trained_on",
          "source": "model:sora_2_core",
          "target": "dataset:common_crawl_mini",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.55,
          "evidence": [
            {
              "docId": "2025-12-21_ars_technica_3535",
              "url": "https://arstechnica.com/2025/12/21/sora_2_core_common_crawl_mini",
              "published": "2025-12-21",
              "snippet": "Sora 2 Core was trained on Common Crawl Mini comprising billions of tokens..."
            },
            {
              "docId": "2026-01-03_hugging_face_bl_5209",
              "url": "https://huggingface.co/blog/2026/01/03/sora_2_core_common_crawl_mini",
              "published": "2026-01-03",
              "snippet": "Sora 2 Core was trained on Common Crawl Mini comprising billions of tokens..."
            },
            {
              "docId": "2026-01-14_wired_7804",
              "url": "https://wired.com/2026/01/14/sora_2_core_common_crawl_mini",
              "published": "2026-01-14",
              "snippet": "Sora 2 Core was trained on Common Crawl Mini comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_tool_use_core_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_max_slimpajama_mini_trained_on",
          "source": "model:gpt_4o_mini_2_max",
          "target": "dataset:slimpajama_mini",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:vllm_max_sora_2_pro_integrates_with",
          "source": "repo:vllm_max",
          "target": "model:sora_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:vllm_max_falcon_3_ultra_integrates_with",
          "source": "tool:vllm_max",
          "target": "model:falcon_3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-02_bloomberg_7018",
              "url": "https://bloomberg.com/technology/2026/01/02/vllm_max_falcon_3_ultra",
              "published": "2026-01-02",
              "snippet": "The latest release of vLLM Max adds native Falcon 3 Ultra integration..."
            },
            {
              "docId": "2026-01-16_the_verge_1293",
              "url": "https://theverge.com/2026/01/16/vllm_max_falcon_3_ultra",
              "published": "2026-01-16",
              "snippet": "The latest release of vLLM Max adds native Falcon 3 Ultra integration..."
            },
            {
              "docId": "2026-01-18_microsoft_resea_5056",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/vllm_max_falcon_3_ultra",
              "published": "2026-01-18",
              "snippet": "The latest release of vLLM Max adds native Falcon 3 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_lite_fineweb_max_trained_on",
          "source": "model:midjourney_v7_lite",
          "target": "dataset:fineweb_max",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2025-12-31_arxiv_8773",
              "url": "https://arxiv.org/abs/2025/12/31/midjourney_v7_lite_fineweb_max",
              "published": "2025-12-31",
              "snippet": "Midjourney V7 Lite utilized FineWeb Max as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_mini_streamlit_max_integrates_with",
          "source": "tool:dify_mini",
          "target": "tool:streamlit_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__quantization_edge_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:quantization_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_distillation_pro_uses_tech",
          "source": "repo:langchain_lite",
          "target": "tech:distillation_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-23_weights_and_bia_4197",
              "url": "https://wandb.ai/articles/2026/01/23/langchain_lite_distillation_pr",
              "published": "2026-01-23",
              "snippet": "Technical details reveal langchain Lite relies heavily on Distillation Pro..."
            },
            {
              "docId": "2026-01-23_wired_3945",
              "url": "https://wired.com/2026/01/23/langchain_lite_distillation_pr",
              "published": "2026-01-23",
              "snippet": "Technical details reveal langchain Lite relies heavily on Distillation Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_aya_3_next_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:aya_3_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_arc_agi_ultra_evaluated_on",
          "source": "model:sora_2_edge",
          "target": "benchmark:arc_agi_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-15_microsoft_resea_1930",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/15/sora_2_edge_arc_agi_ultra",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Sora 2 Edge reaching 92% on ARC-AGI Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_claude_opus_45_core_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:claude_opus_45_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__constitutional_ai_next_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_core_gemma_3_mini_measures",
          "source": "benchmark:lmsys_chatbot_arena_core",
          "target": "model:gemma_3_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_6307",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/lmsys_chatbot_arena_core_gemma",
              "published": "2026-01-25",
              "snippet": "LMSYS Chatbot Arena Core provides standardized evaluation of Gemma 3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_kv_cache_optimization_next_uses_tech",
          "source": "tool:cursor_v2",
          "target": "tech:kv_cache_optimization_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:common_crawl_synthetic_data_generation_max_uses_tech",
          "source": "dataset:common_crawl",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_midjourney_v7_ultra_depends_on",
          "source": "model:mixtral_8x22b_plus",
          "target": "model:midjourney_v7_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_llamaindex_integrates_with",
          "source": "tool:flowise_plus",
          "target": "tool:llamaindex",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-19_venturebeat_5622",
              "url": "https://venturebeat.com/2026/01/19/flowise_plus_llamaindex",
              "published": "2026-01-19",
              "snippet": "Flowise Plus announced official support for LlamaIndex..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_pro_jamba_2_v2_measures",
          "source": "benchmark:gpqa_pro",
          "target": "model:jamba_2_v2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_constitutional_ai_edge_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:dolma_pro_rlhf_next_uses_tech",
          "source": "dataset:dolma_pro",
          "target": "tech:rlhf_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:phi_4_gguf_max_uses_tech",
          "source": "model:phi_4",
          "target": "tech:gguf_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_gemma_3_pro_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:gemma_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_rotary_position_embedding_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-18_reuters_4706",
              "url": "https://reuters.com/technology/2026/01/18/mixture_of_experts_meets_instr",
              "published": "2026-01-18",
              "snippet": "Mixture of Experts Meets Instruction Tuning leverages Rotary Position Embedding to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-18_hugging_face_bl_1287",
              "url": "https://huggingface.co/blog/2026/01/18/mixture_of_experts_meets_instr",
              "published": "2026-01-18",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements Rotary Position Embedding for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_rlhf_mini_uses_tech",
          "source": "model:command_r_plus",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2025-12-26_techcrunch_8460",
              "url": "https://techcrunch.com/2025/12/26/command_r_plus_rlhf_mini",
              "published": "2025-12-26",
              "snippet": "Under the hood, Command R+ implements RLHF Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-02_meta_ai_blog_7607",
              "url": "https://ai.meta.com/blog/2026/01/02/command_r_plus_rlhf_mini",
              "published": "2026-01-02",
              "snippet": "Under the hood, Command R+ implements RLHF Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_core_langchain_plus_integrates_with",
          "source": "tool:dify_core",
          "target": "tool:langchain_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_copilot_max_integrates_with",
          "source": "tool:ollama_lite",
          "target": "tool:copilot_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_3020",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/ollama_lite_copilot_max",
              "published": "2026-01-18",
              "snippet": "Ollama Lite now supports Copilot Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_aya_3_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:aya_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-20_mit_technology__6202",
              "url": "https://technologyreview.com/2026/01/20/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-20",
              "snippet": "On the Aya 3 benchmark, Tree of Thoughts: Deliberate Problem Solving with LLMs scored 93%..."
            },
            {
              "docId": "2026-01-21_venturebeat_9700",
              "url": "https://venturebeat.com/2026/01/21/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Tree of Thoughts: Deliberate Problem Solving with LLMs reaching 80% on Aya 3..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_3173",
              "url": "https://huggingface.co/blog/2026/01/24/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-24",
              "snippet": "Tree of Thoughts: Deliberate Problem Solving with LLMs achieves 75% on Aya 3, setting a new record..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_1620",
              "url": "https://blog.google/technology/ai/2026/01/24/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-24",
              "snippet": "Tree of Thoughts: Deliberate Problem Solving with LLMs achieves 94% on Aya 3, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_dolma_core_trained_on",
          "source": "model:grok_3_next",
          "target": "dataset:dolma_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_mmlu_next_evaluated_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "benchmark:mmlu_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-14_mit_technology__5847",
              "url": "https://technologyreview.com/2026/01/14/stable_diffusion_4_plus_mmlu_n",
              "published": "2026-01-14",
              "snippet": "Evaluation results show Stable Diffusion 4 Plus reaching 94% on MMLU Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_langchain_next_integrates_with",
          "source": "tool:langchain_edge",
          "target": "tool:langchain_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-05_weights_and_bia_3753",
              "url": "https://wandb.ai/articles/2026/01/05/langchain_edge_langchain_next",
              "published": "2026-01-05",
              "snippet": "LangChain Edge announced official support for LangChain Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_sliding_window_attention_max_uses_tech",
          "source": "model:claude_opus_45_pro",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_red_teaming_ultra_uses_tech",
          "source": "model:llama_4_pro",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-25_nextgov_1747",
              "url": "https://nextgov.com/2025/12/25/llama_4_pro_red_teaming_ultra",
              "published": "2025-12-25",
              "snippet": "Llama 4 Pro leverages Red Teaming Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-27_mit_technology__3994",
              "url": "https://technologyreview.com/2025/12/27/llama_4_pro_red_teaming_ultra",
              "published": "2025-12-27",
              "snippet": "Llama 4 Pro leverages Red Teaming Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_lite_codex_2_pro_integrates_with",
          "source": "repo:localai_lite",
          "target": "model:codex_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__9251",
              "url": "https://technologyreview.com/2026/01/24/localai_lite_codex_2_pro",
              "published": "2026-01-24",
              "snippet": "LocalAI Lite announced official support for Codex 2 Pro..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_3833",
              "url": "https://blog.google/technology/ai/2026/01/25/localai_lite_codex_2_pro",
              "published": "2026-01-25",
              "snippet": "The latest release of LocalAI Lite adds native Codex 2 Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_v2_rlhf_core_uses_tech",
          "source": "repo:vllm_v2",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_c4_edge_trained_on",
          "source": "model:aya_3_max",
          "target": "dataset:c4_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:palm_3_edge_winogrande_ultra_evaluated_on",
          "source": "model:palm_3_edge",
          "target": "benchmark:winogrande_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-21_anthropic_blog_7012",
              "url": "https://anthropic.com/news/2026/01/21/palm_3_edge_winogrande_ultra",
              "published": "2026-01-21",
              "snippet": "Evaluation results show PaLM 3 Edge reaching 89% on WinoGrande Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_edge_sliding_window_attention_lite_uses_tech",
          "source": "model:gemini_ultra_2_edge",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_transformer_architecture_core_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:transformer_architecture_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llamaindex_core_mixtral_8x22b_pro_integrates_with",
          "source": "tool:llamaindex_core",
          "target": "model:mixtral_8x22b_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_next_speculative_decoding_lite_uses_tech",
          "source": "dataset:openwebtext2_next",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:fineweb_plus_sliding_window_attention_next_uses_tech",
          "source": "dataset:fineweb_plus",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:gpt_5_next_tree_of_thought_uses_tech",
          "source": "model:gpt_5_next",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_next_gpt_5_ultra_measures",
          "source": "benchmark:bigbench_hard_next",
          "target": "model:gpt_5_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-24_openai_blog_7598",
              "url": "https://openai.com/blog/2026/01/24/bigbench_hard_next_gpt_5_ultra",
              "published": "2026-01-24",
              "snippet": "The BigBench Hard Next benchmark measures GPT-5 Ultra across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_red_teaming_lite_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:phi_4_mini_constitutional_ai_plus_uses_tech",
          "source": "model:phi_4_mini",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_edge_grok_3_v2_measures",
          "source": "benchmark:truthfulqa_edge",
          "target": "model:grok_3_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-16_venturebeat_2335",
              "url": "https://venturebeat.com/2026/01/16/truthfulqa_edge_grok_3_v2",
              "published": "2026-01-16",
              "snippet": "The TruthfulQA Edge benchmark measures Grok-3 v2 across multiple tasks..."
            },
            {
              "docId": "2026-01-19_ars_technica_4033",
              "url": "https://arstechnica.com/2026/01/19/truthfulqa_edge_grok_3_v2",
              "published": "2026-01-19",
              "snippet": "The TruthfulQA Edge benchmark measures Grok-3 v2 across multiple tasks..."
            },
            {
              "docId": "2026-01-25_bloomberg_8821",
              "url": "https://bloomberg.com/technology/2026/01/25/truthfulqa_edge_grok_3_v2",
              "published": "2026-01-25",
              "snippet": "The TruthfulQA Edge benchmark measures Grok-3 v2 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_phi_4_max_depends_on",
          "source": "model:whisper_v4",
          "target": "model:phi_4_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_constitutional_ai_plus_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-24_meta_ai_blog_4630",
              "url": "https://ai.meta.com/blog/2026/01/24/llamacpp_mini_constitutional_a",
              "published": "2026-01-24",
              "snippet": "llama.cpp Mini leverages Constitutional AI Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_gpt_5_plus_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:gpt_5_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:math_core_jamba_2_measures",
          "source": "benchmark:math_core",
          "target": "model:jamba_2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_9458",
              "url": "https://blog.google/technology/ai/2026/01/25/math_core_jamba_2",
              "published": "2026-01-25",
              "snippet": "MATH Core provides standardized evaluation of Jamba 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_pro_constitutional_ai_edge_uses_tech",
          "source": "tool:haystack_pro",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2025-12-11_techcrunch_6707",
              "url": "https://techcrunch.com/2025/12/11/haystack_pro_constitutional_ai",
              "published": "2025-12-11",
              "snippet": "Technical details reveal Haystack Pro relies heavily on Constitutional AI Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_phi_4_pro_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:phi_4_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:swe_bench_next_stable_diffusion_4_edge_measures",
          "source": "benchmark:swe_bench_next",
          "target": "model:stable_diffusion_4_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_3837",
              "url": "https://blogs.nvidia.com/2026/01/25/swe_bench_next_stable_diffusio",
              "published": "2026-01-25",
              "snippet": "SWE-bench Next has become the standard for evaluating Stable Diffusion 4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_llama_4_v2_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:llama_4_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:jamba_2_swe_bench_max_evaluated_on",
          "source": "model:jamba_2",
          "target": "benchmark:swe_bench_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_8222",
              "url": "https://theverge.com/2026/01/25/jamba_2_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Jamba 2 achieves 71% on SWE-bench Max, setting a new record..."
            },
            {
              "docId": "2026-01-25_openai_blog_8491",
              "url": "https://openai.com/blog/2026/01/25/jamba_2_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Jamba 2 reaching 75% on SWE-bench Max..."
            },
            {
              "docId": "2026-01-25_wired_3404",
              "url": "https://wired.com/2026/01/25/jamba_2_swe_bench_max",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Jamba 2 reaching 81% on SWE-bench Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_synthetic_data_generation_core_uses_tech",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-21_mit_technology__5478",
              "url": "https://technologyreview.com/2026/01/21/textbooks_are_all_you_need_nex",
              "published": "2026-01-21",
              "snippet": "Under the hood, Textbooks Are All You Need Next implements Synthetic Data Generation Core for improved efficiency..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_1408",
              "url": "https://ai.meta.com/blog/2026/01/24/textbooks_are_all_you_need_nex",
              "published": "2026-01-24",
              "snippet": "Textbooks Are All You Need Next leverages Synthetic Data Generation Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_bloomberg_3211",
              "url": "https://bloomberg.com/technology/2026/01/25/textbooks_are_all_you_need_nex",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Textbooks Are All You Need Next relies heavily on Synthetic Data Generation Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_lite_gguf_pro_uses_tech",
          "source": "tool:streamlit_lite",
          "target": "tech:gguf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-11_hugging_face_bl_9802",
              "url": "https://huggingface.co/blog/2025/12/11/streamlit_lite_gguf_pro",
              "published": "2025-12-11",
              "snippet": "Under the hood, Streamlit Lite implements GGUF Pro for improved efficiency..."
            },
            {
              "docId": "2025-12-17_techcrunch_9303",
              "url": "https://techcrunch.com/2025/12/17/streamlit_lite_gguf_pro",
              "published": "2025-12-17",
              "snippet": "Under the hood, Streamlit Lite implements GGUF Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-08_arxiv_2516",
              "url": "https://arxiv.org/abs/2026/01/08/streamlit_lite_gguf_pro",
              "published": "2026-01-08",
              "snippet": "Technical details reveal Streamlit Lite relies heavily on GGUF Pro..."
            },
            {
              "docId": "2026-01-21_the_verge_4506",
              "url": "https://theverge.com/2026/01/21/streamlit_lite_gguf_pro",
              "published": "2026-01-21",
              "snippet": "Under the hood, Streamlit Lite implements GGUF Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_pro_slimpajama_trained_on",
          "source": "model:gemma_3_pro",
          "target": "dataset:slimpajama",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:gpt_5_ultra_winogrande_pro_evaluated_on",
          "source": "model:gpt_5_ultra",
          "target": "benchmark:winogrande_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-12-11_venturebeat_1953",
              "url": "https://venturebeat.com/2025/12/11/gpt_5_ultra_winogrande_pro",
              "published": "2025-12-11",
              "snippet": "On the WinoGrande Pro benchmark, GPT-5 Ultra scored 89%..."
            },
            {
              "docId": "2026-01-05_the_verge_2317",
              "url": "https://theverge.com/2026/01/05/gpt_5_ultra_winogrande_pro",
              "published": "2026-01-05",
              "snippet": "On the WinoGrande Pro benchmark, GPT-5 Ultra scored 78%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_transformer_architecture_next_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "tech:transformer_architecture_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-01_arxiv_9849",
              "url": "https://arxiv.org/abs/2026/01/01/attention_is_all_you_need_v2_v",
              "published": "2026-01-01",
              "snippet": "Technical details reveal Attention Is All You Need v2 v2 relies heavily on Transformer Architecture Next..."
            },
            {
              "docId": "2026-01-19_reuters_6039",
              "url": "https://reuters.com/technology/2026/01/19/attention_is_all_you_need_v2_v",
              "published": "2026-01-19",
              "snippet": "Technical details reveal Attention Is All You Need v2 v2 relies heavily on Transformer Architecture Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_lite_codex_2_plus_depends_on",
          "source": "model:gpt_4o_mini_2_lite",
          "target": "model:codex_2_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:flowise_max_stable_diffusion_4_ultra_integrates_with",
          "source": "tool:flowise_max",
          "target": "model:stable_diffusion_4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:sora_2_next_alpacaeval_2_v2_evaluated_on",
          "source": "model:sora_2_next",
          "target": "benchmark:alpacaeval_2_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-21_ars_technica_5132",
              "url": "https://arstechnica.com/2026/01/21/sora_2_next_alpacaeval_2_v2",
              "published": "2026-01-21",
              "snippet": "On the AlpacaEval 2 v2 benchmark, Sora 2 Next scored 94%..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_1344",
              "url": "https://anthropic.com/news/2026/01/25/sora_2_next_alpacaeval_2_v2",
              "published": "2026-01-25",
              "snippet": "Sora 2 Next achieves 96% on AlpacaEval 2 v2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_core_qwen_3_max_integrates_with",
          "source": "tool:weights_and_biases_core",
          "target": "model:qwen_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_qwen_3_next_integrates_with",
          "source": "repo:llamacpp_ultra",
          "target": "model:qwen_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-28_bloomberg_9979",
              "url": "https://bloomberg.com/technology/2025/12/28/llamacpp_ultra_qwen_3_next",
              "published": "2025-12-28",
              "snippet": "llama.cpp Ultra announced official support for Qwen-3 Next..."
            },
            {
              "docId": "2026-01-19_nextgov_4392",
              "url": "https://nextgov.com/2026/01/19/llamacpp_ultra_qwen_3_next",
              "published": "2026-01-19",
              "snippet": "The latest release of llama.cpp Ultra adds native Qwen-3 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_pro_retrieval_augmented_generation_uses_tech",
          "source": "tool:langchain_pro",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_2465",
              "url": "https://techcrunch.com/2026/01/25/langchain_pro_retrieval_augmen",
              "published": "2026-01-25",
              "snippet": "LangChain Pro leverages Retrieval-Augmented Generation to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_sliding_window_attention_max_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-15_google_ai_blog_3056",
              "url": "https://blog.google/technology/ai/2026/01/15/retrieval_augmented_generation",
              "published": "2026-01-15",
              "snippet": "Under the hood, Retrieval-Augmented Generation for Knowledge-Intensive NLP implements Sliding Window Attention Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_langchain_next_integrates_with",
          "source": "tool:tensorrt_llm_v2",
          "target": "tool:langchain_next",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:hellaswag_edge_gemini_ultra_2_plus_measures",
          "source": "benchmark:hellaswag_edge",
          "target": "model:gemini_ultra_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_8879",
              "url": "https://anthropic.com/news/2026/01/24/hellaswag_edge_gemini_ultra_2_",
              "published": "2026-01-24",
              "snippet": "HellaSwag Edge provides standardized evaluation of Gemini Ultra 2 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_pro_gemini_ultra_2_pro_depends_on",
          "source": "model:deepseek_v3_pro",
          "target": "model:gemini_ultra_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_pro_rlhf_core_uses_tech",
          "source": "dataset:redpajama_v2_pro",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_stable_diffusion_4_next_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:stable_diffusion_4_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_distillation_mini_uses_tech",
          "source": "model:sora_2_plus",
          "target": "tech:distillation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-21_anthropic_blog_3842",
              "url": "https://anthropic.com/news/2026/01/21/sora_2_plus_distillation_mini",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Sora 2 Plus relies heavily on Distillation Mini..."
            },
            {
              "docId": "2026-01-22_venturebeat_3605",
              "url": "https://venturebeat.com/2026/01/22/sora_2_plus_distillation_mini",
              "published": "2026-01-22",
              "snippet": "Under the hood, Sora 2 Plus implements Distillation Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_yi_large_lite_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:yi_large_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-13_langchain_blog_5179",
              "url": "https://blog.langchain.dev/2026/01/13/textbooks_are_all_you_need_max",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Textbooks Are All You Need Max reaching 77% on Yi-Large Lite..."
            },
            {
              "docId": "2026-01-21_anthropic_blog_3181",
              "url": "https://anthropic.com/news/2026/01/21/textbooks_are_all_you_need_max",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Textbooks Are All You Need Max reaching 99% on Yi-Large Lite..."
            },
            {
              "docId": "2026-01-23_arxiv_4080",
              "url": "https://arxiv.org/abs/2026/01/23/textbooks_are_all_you_need_max",
              "published": "2026-01-23",
              "snippet": "Textbooks Are All You Need Max achieves 86% on Yi-Large Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_deepseek_v3_mini_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:deepseek_v3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-14_ars_technica_9050",
              "url": "https://arstechnica.com/2026/01/14/open_interpreter_pro_deepseek_",
              "published": "2026-01-14",
              "snippet": "open-interpreter Pro announced official support for DeepSeek-V3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_gguf_edge_uses_tech",
          "source": "tool:cursor_mini",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-11-27_weights_and_bia_3324",
              "url": "https://wandb.ai/articles/2025/11/27/cursor_mini_gguf_edge",
              "published": "2025-11-27",
              "snippet": "Technical details reveal Cursor Mini relies heavily on GGUF Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_grok_3_lite_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:grok_3_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-10_wired_7131",
              "url": "https://wired.com/2025/12/10/lora:_low_rank_adaptation_of_l",
              "published": "2025-12-10",
              "snippet": "On the Grok-3 Lite benchmark, LoRA: Low-Rank Adaptation of Large Language Models scored 78%..."
            },
            {
              "docId": "2026-01-03_bloomberg_5902",
              "url": "https://bloomberg.com/technology/2026/01/03/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-03",
              "snippet": "Evaluation results show LoRA: Low-Rank Adaptation of Large Language Models reaching 89% on Grok-3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_plus_group_query_attention_uses_tech",
          "source": "tool:tensorrt_llm_plus",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:ollama_core_grok_3_core_integrates_with",
          "source": "repo:ollama_core",
          "target": "model:grok_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:gpt_5_pro_mbpp_evaluated_on",
          "source": "model:gpt_5_pro",
          "target": "benchmark:mbpp",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_red_teaming_lite_uses_tech",
          "source": "model:gpt_5_v2",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-23_ars_technica_3931",
              "url": "https://arstechnica.com/2026/01/23/gpt_5_v2_red_teaming_lite",
              "published": "2026-01-23",
              "snippet": "GPT-5 v2 leverages Red Teaming Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_claude_opus_45_plus_evaluated_on",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "model:claude_opus_45_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-15_bloomberg_8217",
              "url": "https://bloomberg.com/technology/2026/01/15/direct_preference_optimization",
              "published": "2026-01-15",
              "snippet": "Direct Preference Optimization Ultra achieves 74% on Claude Opus 4.5 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_flash_attention_next_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:flash_attention_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-17_reuters_1722",
              "url": "https://reuters.com/technology/2026/01/17/direct_preference_optimization",
              "published": "2026-01-17",
              "snippet": "Direct Preference Optimization Ultra leverages Flash Attention Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_falcon_3_lite_integrates_with",
          "source": "repo:vllm_edge",
          "target": "model:falcon_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_hellaswag_evaluated_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-20_hugging_face_bl_1076",
              "url": "https://huggingface.co/blog/2026/01/20/stable_diffusion_4_plus_hellas",
              "published": "2026-01-20",
              "snippet": "Stable Diffusion 4 Plus achieves 77% on HellaSwag, setting a new record..."
            },
            {
              "docId": "2026-01-20_meta_ai_blog_1920",
              "url": "https://ai.meta.com/blog/2026/01/20/stable_diffusion_4_plus_hellas",
              "published": "2026-01-20",
              "snippet": "Stable Diffusion 4 Plus achieves 84% on HellaSwag, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_plus_mixtral_8x22b_edge_measures",
          "source": "benchmark:lmsys_chatbot_arena_plus",
          "target": "model:mixtral_8x22b_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_8633",
              "url": "https://blog.google/technology/ai/2026/01/25/lmsys_chatbot_arena_plus_mixtr",
              "published": "2026-01-25",
              "snippet": "The LMSYS Chatbot Arena Plus benchmark measures Mixtral 8x22B Edge across multiple tasks..."
            },
            {
              "docId": "2026-01-25_mit_technology__4088",
              "url": "https://technologyreview.com/2026/01/25/lmsys_chatbot_arena_plus_mixtr",
              "published": "2026-01-25",
              "snippet": "LMSYS Chatbot Arena Plus provides standardized evaluation of Mixtral 8x22B Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_max_rlhf_ultra_uses_tech",
          "source": "model:midjourney_v7_max",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_phi_4_edge_integrates_with",
          "source": "repo:ollama_edge",
          "target": "model:phi_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-03_bloomberg_1560",
              "url": "https://bloomberg.com/technology/2025/12/03/ollama_edge_phi_4_edge",
              "published": "2025-12-03",
              "snippet": "ollama Edge announced official support for Phi-4 Edge..."
            },
            {
              "docId": "2025-12-07_bloomberg_2239",
              "url": "https://bloomberg.com/technology/2025/12/07/ollama_edge_phi_4_edge",
              "published": "2025-12-07",
              "snippet": "The latest release of ollama Edge adds native Phi-4 Edge integration..."
            },
            {
              "docId": "2025-12-11_bloomberg_4129",
              "url": "https://bloomberg.com/technology/2025/12/11/ollama_edge_phi_4_edge",
              "published": "2025-12-11",
              "snippet": "ollama Edge now supports Phi-4 Edge with full feature parity..."
            },
            {
              "docId": "2025-12-29_the_verge_6325",
              "url": "https://theverge.com/2025/12/29/ollama_edge_phi_4_edge",
              "published": "2025-12-29",
              "snippet": "The latest release of ollama Edge adds native Phi-4 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_gguf_uses_tech",
          "source": "model:mixtral_8x22b_plus",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_qwen_3_pro_evaluated_on",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "model:qwen_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_1181",
              "url": "https://reuters.com/technology/2026/01/24/mixture_of_experts_meets_instr",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Mixture of Experts Meets Instruction Tuning reaching 88% on Qwen-3 Pro..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_2410",
              "url": "https://blog.google/technology/ai/2026/01/24/mixture_of_experts_meets_instr",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Mixture of Experts Meets Instruction Tuning reaching 71% on Qwen-3 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_mixtral_8x22b_pro_depends_on",
          "source": "model:midjourney_v7",
          "target": "model:mixtral_8x22b_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_mmlu_mini_evaluated_on",
          "source": "model:yi_large_next",
          "target": "benchmark:mmlu_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-02_reuters_2724",
              "url": "https://reuters.com/technology/2026/01/02/yi_large_next_mmlu_mini",
              "published": "2026-01-02",
              "snippet": "On the MMLU Mini benchmark, Yi-Large Next scored 79%..."
            },
            {
              "docId": "2026-01-03_hugging_face_bl_4409",
              "url": "https://huggingface.co/blog/2026/01/03/yi_large_next_mmlu_mini",
              "published": "2026-01-03",
              "snippet": "Yi-Large Next achieves 93% on MMLU Mini, setting a new record..."
            },
            {
              "docId": "2026-01-04_the_verge_1864",
              "url": "https://theverge.com/2026/01/04/yi_large_next_mmlu_mini",
              "published": "2026-01-04",
              "snippet": "Evaluation results show Yi-Large Next reaching 97% on MMLU Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_deepseek_v3_next_depends_on",
          "source": "model:yi_large_next",
          "target": "model:deepseek_v3_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_mixtral_8x22b_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:mixtral_8x22b",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-17_techcrunch_5833",
              "url": "https://techcrunch.com/2026/01/17/direct_preference_optimization",
              "published": "2026-01-17",
              "snippet": "On the Mixtral 8x22B benchmark, Direct Preference Optimization scored 98%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:refinedweb_next_tree_of_thought_edge_uses_tech",
          "source": "dataset:refinedweb_next",
          "target": "tech:tree_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:crewai_edge_streamlit_mini_integrates_with",
          "source": "tool:crewai_edge",
          "target": "tool:streamlit_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_2276",
              "url": "https://techcrunch.com/2026/01/25/crewai_edge_streamlit_mini",
              "published": "2026-01-25",
              "snippet": "The latest release of CrewAI Edge adds native Streamlit Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_chain_of_thought_uses_tech",
          "source": "dataset:wikipedia_dump_2025",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_claude_sonnet_4_integrates_with",
          "source": "tool:tensorrt_llm_v2",
          "target": "model:claude_sonnet_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-20_wired_5764",
              "url": "https://wired.com/2026/01/20/tensorrt_llm_v2_claude_sonnet_",
              "published": "2026-01-20",
              "snippet": "The latest release of TensorRT-LLM v2 adds native Claude Sonnet 4 integration..."
            },
            {
              "docId": "2026-01-23_bloomberg_3658",
              "url": "https://bloomberg.com/technology/2026/01/23/tensorrt_llm_v2_claude_sonnet_",
              "published": "2026-01-23",
              "snippet": "The latest release of TensorRT-LLM v2 adds native Claude Sonnet 4 integration..."
            },
            {
              "docId": "2026-01-23_microsoft_resea_9928",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/tensorrt_llm_v2_claude_sonnet_",
              "published": "2026-01-23",
              "snippet": "TensorRT-LLM v2 now supports Claude Sonnet 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_cody_edge_integrates_with",
          "source": "tool:mlflow",
          "target": "tool:cody_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:humaneval_next_aya_3_measures",
          "source": "benchmark:humaneval_next",
          "target": "model:aya_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-19_langchain_blog_7137",
              "url": "https://blog.langchain.dev/2025/12/19/humaneval_next_aya_3",
              "published": "2025-12-19",
              "snippet": "The HumanEval Next benchmark measures Aya 3 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_pro_tokenizer_bpe_core_uses_tech",
          "source": "dataset:starcoder_data_pro",
          "target": "tech:tokenizer_bpe_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:litellm_max_vllm_next_integrates_with",
          "source": "tool:litellm_max",
          "target": "tool:vllm_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-14_techcrunch_9698",
              "url": "https://techcrunch.com/2026/01/14/litellm_max_vllm_next",
              "published": "2026-01-14",
              "snippet": "The latest release of LiteLLM Max adds native vLLM Next integration..."
            },
            {
              "docId": "2026-01-23_nextgov_9825",
              "url": "https://nextgov.com/2026/01/23/litellm_max_vllm_next",
              "published": "2026-01-23",
              "snippet": "LiteLLM Max announced official support for vLLM Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_next_copilot_core_integrates_with",
          "source": "tool:langchain_next",
          "target": "tool:copilot_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:streamlit_next_dify_max_integrates_with",
          "source": "tool:streamlit_next",
          "target": "tool:dify_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-14_microsoft_resea_8913",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/14/streamlit_next_dify_max",
              "published": "2026-01-14",
              "snippet": "Streamlit Next announced official support for Dify Max..."
            },
            {
              "docId": "2026-01-22_the_gradient_7206",
              "url": "https://thegradient.pub/2026/01/22/streamlit_next_dify_max",
              "published": "2026-01-22",
              "snippet": "Streamlit Next announced official support for Dify Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_nemotron_5_mini_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:nemotron_5_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-13_reuters_5195",
              "url": "https://reuters.com/technology/2026/01/13/textbooks_are_all_you_need_min",
              "published": "2026-01-13",
              "snippet": "Textbooks Are All You Need Mini achieves 84% on Nemotron-5 Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_next_copilot_next_integrates_with",
          "source": "tool:haystack_next",
          "target": "tool:copilot_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_midjourney_v7_max_depends_on",
          "source": "model:command_r_plus_edge",
          "target": "model:midjourney_v7_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_mixture_of_experts_plus_uses_tech",
          "source": "dataset:redpajama_v2",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:langchain_pro_litellm_v2_integrates_with",
          "source": "tool:langchain_pro",
          "target": "tool:litellm_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_humaneval_next_evaluated_on",
          "source": "model:codex_2_core",
          "target": "benchmark:humaneval_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-20_ars_technica_8620",
              "url": "https://arstechnica.com/2026/01/20/codex_2_core_humaneval_next",
              "published": "2026-01-20",
              "snippet": "On the HumanEval Next benchmark, Codex 2 Core scored 97%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_plus_tree_of_thought_core_uses_tech",
          "source": "tool:vllm_plus",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_qwen_3_plus_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:qwen_3_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-27_ars_technica_1787",
              "url": "https://arstechnica.com/2025/12/27/direct_preference_optimization",
              "published": "2025-12-27",
              "snippet": "Evaluation results show Direct Preference Optimization v2 reaching 91% on Qwen-3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_max_qwen_3_max_measures",
          "source": "benchmark:hellaswag_max",
          "target": "model:qwen_3_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-21_meta_ai_blog_1106",
              "url": "https://ai.meta.com/blog/2026/01/21/hellaswag_max_qwen_3_max",
              "published": "2026-01-21",
              "snippet": "The HellaSwag Max benchmark measures Qwen-3 Max across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_whisper_v4_ultra_integrates_with",
          "source": "tool:ollama",
          "target": "model:whisper_v4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-12-24_venturebeat_1563",
              "url": "https://venturebeat.com/2025/12/24/ollama_whisper_v4_ultra",
              "published": "2025-12-24",
              "snippet": "The latest release of Ollama adds native Whisper v4 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:refinedweb_ultra_chain_of_thought_plus_uses_tech",
          "source": "dataset:refinedweb_ultra",
          "target": "tech:chain_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:ollama_claude_opus_45_max_integrates_with",
          "source": "repo:ollama",
          "target": "model:claude_opus_45_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-30_langchain_blog_7574",
              "url": "https://blog.langchain.dev/2025/12/30/ollama_claude_opus_45_max",
              "published": "2025-12-30",
              "snippet": "ollama announced official support for Claude Opus 4.5 Max..."
            },
            {
              "docId": "2026-01-09_bloomberg_5469",
              "url": "https://bloomberg.com/technology/2026/01/09/ollama_claude_opus_45_max",
              "published": "2026-01-09",
              "snippet": "The latest release of ollama adds native Claude Opus 4.5 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_next_gpt_4o_mini_2_pro_integrates_with",
          "source": "tool:flowise_next",
          "target": "model:gpt_4o_mini_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:llamaindex_ultra_weights_and_biases_core_integrates_with",
          "source": "tool:llamaindex_ultra",
          "target": "tool:weights_and_biases_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_stable_diffusion_4_lite_depends_on",
          "source": "model:claude_opus_45_lite",
          "target": "model:stable_diffusion_4_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_claude_sonnet_4_mini_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:claude_sonnet_4_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_6874",
              "url": "https://anthropic.com/news/2026/01/25/attention_is_all_you_need_v2_u",
              "published": "2026-01-25",
              "snippet": "On the Claude Sonnet 4 Mini benchmark, Attention Is All You Need v2 Ultra scored 73%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_next_palm_3_measures",
          "source": "benchmark:mmlu_next",
          "target": "model:palm_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-16_hugging_face_bl_7655",
              "url": "https://huggingface.co/blog/2025/12/16/mmlu_next_palm_3",
              "published": "2025-12-16",
              "snippet": "MMLU Next provides standardized evaluation of PaLM 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_tokenizer_bpe_lite_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:math_edge_codex_2_mini_measures",
          "source": "benchmark:math_edge",
          "target": "model:codex_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-17_google_ai_blog_5811",
              "url": "https://blog.google/technology/ai/2026/01/17/math_edge_codex_2_mini",
              "published": "2026-01-17",
              "snippet": "The MATH Edge benchmark measures Codex 2 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-24_wired_7091",
              "url": "https://wired.com/2026/01/24/math_edge_codex_2_mini",
              "published": "2026-01-24",
              "snippet": "MATH Edge provides standardized evaluation of Codex 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_refinedweb_next_trained_on",
          "source": "model:stable_diffusion_4_v2",
          "target": "dataset:refinedweb_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-13_openai_blog_2722",
              "url": "https://openai.com/blog/2026/01/13/stable_diffusion_4_v2_refinedw",
              "published": "2026-01-13",
              "snippet": "The training corpus for Stable Diffusion 4 v2 includes RefinedWeb Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_math_pro_evaluated_on",
          "source": "model:gemma_3_plus",
          "target": "benchmark:math_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-20_nextgov_4289",
              "url": "https://nextgov.com/2026/01/20/gemma_3_plus_math_pro",
              "published": "2026-01-20",
              "snippet": "On the MATH Pro benchmark, Gemma 3 Plus scored 76%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_synthetic_data_generation_core_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_core_quantization_v2_uses_tech",
          "source": "tool:weights_and_biases_core",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_midjourney_v7_edge_evaluated_on",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "model:midjourney_v7_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-02_wired_1032",
              "url": "https://wired.com/2026/01/02/llm_agents:_a_survey_core_midj",
              "published": "2026-01-02",
              "snippet": "On the Midjourney V7 Edge benchmark, LLM Agents: A Survey Core scored 86%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_lmsys_chatbot_arena_core_evaluated_on",
          "source": "model:midjourney_v7",
          "target": "benchmark:lmsys_chatbot_arena_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2025-12-15_techcrunch_2959",
              "url": "https://techcrunch.com/2025/12/15/midjourney_v7_lmsys_chatbot_ar",
              "published": "2025-12-15",
              "snippet": "On the LMSYS Chatbot Arena Core benchmark, Midjourney V7 scored 82%..."
            },
            {
              "docId": "2025-12-20_bloomberg_2642",
              "url": "https://bloomberg.com/technology/2025/12/20/midjourney_v7_lmsys_chatbot_ar",
              "published": "2025-12-20",
              "snippet": "Midjourney V7 achieves 91% on LMSYS Chatbot Arena Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_claude_opus_45_pro_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:claude_opus_45_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:winogrande_pro_yi_large_measures",
          "source": "benchmark:winogrande_pro",
          "target": "model:yi_large",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-18_wired_1299",
              "url": "https://wired.com/2026/01/18/winogrande_pro_yi_large",
              "published": "2026-01-18",
              "snippet": "WinoGrande Pro provides standardized evaluation of Yi-Large..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_multimodal_fusion_max_uses_tech",
          "source": "repo:llamacpp_v2",
          "target": "tech:multimodal_fusion_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2025-12-31_bloomberg_8742",
              "url": "https://bloomberg.com/technology/2025/12/31/llamacpp_v2_multimodal_fusion_",
              "published": "2025-12-31",
              "snippet": "Under the hood, llama.cpp v2 implements Multimodal Fusion Max for improved efficiency..."
            },
            {
              "docId": "2026-01-13_the_verge_7565",
              "url": "https://theverge.com/2026/01/13/llamacpp_v2_multimodal_fusion_",
              "published": "2026-01-13",
              "snippet": "llama.cpp v2 leverages Multimodal Fusion Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_rlhf_ultra_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_sparse_attention_pro_uses_tech",
          "source": "repo:llamacpp_v2",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:vllm_max_constitutional_ai_plus_uses_tech",
          "source": "repo:vllm_max",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-19_google_ai_blog_9007",
              "url": "https://blog.google/technology/ai/2026/01/19/vllm_max_constitutional_ai_plu",
              "published": "2026-01-19",
              "snippet": "Technical details reveal vllm Max relies heavily on Constitutional AI Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_alpacaeval_2_plus_evaluated_on",
          "source": "model:deepseek_v3_max",
          "target": "benchmark:alpacaeval_2_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.96
        }
      },
      {
        "data": {
          "id": "e:codex_2_mini_refinedweb_lite_trained_on",
          "source": "model:codex_2_mini",
          "target": "dataset:refinedweb_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_phi_4_pro_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:phi_4_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-18_the_gradient_3976",
              "url": "https://thegradient.pub/2026/01/18/self_play_fine_tuning_for_lang",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Self-Play Fine-Tuning for Language Models reaching 89% on Phi-4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_next_gguf_core_uses_tech",
          "source": "tool:localai_next",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:mt_bench_v2_command_r_plus_measures",
          "source": "benchmark:mt_bench_v2",
          "target": "model:command_r_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-15_google_ai_blog_6455",
              "url": "https://blog.google/technology/ai/2026/01/15/mt_bench_v2_command_r_plus",
              "published": "2026-01-15",
              "snippet": "MT-Bench v2 has become the standard for evaluating Command R+..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_qwen_3_lite_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:qwen_3_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-09_bloomberg_8676",
              "url": "https://bloomberg.com/technology/2026/01/09/attention_is_all_you_need_v2_p",
              "published": "2026-01-09",
              "snippet": "On the Qwen-3 Lite benchmark, Attention Is All You Need v2 Plus scored 93%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_sliding_window_attention_plus_uses_tech",
          "source": "model:codex_2_core",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:vllm_v2_yi_large_lite_integrates_with",
          "source": "repo:vllm_v2",
          "target": "model:yi_large_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:math_pro_sora_2_measures",
          "source": "benchmark:math_pro",
          "target": "model:sora_2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-27_arxiv_8666",
              "url": "https://arxiv.org/abs/2025/12/27/math_pro_sora_2",
              "published": "2025-12-27",
              "snippet": "MATH Pro provides standardized evaluation of Sora 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_tokenizer_bpe_edge_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:tokenizer_bpe_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_group_query_attention_lite_uses_tech",
          "source": "model:jamba_2_lite",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:llamaindex_max_haystack_lite_integrates_with",
          "source": "tool:llamaindex_max",
          "target": "tool:haystack_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_llama_4_pro_measures",
          "source": "benchmark:bigbench_hard",
          "target": "model:llama_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-11-25_anthropic_blog_5881",
              "url": "https://anthropic.com/news/2025/11/25/bigbench_hard_llama_4_pro",
              "published": "2025-11-25",
              "snippet": "BigBench Hard provides standardized evaluation of Llama 4 Pro..."
            },
            {
              "docId": "2026-01-01_venturebeat_9907",
              "url": "https://venturebeat.com/2026/01/01/bigbench_hard_llama_4_pro",
              "published": "2026-01-01",
              "snippet": "The BigBench Hard benchmark measures Llama 4 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_rlhf_mini_uses_tech",
          "source": "tool:semantic_kernel_v2",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-11-20_the_verge_5781",
              "url": "https://theverge.com/2025/11/20/semantic_kernel_v2_rlhf_mini",
              "published": "2025-11-20",
              "snippet": "Technical details reveal Semantic Kernel v2 relies heavily on RLHF Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_gpqa_plus_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:gpqa_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_3934",
              "url": "https://theverge.com/2026/01/25/claude_opus_45_gpqa_plus",
              "published": "2026-01-25",
              "snippet": "On the GPQA Plus benchmark, Claude Opus 4.5 scored 78%..."
            },
            {
              "docId": "2026-01-25_wired_9808",
              "url": "https://wired.com/2026/01/25/claude_opus_45_gpqa_plus",
              "published": "2026-01-25",
              "snippet": "On the GPQA Plus benchmark, Claude Opus 4.5 scored 99%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_ultra_jamba_2_mini_integrates_with",
          "source": "tool:cody_ultra",
          "target": "model:jamba_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_max_qwen_3_integrates_with",
          "source": "tool:semantic_kernel_max",
          "target": "model:qwen_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_1027",
              "url": "https://openai.com/blog/2026/01/25/semantic_kernel_max_qwen_3",
              "published": "2026-01-25",
              "snippet": "Semantic Kernel Max now supports Qwen-3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_winogrande_evaluated_on",
          "source": "model:whisper_v4_core",
          "target": "benchmark:winogrande",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_3006",
              "url": "https://blog.langchain.dev/2026/01/17/whisper_v4_core_winogrande",
              "published": "2026-01-17",
              "snippet": "On the WinoGrande benchmark, Whisper v4 Core scored 75%..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_8337",
              "url": "https://blogs.nvidia.com/2026/01/24/whisper_v4_core_winogrande",
              "published": "2026-01-24",
              "snippet": "On the WinoGrande benchmark, Whisper v4 Core scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_palm_3_core_integrates_with",
          "source": "repo:vllm_core",
          "target": "model:palm_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-15_microsoft_resea_5200",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/15/vllm_core_palm_3_core",
              "published": "2026-01-15",
              "snippet": "vllm Core now supports PaLM 3 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__deepseek_v3_v2_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:deepseek_v3_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-16_anthropic_blog_5560",
              "url": "https://anthropic.com/news/2026/01/16/constitutional_ai:_harmlessnes",
              "published": "2026-01-16",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 90% on DeepSeek-V3 v2, setting a new record..."
            },
            {
              "docId": "2026-01-23_hugging_face_bl_2983",
              "url": "https://huggingface.co/blog/2026/01/23/constitutional_ai:_harmlessnes",
              "published": "2026-01-23",
              "snippet": "On the DeepSeek-V3 v2 benchmark, Constitutional AI: Harmlessness from AI Feedback scored 79%..."
            },
            {
              "docId": "2026-01-23_mit_technology__4023",
              "url": "https://technologyreview.com/2026/01/23/constitutional_ai:_harmlessnes",
              "published": "2026-01-23",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 90% on DeepSeek-V3 v2, setting a new record..."
            },
            {
              "docId": "2026-01-25_nextgov_4739",
              "url": "https://nextgov.com/2026/01/25/constitutional_ai:_harmlessnes",
              "published": "2026-01-25",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 88% on DeepSeek-V3 v2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_qlora_mini_uses_tech",
          "source": "model:aya_3_max",
          "target": "tech:qlora_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2025-11-22_ars_technica_5108",
              "url": "https://arstechnica.com/2025/11/22/aya_3_max_qlora_mini",
              "published": "2025-11-22",
              "snippet": "Under the hood, Aya 3 Max implements QLoRA Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_max_sliding_window_attention_mini_uses_tech",
          "source": "dataset:c4_max",
          "target": "tech:sliding_window_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:ollama_phi_4_pro_integrates_with",
          "source": "repo:ollama",
          "target": "model:phi_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-06_nvidia_blog_7328",
              "url": "https://blogs.nvidia.com/2026/01/06/ollama_phi_4_pro",
              "published": "2026-01-06",
              "snippet": "The latest release of ollama adds native Phi-4 Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_nemotron_5_next_measures",
          "source": "benchmark:math",
          "target": "model:nemotron_5_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2025-12-08_ars_technica_9088",
              "url": "https://arstechnica.com/2025/12/08/math_nemotron_5_next",
              "published": "2025-12-08",
              "snippet": "The MATH benchmark measures Nemotron-5 Next across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_max_synthetic_data_generation_mini_uses_tech",
          "source": "repo:vllm_max",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:cursor_max_sora_2_mini_integrates_with",
          "source": "tool:cursor_max",
          "target": "model:sora_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_whisper_v4_ultra_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:whisper_v4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-17_anthropic_blog_3611",
              "url": "https://anthropic.com/news/2026/01/17/llamacpp_mini_whisper_v4_ultra",
              "published": "2026-01-17",
              "snippet": "The latest release of llama.cpp Mini adds native Whisper v4 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_qwen_3_v2_depends_on",
          "source": "model:whisper_v4_core",
          "target": "model:qwen_3_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:streamlit_max_sora_2_integrates_with",
          "source": "tool:streamlit_max",
          "target": "model:sora_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-06_hugging_face_bl_2343",
              "url": "https://huggingface.co/blog/2026/01/06/streamlit_max_sora_2",
              "published": "2026-01-06",
              "snippet": "Streamlit Max announced official support for Sora 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_sora_2_integrates_with",
          "source": "tool:mlflow_max",
          "target": "model:sora_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_gpt_4o_mini_2_pro_depends_on",
          "source": "model:claude_opus_45_core",
          "target": "model:gpt_4o_mini_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_chain_of_thought_edge_uses_tech",
          "source": "model:mixtral_8x22b",
          "target": "tech:chain_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:gpt_5_next_synthetic_data_generation_max_uses_tech",
          "source": "model:gpt_5_next",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-07_the_verge_7030",
              "url": "https://theverge.com/2026/01/07/gpt_5_next_synthetic_data_gene",
              "published": "2026-01-07",
              "snippet": "Under the hood, GPT-5 Next implements Synthetic Data Generation Max for improved efficiency..."
            },
            {
              "docId": "2026-01-16_techcrunch_3405",
              "url": "https://techcrunch.com/2026/01/16/gpt_5_next_synthetic_data_gene",
              "published": "2026-01-16",
              "snippet": "GPT-5 Next leverages Synthetic Data Generation Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_sliding_window_attention_next_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-23_arxiv_8455",
              "url": "https://arxiv.org/abs/2025/12/23/flash_attention:_fast_and_memo",
              "published": "2025-12-23",
              "snippet": "Under the hood, Flash Attention: Fast and Memory-Efficient Attention implements Sliding Window Attention Next for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_pro_vllm_integrates_with",
          "source": "tool:haystack_pro",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:palm_3_edge_redpajama_v2_plus_trained_on",
          "source": "model:palm_3_edge",
          "target": "dataset:redpajama_v2_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-22_meta_ai_blog_3140",
              "url": "https://ai.meta.com/blog/2026/01/22/palm_3_edge_redpajama_v2_plus",
              "published": "2026-01-22",
              "snippet": "PaLM 3 Edge was trained on RedPajama v2 Plus comprising billions of tokens..."
            },
            {
              "docId": "2026-01-22_weights_and_bia_5992",
              "url": "https://wandb.ai/articles/2026/01/22/palm_3_edge_redpajama_v2_plus",
              "published": "2026-01-22",
              "snippet": "PaLM 3 Edge was trained on RedPajama v2 Plus comprising billions of tokens..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_6975",
              "url": "https://wandb.ai/articles/2026/01/24/palm_3_edge_redpajama_v2_plus",
              "published": "2026-01-24",
              "snippet": "PaLM 3 Edge utilized RedPajama v2 Plus as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_v2_deepseek_v3_mini_depends_on",
          "source": "model:nemotron_5_v2",
          "target": "model:deepseek_v3_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gpt4all_core_synthetic_data_generation_plus_uses_tech",
          "source": "repo:gpt4all_core",
          "target": "tech:synthetic_data_generation_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-23_meta_ai_blog_8893",
              "url": "https://ai.meta.com/blog/2026/01/23/gpt4all_core_synthetic_data_ge",
              "published": "2026-01-23",
              "snippet": "Technical details reveal gpt4all Core relies heavily on Synthetic Data Generation Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_core_gpt_4o_mini_2_next_measures",
          "source": "benchmark:mmlu_core",
          "target": "model:gpt_4o_mini_2_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2025-12-25_techcrunch_2600",
              "url": "https://techcrunch.com/2025/12/25/mmlu_core_gpt_4o_mini_2_next",
              "published": "2025-12-25",
              "snippet": "MMLU Core has become the standard for evaluating GPT-4o Mini 2 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_next_gsm8k_mini_evaluated_on",
          "source": "model:aya_3_next",
          "target": "benchmark:gsm8k_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_4861",
              "url": "https://arstechnica.com/2026/01/25/aya_3_next_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Aya 3 Next reaching 92% on GSM8K Mini..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_1554",
              "url": "https://ai.meta.com/blog/2026/01/25/aya_3_next_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Aya 3 Next reaching 74% on GSM8K Mini..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_5222",
              "url": "https://blog.google/technology/ai/2026/01/25/aya_3_next_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Aya 3 Next reaching 90% on GSM8K Mini..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_2700",
              "url": "https://anthropic.com/news/2026/01/25/aya_3_next_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "On the GSM8K Mini benchmark, Aya 3 Next scored 72%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_humaneval_evaluated_on",
          "source": "model:gemma_3_mini",
          "target": "benchmark:humaneval",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_sliding_window_attention_lite_uses_tech",
          "source": "model:stable_diffusion_4_plus",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:refinedweb_ultra_dpo_v2_uses_tech",
          "source": "dataset:refinedweb_ultra",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:flowise_v2_dall_e_4_mini_integrates_with",
          "source": "tool:flowise_v2",
          "target": "model:dall_e_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:cursor_core_gemini_ultra_2_core_integrates_with",
          "source": "tool:cursor_core",
          "target": "model:gemini_ultra_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-09_microsoft_resea_1147",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/09/cursor_core_gemini_ultra_2_cor",
              "published": "2026-01-09",
              "snippet": "Cursor Core announced official support for Gemini Ultra 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_pro_red_teaming_lite_uses_tech",
          "source": "tool:crewai_pro",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:crewai_v2_phi_4_lite_integrates_with",
          "source": "tool:crewai_v2",
          "target": "model:phi_4_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-24_openai_blog_3557",
              "url": "https://openai.com/blog/2026/01/24/crewai_v2_phi_4_lite",
              "published": "2026-01-24",
              "snippet": "CrewAI v2 announced official support for Phi-4 Lite..."
            },
            {
              "docId": "2026-01-24_langchain_blog_1853",
              "url": "https://blog.langchain.dev/2026/01/24/crewai_v2_phi_4_lite",
              "published": "2026-01-24",
              "snippet": "CrewAI v2 now supports Phi-4 Lite with full feature parity..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_3893",
              "url": "https://wandb.ai/articles/2026/01/25/crewai_v2_phi_4_lite",
              "published": "2026-01-25",
              "snippet": "The latest release of CrewAI v2 adds native Phi-4 Lite integration..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_4629",
              "url": "https://anthropic.com/news/2026/01/25/crewai_v2_phi_4_lite",
              "published": "2026-01-25",
              "snippet": "CrewAI v2 now supports Phi-4 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_lite_kv_cache_optimization_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-19_langchain_blog_2309",
              "url": "https://blog.langchain.dev/2026/01/19/localai_lite_kv_cache_optimiza",
              "published": "2026-01-19",
              "snippet": "Technical details reveal LocalAI Lite relies heavily on KV Cache Optimization..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_aya_3_next_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:aya_3_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:grok_3_lite_quantization_ultra_uses_tech",
          "source": "model:grok_3_lite",
          "target": "tech:quantization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:arc_agi_max_yi_large_lite_measures",
          "source": "benchmark:arc_agi_max",
          "target": "model:yi_large_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-14_nvidia_blog_6380",
              "url": "https://blogs.nvidia.com/2026/01/14/arc_agi_max_yi_large_lite",
              "published": "2026-01-14",
              "snippet": "ARC-AGI Max has become the standard for evaluating Yi-Large Lite..."
            },
            {
              "docId": "2026-01-21_mit_technology__5679",
              "url": "https://technologyreview.com/2026/01/21/arc_agi_max_yi_large_lite",
              "published": "2026-01-21",
              "snippet": "ARC-AGI Max has become the standard for evaluating Yi-Large Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_v2_humaneval_pro_evaluated_on",
          "source": "model:gemini_ultra_2_v2",
          "target": "benchmark:humaneval_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-14_techcrunch_6570",
              "url": "https://techcrunch.com/2026/01/14/gemini_ultra_2_v2_humaneval_pr",
              "published": "2026-01-14",
              "snippet": "On the HumanEval Pro benchmark, Gemini Ultra 2 v2 scored 82%..."
            },
            {
              "docId": "2026-01-21_google_ai_blog_5973",
              "url": "https://blog.google/technology/ai/2026/01/21/gemini_ultra_2_v2_humaneval_pr",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Gemini Ultra 2 v2 reaching 81% on HumanEval Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_tokenizer_bpe_core_uses_tech",
          "source": "tool:crewai",
          "target": "tech:tokenizer_bpe_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:llamaindex_core_aya_3_next_integrates_with",
          "source": "tool:llamaindex_core",
          "target": "model:aya_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-08_nextgov_7206",
              "url": "https://nextgov.com/2025/12/08/llamaindex_core_aya_3_next",
              "published": "2025-12-08",
              "snippet": "LlamaIndex Core now supports Aya 3 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_ultra_mc4_plus_trained_on",
          "source": "model:dall_e_4_ultra",
          "target": "dataset:mc4_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:vllm_gguf_pro_uses_tech",
          "source": "repo:vllm",
          "target": "tech:gguf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-21_weights_and_bia_3059",
              "url": "https://wandb.ai/articles/2026/01/21/vllm_gguf_pro",
              "published": "2026-01-21",
              "snippet": "Under the hood, vllm implements GGUF Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-23_mit_technology__6128",
              "url": "https://technologyreview.com/2026/01/23/vllm_gguf_pro",
              "published": "2026-01-23",
              "snippet": "vllm leverages GGUF Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_red_teaming_v2_uses_tech",
          "source": "repo:text_generation_webui_pro",
          "target": "tech:red_teaming_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-14_mit_technology__5141",
              "url": "https://technologyreview.com/2025/12/14/text_generation_webui_pro_red_",
              "published": "2025-12-14",
              "snippet": "Technical details reveal text-generation-webui Pro relies heavily on Red Teaming v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_core_roots_v2_trained_on",
          "source": "model:palm_3_core",
          "target": "dataset:roots_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-01_google_ai_blog_2592",
              "url": "https://blog.google/technology/ai/2026/01/01/palm_3_core_roots_v2",
              "published": "2026-01-01",
              "snippet": "PaLM 3 Core utilized ROOTS v2 as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_next_claude_opus_45_edge_integrates_with",
          "source": "tool:localai_next",
          "target": "model:claude_opus_45_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-15_microsoft_resea_5550",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/15/localai_next_claude_opus_45_ed",
              "published": "2026-01-15",
              "snippet": "The latest release of LocalAI Next adds native Claude Opus 4.5 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_sliding_window_attention_plus_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_openwebtext2_trained_on",
          "source": "model:midjourney_v7",
          "target": "dataset:openwebtext2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:qwen_3_plus_rotary_position_embedding_v2_uses_tech",
          "source": "model:qwen_3_plus",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_wikipedia_dump_2025_trained_on",
          "source": "model:claude_sonnet_4",
          "target": "dataset:wikipedia_dump_2025",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-09_nvidia_blog_1170",
              "url": "https://blogs.nvidia.com/2026/01/09/claude_sonnet_4_wikipedia_dump",
              "published": "2026-01-09",
              "snippet": "The training corpus for Claude Sonnet 4 includes Wikipedia Dump 2025..."
            },
            {
              "docId": "2026-01-16_weights_and_bia_5848",
              "url": "https://wandb.ai/articles/2026/01/16/claude_sonnet_4_wikipedia_dump",
              "published": "2026-01-16",
              "snippet": "Claude Sonnet 4 utilized Wikipedia Dump 2025 as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_pro_synthetic_data_generation_lite_uses_tech",
          "source": "tool:semantic_kernel_pro",
          "target": "tech:synthetic_data_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-05_techcrunch_7571",
              "url": "https://techcrunch.com/2026/01/05/semantic_kernel_pro_synthetic_",
              "published": "2026-01-05",
              "snippet": "Technical details reveal Semantic Kernel Pro relies heavily on Synthetic Data Generation Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_mini_ollama_v2_integrates_with",
          "source": "tool:dify_mini",
          "target": "tool:ollama_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_max_constitutional_ai_v2_uses_tech",
          "source": "dataset:the_stack_v2_max",
          "target": "tech:constitutional_ai_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:llamacpp_plus_chain_of_thought_plus_uses_tech",
          "source": "repo:llamacpp_plus",
          "target": "tech:chain_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2025-12-08_weights_and_bia_3757",
              "url": "https://wandb.ai/articles/2025/12/08/llamacpp_plus_chain_of_thought",
              "published": "2025-12-08",
              "snippet": "Technical details reveal llama.cpp Plus relies heavily on Chain-of-Thought Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_mini_roots_next_trained_on",
          "source": "model:qwen_3_mini",
          "target": "dataset:roots_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2025-12-04_google_ai_blog_2341",
              "url": "https://blog.google/technology/ai/2025/12/04/qwen_3_mini_roots_next",
              "published": "2025-12-04",
              "snippet": "Qwen-3 Mini was trained on ROOTS Next comprising billions of tokens..."
            },
            {
              "docId": "2025-12-09_weights_and_bia_5907",
              "url": "https://wandb.ai/articles/2025/12/09/qwen_3_mini_roots_next",
              "published": "2025-12-09",
              "snippet": "Qwen-3 Mini utilized ROOTS Next as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_mini_slimpajama_pro_trained_on",
          "source": "model:qwen_3_mini",
          "target": "dataset:slimpajama_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:cursor_max_gemini_ultra_2_edge_integrates_with",
          "source": "tool:cursor_max",
          "target": "model:gemini_ultra_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_5103",
              "url": "https://blog.google/technology/ai/2026/01/25/cursor_max_gemini_ultra_2_edge",
              "published": "2026-01-25",
              "snippet": "The latest release of Cursor Max adds native Gemini Ultra 2 Edge integration..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_7795",
              "url": "https://ai.meta.com/blog/2026/01/25/cursor_max_gemini_ultra_2_edge",
              "published": "2026-01-25",
              "snippet": "Cursor Max now supports Gemini Ultra 2 Edge with full feature parity..."
            },
            {
              "docId": "2026-01-25_the_gradient_2021",
              "url": "https://thegradient.pub/2026/01/25/cursor_max_gemini_ultra_2_edge",
              "published": "2026-01-25",
              "snippet": "Cursor Max now supports Gemini Ultra 2 Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_v2_synthetic_data_generation_edge_uses_tech",
          "source": "model:gpt_5_v2",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-20_microsoft_resea_7118",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/gpt_5_v2_synthetic_data_genera",
              "published": "2026-01-20",
              "snippet": "Under the hood, GPT-5 v2 implements Synthetic Data Generation Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-23_weights_and_bia_5457",
              "url": "https://wandb.ai/articles/2026/01/23/gpt_5_v2_synthetic_data_genera",
              "published": "2026-01-23",
              "snippet": "GPT-5 v2 leverages Synthetic Data Generation Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_openai_blog_8222",
              "url": "https://openai.com/blog/2026/01/23/gpt_5_v2_synthetic_data_genera",
              "published": "2026-01-23",
              "snippet": "Technical details reveal GPT-5 v2 relies heavily on Synthetic Data Generation Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_math_v2_evaluated_on",
          "source": "model:nemotron_5_next",
          "target": "benchmark:math_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.96
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_flash_attention_mini_uses_tech",
          "source": "model:deepseek_v3",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-19_the_verge_3633",
              "url": "https://theverge.com/2026/01/19/deepseek_v3_flash_attention_mi",
              "published": "2026-01-19",
              "snippet": "Technical details reveal DeepSeek-V3 relies heavily on Flash Attention Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_mini_rlhf_pro_uses_tech",
          "source": "dataset:the_stack_v2_mini",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_max_midjourney_v7_mini_integrates_with",
          "source": "tool:tensorrt_llm_max",
          "target": "model:midjourney_v7_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-16_arxiv_5806",
              "url": "https://arxiv.org/abs/2026/01/16/tensorrt_llm_max_midjourney_v7",
              "published": "2026-01-16",
              "snippet": "The latest release of TensorRT-LLM Max adds native Midjourney V7 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_tool_use_core_uses_tech",
          "source": "repo:autogpt_v2",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-22_the_verge_3214",
              "url": "https://theverge.com/2026/01/22/autogpt_v2_tool_use_core",
              "published": "2026-01-22",
              "snippet": "Under the hood, AutoGPT v2 implements Tool Use Core for improved efficiency..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_5704",
              "url": "https://ai.meta.com/blog/2026/01/25/autogpt_v2_tool_use_core",
              "published": "2026-01-25",
              "snippet": "AutoGPT v2 leverages Tool Use Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_constitutional_ai_edge_uses_tech",
          "source": "model:mixtral_8x22b_mini",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_the_stack_v2_trained_on",
          "source": "model:mixtral_8x22b_mini",
          "target": "dataset:the_stack_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-19_wired_8608",
              "url": "https://wired.com/2026/01/19/mixtral_8x22b_mini_the_stack_v",
              "published": "2026-01-19",
              "snippet": "Mixtral 8x22B Mini was trained on The Stack v2 comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_max_tensorrt_llm_edge_integrates_with",
          "source": "tool:localai_max",
          "target": "tool:tensorrt_llm_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_gemini_ultra_2_plus_evaluated_on",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "model:gemini_ultra_2_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:slimpajama_core_rotary_position_embedding_v2_uses_tech",
          "source": "dataset:slimpajama_core",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_mt_bench_edge_evaluated_on",
          "source": "model:midjourney_v7",
          "target": "benchmark:mt_bench_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-19_the_verge_9268",
              "url": "https://theverge.com/2026/01/19/midjourney_v7_mt_bench_edge",
              "published": "2026-01-19",
              "snippet": "On the MT-Bench Edge benchmark, Midjourney V7 scored 78%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_edge_openwebtext2_trained_on",
          "source": "model:whisper_v4_edge",
          "target": "dataset:openwebtext2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:mt_bench_mixtral_8x22b_core_measures",
          "source": "benchmark:mt_bench",
          "target": "model:mixtral_8x22b_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-20_langchain_blog_1128",
              "url": "https://blog.langchain.dev/2026/01/20/mt_bench_mixtral_8x22b_core",
              "published": "2026-01-20",
              "snippet": "MT-Bench has become the standard for evaluating Mixtral 8x22B Core..."
            },
            {
              "docId": "2026-01-24_nextgov_5788",
              "url": "https://nextgov.com/2026/01/24/mt_bench_mixtral_8x22b_core",
              "published": "2026-01-24",
              "snippet": "MT-Bench has become the standard for evaluating Mixtral 8x22B Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_llama_4_plus_depends_on",
          "source": "model:llama_4_pro",
          "target": "model:llama_4_plus",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_nemotron_5_max_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:nemotron_5_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_2526",
              "url": "https://reuters.com/technology/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Direct Preference Optimization v2 reaching 72% on Nemotron-5 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_sora_2_edge_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:sora_2_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-01_bloomberg_8966",
              "url": "https://bloomberg.com/technology/2025/12/01/lora:_low_rank_adaptation_of_l",
              "published": "2025-12-01",
              "snippet": "Evaluation results show LoRA: Low-Rank Adaptation of Large Language Models reaching 73% on Sora 2 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_the_pile_core_trained_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "dataset:the_pile_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-09_anthropic_blog_7207",
              "url": "https://anthropic.com/news/2026/01/09/gemini_ultra_2_pro_the_pile_co",
              "published": "2026-01-09",
              "snippet": "Gemini Ultra 2 Pro utilized The Pile Core as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-20_hugging_face_bl_9721",
              "url": "https://huggingface.co/blog/2026/01/20/gemini_ultra_2_pro_the_pile_co",
              "published": "2026-01-20",
              "snippet": "Gemini Ultra 2 Pro utilized The Pile Core as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-21_mit_technology__4644",
              "url": "https://technologyreview.com/2026/01/21/gemini_ultra_2_pro_the_pile_co",
              "published": "2026-01-21",
              "snippet": "Gemini Ultra 2 Pro was trained on The Pile Core comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_ultra_synthetic_data_generation_core_uses_tech",
          "source": "dataset:fineweb_ultra",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_constitutional_ai_lite_uses_tech",
          "source": "model:gemma_3_mini",
          "target": "tech:constitutional_ai_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_mbpp_core_evaluated_on",
          "source": "model:yi_large_next",
          "target": "benchmark:mbpp_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-22_openai_blog_6521",
              "url": "https://openai.com/blog/2026/01/22/yi_large_next_mbpp_core",
              "published": "2026-01-22",
              "snippet": "On the MBPP Core benchmark, Yi-Large Next scored 82%..."
            },
            {
              "docId": "2026-01-24_openai_blog_5400",
              "url": "https://openai.com/blog/2026/01/24/yi_large_next_mbpp_core",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Yi-Large Next reaching 96% on MBPP Core..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_4398",
              "url": "https://blog.google/technology/ai/2026/01/25/yi_large_next_mbpp_core",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Yi-Large Next reaching 82% on MBPP Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_sliding_window_attention_max_uses_tech",
          "source": "model:midjourney_v7",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-12_google_ai_blog_4244",
              "url": "https://blog.google/technology/ai/2026/01/12/midjourney_v7_sliding_window_a",
              "published": "2026-01-12",
              "snippet": "Under the hood, Midjourney V7 implements Sliding Window Attention Max for improved efficiency..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_8253",
              "url": "https://huggingface.co/blog/2026/01/24/midjourney_v7_sliding_window_a",
              "published": "2026-01-24",
              "snippet": "Midjourney V7 leverages Sliding Window Attention Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_ultra_hellaswag_evaluated_on",
          "source": "model:midjourney_v7_ultra",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-05_meta_ai_blog_6342",
              "url": "https://ai.meta.com/blog/2025/12/05/midjourney_v7_ultra_hellaswag",
              "published": "2025-12-05",
              "snippet": "Midjourney V7 Ultra achieves 91% on HellaSwag, setting a new record..."
            },
            {
              "docId": "2025-12-26_ars_technica_9071",
              "url": "https://arstechnica.com/2025/12/26/midjourney_v7_ultra_hellaswag",
              "published": "2025-12-26",
              "snippet": "Midjourney V7 Ultra achieves 85% on HellaSwag, setting a new record..."
            },
            {
              "docId": "2025-12-27_anthropic_blog_8232",
              "url": "https://anthropic.com/news/2025/12/27/midjourney_v7_ultra_hellaswag",
              "published": "2025-12-27",
              "snippet": "Evaluation results show Midjourney V7 Ultra reaching 78% on HellaSwag..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_ultra_autogpt_edge_integrates_with",
          "source": "tool:litellm_ultra",
          "target": "tool:autogpt_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:palm_3_edge_roots_max_trained_on",
          "source": "model:palm_3_edge",
          "target": "dataset:roots_max",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:palm_3_v2_quantization_v2_uses_tech",
          "source": "model:palm_3_v2",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:localai_mini_claude_sonnet_4_mini_integrates_with",
          "source": "repo:localai_mini",
          "target": "model:claude_sonnet_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-19_techcrunch_2956",
              "url": "https://techcrunch.com/2026/01/19/localai_mini_claude_sonnet_4_m",
              "published": "2026-01-19",
              "snippet": "LocalAI Mini now supports Claude Sonnet 4 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_starcoder_data_lite_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:starcoder_data_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:litellm_max_transformer_architecture_plus_uses_tech",
          "source": "tool:litellm_max",
          "target": "tech:transformer_architecture_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:transformers_next_sparse_attention_mini_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:sparse_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__5676",
              "url": "https://technologyreview.com/2026/01/25/transformers_next_sparse_atten",
              "published": "2026-01-25",
              "snippet": "Technical details reveal transformers Next relies heavily on Sparse Attention Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_kv_cache_optimization_v2_uses_tech",
          "source": "repo:langchain_plus",
          "target": "tech:kv_cache_optimization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:arc_agi_lite_command_r_plus_measures",
          "source": "benchmark:arc_agi_lite",
          "target": "model:command_r_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:ollama_next_tool_use_max_uses_tech",
          "source": "tool:ollama_next",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-21_openai_blog_7681",
              "url": "https://openai.com/blog/2026/01/21/ollama_next_tool_use_max",
              "published": "2026-01-21",
              "snippet": "Ollama Next leverages Tool Use Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_qwen_3_core_integrates_with",
          "source": "repo:text_generation_webui_pro",
          "target": "model:qwen_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-28_techcrunch_8004",
              "url": "https://techcrunch.com/2025/12/28/text_generation_webui_pro_qwen",
              "published": "2025-12-28",
              "snippet": "The latest release of text-generation-webui Pro adds native Qwen-3 Core integration..."
            },
            {
              "docId": "2026-01-17_the_verge_8287",
              "url": "https://theverge.com/2026/01/17/text_generation_webui_pro_qwen",
              "published": "2026-01-17",
              "snippet": "text-generation-webui Pro announced official support for Qwen-3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_plus_nemotron_5_v2_measures",
          "source": "benchmark:mmlu_plus",
          "target": "model:nemotron_5_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-22_arxiv_4843",
              "url": "https://arxiv.org/abs/2026/01/22/mmlu_plus_nemotron_5_v2",
              "published": "2026-01-22",
              "snippet": "MMLU Plus has become the standard for evaluating Nemotron-5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_gpqa_max_evaluated_on",
          "source": "model:dall_e_4_core",
          "target": "benchmark:gpqa_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.96
        }
      },
      {
        "data": {
          "id": "e:c4_edge_sliding_window_attention_ultra_uses_tech",
          "source": "dataset:c4_edge",
          "target": "tech:sliding_window_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:the_pile_sparse_attention_next_uses_tech",
          "source": "dataset:the_pile",
          "target": "tech:sparse_attention_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_mixtral_8x22b_evaluated_on",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "model:mixtral_8x22b",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-16_wired_6649",
              "url": "https://wired.com/2026/01/16/mixture_of_experts_meets_instr",
              "published": "2026-01-16",
              "snippet": "Mixture of Experts Meets Instruction Tuning achieves 99% on Mixtral 8x22B, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_mixture_of_experts_edge_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:mixture_of_experts_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-11_google_ai_blog_6966",
              "url": "https://blog.google/technology/ai/2026/01/11/direct_preference_optimization",
              "published": "2026-01-11",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Mixture of Experts Edge..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_3074",
              "url": "https://huggingface.co/blog/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "Under the hood, Direct Preference Optimization implements Mixture of Experts Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_3777",
              "url": "https://wandb.ai/articles/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Mixture of Experts Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_mini_gpt_4o_mini_2_lite_depends_on",
          "source": "model:phi_4_mini",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:autogpt_edge_multimodal_fusion_v2_uses_tech",
          "source": "tool:autogpt_edge",
          "target": "tech:multimodal_fusion_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-20_weights_and_bia_6890",
              "url": "https://wandb.ai/articles/2026/01/20/autogpt_edge_multimodal_fusion",
              "published": "2026-01-20",
              "snippet": "AutoGPT Edge leverages Multimodal Fusion v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_constitutional_ai_plus_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-22_microsoft_resea_8601",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/22/transformers_next_constitution",
              "published": "2026-01-22",
              "snippet": "Under the hood, transformers Next implements Constitutional AI Plus for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_falcon_3_next_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:falcon_3_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:mbpp_v2_midjourney_v7_ultra_measures",
          "source": "benchmark:mbpp_v2",
          "target": "model:midjourney_v7_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-07_meta_ai_blog_2423",
              "url": "https://ai.meta.com/blog/2026/01/07/mbpp_v2_midjourney_v7_ultra",
              "published": "2026-01-07",
              "snippet": "MBPP v2 has become the standard for evaluating Midjourney V7 Ultra..."
            },
            {
              "docId": "2026-01-13_weights_and_bia_3830",
              "url": "https://wandb.ai/articles/2026/01/13/mbpp_v2_midjourney_v7_ultra",
              "published": "2026-01-13",
              "snippet": "MBPP v2 provides standardized evaluation of Midjourney V7 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_openwebtext2_core_trained_on",
          "source": "model:yi_large_next",
          "target": "dataset:openwebtext2_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.53,
          "evidence": [
            {
              "docId": "2026-01-20_ars_technica_5400",
              "url": "https://arstechnica.com/2026/01/20/yi_large_next_openwebtext2_cor",
              "published": "2026-01-20",
              "snippet": "The training corpus for Yi-Large Next includes OpenWebText2 Core..."
            },
            {
              "docId": "2026-01-23_openai_blog_4881",
              "url": "https://openai.com/blog/2026/01/23/yi_large_next_openwebtext2_cor",
              "published": "2026-01-23",
              "snippet": "The training corpus for Yi-Large Next includes OpenWebText2 Core..."
            },
            {
              "docId": "2026-01-23_google_ai_blog_7926",
              "url": "https://blog.google/technology/ai/2026/01/23/yi_large_next_openwebtext2_cor",
              "published": "2026-01-23",
              "snippet": "The training corpus for Yi-Large Next includes OpenWebText2 Core..."
            },
            {
              "docId": "2026-01-23_langchain_blog_1405",
              "url": "https://blog.langchain.dev/2026/01/23/yi_large_next_openwebtext2_cor",
              "published": "2026-01-23",
              "snippet": "Yi-Large Next was trained on OpenWebText2 Core comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_ultra_kv_cache_optimization_v2_uses_tech",
          "source": "model:mixtral_8x22b_ultra",
          "target": "tech:kv_cache_optimization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:mmlu_next_mixtral_8x22b_core_measures",
          "source": "benchmark:mmlu_next",
          "target": "model:mixtral_8x22b_core",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_synthetic_data_generation_pro_uses_tech",
          "source": "repo:langchain_edge",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-22_nextgov_7275",
              "url": "https://nextgov.com/2026/01/22/langchain_edge_synthetic_data_",
              "published": "2026-01-22",
              "snippet": "Technical details reveal langchain Edge relies heavily on Synthetic Data Generation Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_flash_attention_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_gguf_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-14_nvidia_blog_4484",
              "url": "https://blogs.nvidia.com/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "Under the hood, Direct Preference Optimization v2 implements GGUF for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_v2_llama_4_next_integrates_with",
          "source": "tool:litellm_v2",
          "target": "model:llama_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-10_hugging_face_bl_2266",
              "url": "https://huggingface.co/blog/2026/01/10/litellm_v2_llama_4_next",
              "published": "2026-01-10",
              "snippet": "The latest release of LiteLLM v2 adds native Llama 4 Next integration..."
            },
            {
              "docId": "2026-01-25_venturebeat_8581",
              "url": "https://venturebeat.com/2026/01/25/litellm_v2_llama_4_next",
              "published": "2026-01-25",
              "snippet": "The latest release of LiteLLM v2 adds native Llama 4 Next integration..."
            },
            {
              "docId": "2026-01-25_arxiv_6582",
              "url": "https://arxiv.org/abs/2026/01/25/litellm_v2_llama_4_next",
              "published": "2026-01-25",
              "snippet": "LiteLLM v2 now supports Llama 4 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_midjourney_v7_v2_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:midjourney_v7_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:arc_agi_core_yi_large_max_measures",
          "source": "benchmark:arc_agi_core",
          "target": "model:yi_large_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-15_weights_and_bia_6429",
              "url": "https://wandb.ai/articles/2026/01/15/arc_agi_core_yi_large_max",
              "published": "2026-01-15",
              "snippet": "The ARC-AGI Core benchmark measures Yi-Large Max across multiple tasks..."
            },
            {
              "docId": "2026-01-20_venturebeat_1882",
              "url": "https://venturebeat.com/2026/01/20/arc_agi_core_yi_large_max",
              "published": "2026-01-20",
              "snippet": "ARC-AGI Core has become the standard for evaluating Yi-Large Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_weights_and_biases_core_integrates_with",
          "source": "tool:crewai",
          "target": "tool:weights_and_biases_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-12_the_verge_5973",
              "url": "https://theverge.com/2026/01/12/crewai_weights_and_biases_core",
              "published": "2026-01-12",
              "snippet": "The latest release of CrewAI adds native Weights & Biases Core integration..."
            },
            {
              "docId": "2026-01-19_arxiv_2449",
              "url": "https://arxiv.org/abs/2026/01/19/crewai_weights_and_biases_core",
              "published": "2026-01-19",
              "snippet": "CrewAI now supports Weights & Biases Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_next_gemini_ultra_2_v2_measures",
          "source": "benchmark:math_next",
          "target": "model:gemini_ultra_2_v2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_alpacaeval_2_max_evaluated_on",
          "source": "model:gemma_3_mini",
          "target": "benchmark:alpacaeval_2_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_5686",
              "url": "https://huggingface.co/blog/2026/01/25/gemma_3_mini_alpacaeval_2_max",
              "published": "2026-01-25",
              "snippet": "Gemma 3 Mini achieves 81% on AlpacaEval 2 Max, setting a new record..."
            },
            {
              "docId": "2026-01-25_techcrunch_7076",
              "url": "https://techcrunch.com/2026/01/25/gemma_3_mini_alpacaeval_2_max",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Gemma 3 Mini reaching 90% on AlpacaEval 2 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_max_rotary_position_embedding_uses_tech",
          "source": "dataset:openwebtext2_max",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:roots_edge_dpo_mini_uses_tech",
          "source": "dataset:roots_edge",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:jamba_2_v2_codex_2_v2_depends_on",
          "source": "model:jamba_2_v2",
          "target": "model:codex_2_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_rlhf_mini_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:cody_mini_gradio_edge_integrates_with",
          "source": "tool:cody_mini",
          "target": "tool:gradio_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:laion_5b_core_tool_use_lite_uses_tech",
          "source": "dataset:laion_5b_core",
          "target": "tech:tool_use_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:c4_edge_multimodal_fusion_lite_uses_tech",
          "source": "dataset:c4_edge",
          "target": "tech:multimodal_fusion_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_pro_flash_attention_mini_uses_tech",
          "source": "model:whisper_v4_pro",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_sliding_window_attention_next_uses_tech",
          "source": "repo:transformers_edge",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:litellm_ultra_phi_4_max_integrates_with",
          "source": "tool:litellm_ultra",
          "target": "model:phi_4_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-17_reuters_8723",
              "url": "https://reuters.com/technology/2026/01/17/litellm_ultra_phi_4_max",
              "published": "2026-01-17",
              "snippet": "The latest release of LiteLLM Ultra adds native Phi-4 Max integration..."
            },
            {
              "docId": "2026-01-21_the_gradient_5228",
              "url": "https://thegradient.pub/2026/01/21/litellm_ultra_phi_4_max",
              "published": "2026-01-21",
              "snippet": "LiteLLM Ultra now supports Phi-4 Max with full feature parity..."
            },
            {
              "docId": "2026-01-21_langchain_blog_1388",
              "url": "https://blog.langchain.dev/2026/01/21/litellm_ultra_phi_4_max",
              "published": "2026-01-21",
              "snippet": "The latest release of LiteLLM Ultra adds native Phi-4 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_tokenizer_bpe_lite_uses_tech",
          "source": "tool:tensorrt_llm",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:arc_agi_ultra_gemma_3_max_measures",
          "source": "benchmark:arc_agi_ultra",
          "target": "model:gemma_3_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-12_venturebeat_7611",
              "url": "https://venturebeat.com/2026/01/12/arc_agi_ultra_gemma_3_max",
              "published": "2026-01-12",
              "snippet": "The ARC-AGI Ultra benchmark measures Gemma 3 Max across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_edge_qwen_3_max_measures",
          "source": "benchmark:hellaswag_edge",
          "target": "model:qwen_3_max",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.98
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_sparse_attention_ultra_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:sparse_attention_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-17_google_ai_blog_2115",
              "url": "https://blog.google/technology/ai/2025/12/17/ollama_v2_sparse_attention_ult",
              "published": "2025-12-17",
              "snippet": "Technical details reveal ollama v2 relies heavily on Sparse Attention Ultra..."
            },
            {
              "docId": "2026-01-09_mit_technology__8089",
              "url": "https://technologyreview.com/2026/01/09/ollama_v2_sparse_attention_ult",
              "published": "2026-01-09",
              "snippet": "Under the hood, ollama v2 implements Sparse Attention Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_max_mixtral_8x22b_core_measures",
          "source": "benchmark:bigbench_hard_max",
          "target": "model:mixtral_8x22b_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-18_google_ai_blog_7357",
              "url": "https://blog.google/technology/ai/2026/01/18/bigbench_hard_max_mixtral_8x22",
              "published": "2026-01-18",
              "snippet": "The BigBench Hard Max benchmark measures Mixtral 8x22B Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_humaneval_mini_evaluated_on",
          "source": "model:mixtral_8x22b_plus",
          "target": "benchmark:humaneval_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-20_bloomberg_6931",
              "url": "https://bloomberg.com/technology/2026/01/20/mixtral_8x22b_plus_humaneval_m",
              "published": "2026-01-20",
              "snippet": "Mixtral 8x22B Plus achieves 79% on HumanEval Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_ultra_retrieval_augmented_generation_uses_tech",
          "source": "model:deepseek_v3_ultra",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_openai_blog_3594",
              "url": "https://openai.com/blog/2026/01/22/deepseek_v3_ultra_retrieval_au",
              "published": "2026-01-22",
              "snippet": "DeepSeek-V3 Ultra leverages Retrieval-Augmented Generation to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_distillation_mini_uses_tech",
          "source": "tool:gradio_plus",
          "target": "tech:distillation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:haystack_edge_lora_v2_uses_tech",
          "source": "tool:haystack_edge",
          "target": "tech:lora_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-14_the_verge_5561",
              "url": "https://theverge.com/2026/01/14/haystack_edge_lora_v2",
              "published": "2026-01-14",
              "snippet": "Technical details reveal Haystack Edge relies heavily on LoRA v2..."
            },
            {
              "docId": "2026-01-22_anthropic_blog_9285",
              "url": "https://anthropic.com/news/2026/01/22/haystack_edge_lora_v2",
              "published": "2026-01-22",
              "snippet": "Under the hood, Haystack Edge implements LoRA v2 for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_synthetic_data_generation_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_tree_of_thought_lite_uses_tech",
          "source": "model:stable_diffusion_4_max",
          "target": "tech:tree_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-13_the_gradient_1528",
              "url": "https://thegradient.pub/2026/01/13/stable_diffusion_4_max_tree_of",
              "published": "2026-01-13",
              "snippet": "Under the hood, Stable Diffusion 4 Max implements Tree of Thought Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_transformer_architecture_plus_uses_tech",
          "source": "dataset:wikipedia_dump_2025",
          "target": "tech:transformer_architecture_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_sliding_window_attention_mini_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:sliding_window_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_5239",
              "url": "https://blog.langchain.dev/2026/01/17/direct_preference_optimization",
              "published": "2026-01-17",
              "snippet": "Under the hood, Direct Preference Optimization implements Sliding Window Attention Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-23_mit_technology__6680",
              "url": "https://technologyreview.com/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Under the hood, Direct Preference Optimization implements Sliding Window Attention Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_3759",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Sliding Window Attention Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_rlhf_ultra_uses_tech",
          "source": "tool:langchain_edge",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:ollama_red_teaming_lite_uses_tech",
          "source": "repo:ollama",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_claude_opus_45_next_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:claude_opus_45_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-08_meta_ai_blog_4676",
              "url": "https://ai.meta.com/blog/2026/01/08/vllm_pro_claude_opus_45_next",
              "published": "2026-01-08",
              "snippet": "The latest release of vllm Pro adds native Claude Opus 4.5 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_speculative_decoding_max_uses_tech",
          "source": "repo:gpt4all_v2",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-07_meta_ai_blog_5557",
              "url": "https://ai.meta.com/blog/2026/01/07/gpt4all_v2_speculative_decodin",
              "published": "2026-01-07",
              "snippet": "gpt4all v2 leverages Speculative Decoding Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_rotary_position_embedding_lite_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:rotary_position_embedding_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:mmlu_mini_deepseek_v3_v2_measures",
          "source": "benchmark:mmlu_mini",
          "target": "model:deepseek_v3_v2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_lora_max_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-15_the_gradient_8935",
              "url": "https://thegradient.pub/2026/01/15/mixture_of_experts_meets_instr",
              "published": "2026-01-15",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements LoRA Max for improved efficiency..."
            },
            {
              "docId": "2026-01-19_anthropic_blog_2194",
              "url": "https://anthropic.com/news/2026/01/19/mixture_of_experts_meets_instr",
              "published": "2026-01-19",
              "snippet": "Under the hood, Mixture of Experts Meets Instruction Tuning implements LoRA Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_next_tensorrt_llm_lite_integrates_with",
          "source": "tool:vllm_next",
          "target": "tool:tensorrt_llm_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:swe_bench_max_command_r_plus_edge_measures",
          "source": "benchmark:swe_bench_max",
          "target": "model:command_r_plus_edge",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:palm_3_mixtral_8x22b_depends_on",
          "source": "model:palm_3",
          "target": "model:mixtral_8x22b",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_mt_bench_plus_evaluated_on",
          "source": "model:claude_opus_45_lite",
          "target": "benchmark:mt_bench_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-06_nextgov_9695",
              "url": "https://nextgov.com/2026/01/06/claude_opus_45_lite_mt_bench_p",
              "published": "2026-01-06",
              "snippet": "On the MT-Bench Plus benchmark, Claude Opus 4.5 Lite scored 82%..."
            },
            {
              "docId": "2026-01-17_mit_technology__7291",
              "url": "https://technologyreview.com/2026/01/17/claude_opus_45_lite_mt_bench_p",
              "published": "2026-01-17",
              "snippet": "Claude Opus 4.5 Lite achieves 82% on MT-Bench Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_quantization_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_hellaswag_edge_evaluated_on",
          "source": "model:jamba_2_core",
          "target": "benchmark:hellaswag_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_8463",
              "url": "https://wandb.ai/articles/2026/01/25/jamba_2_core_hellaswag_edge",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Jamba 2 Core reaching 74% on HellaSwag Edge..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_8100",
              "url": "https://anthropic.com/news/2026/01/25/jamba_2_core_hellaswag_edge",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag Edge benchmark, Jamba 2 Core scored 80%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_lite_truthfulqa_edge_evaluated_on",
          "source": "model:gpt_5_lite",
          "target": "benchmark:truthfulqa_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-27_microsoft_resea_9226",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/27/gpt_5_lite_truthfulqa_edge",
              "published": "2025-12-27",
              "snippet": "On the TruthfulQA Edge benchmark, GPT-5 Lite scored 75%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_v2_llama_4_ultra_integrates_with",
          "source": "tool:llamaindex_v2",
          "target": "model:llama_4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_8915",
              "url": "https://anthropic.com/news/2026/01/25/llamaindex_v2_llama_4_ultra",
              "published": "2026-01-25",
              "snippet": "LlamaIndex v2 announced official support for Llama 4 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_distillation_core_uses_tech",
          "source": "repo:langchain_plus",
          "target": "tech:distillation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-24_microsoft_resea_6867",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/langchain_plus_distillation_co",
              "published": "2026-01-24",
              "snippet": "Technical details reveal langchain Plus relies heavily on Distillation Core..."
            },
            {
              "docId": "2026-01-24_reuters_8477",
              "url": "https://reuters.com/technology/2026/01/24/langchain_plus_distillation_co",
              "published": "2026-01-24",
              "snippet": "langchain Plus leverages Distillation Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_claude_sonnet_4_pro_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:claude_sonnet_4_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-14_meta_ai_blog_4728",
              "url": "https://ai.meta.com/blog/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "On the Claude Sonnet 4 Pro benchmark, Direct Preference Optimization scored 78%..."
            },
            {
              "docId": "2026-01-22_bloomberg_4553",
              "url": "https://bloomberg.com/technology/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Direct Preference Optimization reaching 93% on Claude Sonnet 4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_pro_whisper_v4_ultra_integrates_with",
          "source": "repo:llamacpp_pro",
          "target": "model:whisper_v4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-20_google_ai_blog_8109",
              "url": "https://blog.google/technology/ai/2026/01/20/llamacpp_pro_whisper_v4_ultra",
              "published": "2026-01-20",
              "snippet": "llama.cpp Pro announced official support for Whisper v4 Ultra..."
            },
            {
              "docId": "2026-01-25_nextgov_5176",
              "url": "https://nextgov.com/2026/01/25/llamacpp_pro_whisper_v4_ultra",
              "published": "2026-01-25",
              "snippet": "llama.cpp Pro announced official support for Whisper v4 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_pro_grok_3_max_measures",
          "source": "benchmark:winogrande_pro",
          "target": "model:grok_3_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_wired_6049",
              "url": "https://wired.com/2026/01/25/winogrande_pro_grok_3_max",
              "published": "2026-01-25",
              "snippet": "The WinoGrande Pro benchmark measures Grok-3 Max across multiple tasks..."
            },
            {
              "docId": "2026-01-25_the_gradient_1154",
              "url": "https://thegradient.pub/2026/01/25/winogrande_pro_grok_3_max",
              "published": "2026-01-25",
              "snippet": "The WinoGrande Pro benchmark measures Grok-3 Max across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_gguf_uses_tech",
          "source": "repo:llamacpp",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-02_meta_ai_blog_5583",
              "url": "https://ai.meta.com/blog/2026/01/02/llamacpp_gguf",
              "published": "2026-01-02",
              "snippet": "Technical details reveal llama.cpp relies heavily on GGUF..."
            },
            {
              "docId": "2026-01-19_bloomberg_5075",
              "url": "https://bloomberg.com/technology/2026/01/19/llamacpp_gguf",
              "published": "2026-01-19",
              "snippet": "Technical details reveal llama.cpp relies heavily on GGUF..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_gemma_3_max_depends_on",
          "source": "model:claude_opus_45_pro",
          "target": "model:gemma_3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_rlhf_core_uses_tech",
          "source": "repo:vllm_edge",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:yi_large_lite_slimpajama_core_trained_on",
          "source": "model:yi_large_lite",
          "target": "dataset:slimpajama_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_sora_2_pro_integrates_with",
          "source": "repo:langchain_ultra",
          "target": "model:sora_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-05_bloomberg_9028",
              "url": "https://bloomberg.com/technology/2026/01/05/langchain_ultra_sora_2_pro",
              "published": "2026-01-05",
              "snippet": "langchain Ultra announced official support for Sora 2 Pro..."
            },
            {
              "docId": "2026-01-21_nvidia_blog_5848",
              "url": "https://blogs.nvidia.com/2026/01/21/langchain_ultra_sora_2_pro",
              "published": "2026-01-21",
              "snippet": "langchain Ultra now supports Sora 2 Pro with full feature parity..."
            },
            {
              "docId": "2026-01-22_langchain_blog_7828",
              "url": "https://blog.langchain.dev/2026/01/22/langchain_ultra_sora_2_pro",
              "published": "2026-01-22",
              "snippet": "langchain Ultra announced official support for Sora 2 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_llama_4_next_evaluated_on",
          "source": "paper:direct_preference_optimization_edge",
          "target": "model:llama_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-22_openai_blog_2395",
              "url": "https://openai.com/blog/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Direct Preference Optimization Edge reaching 95% on Llama 4 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_kv_cache_optimization_uses_tech",
          "source": "repo:open_interpreter_core",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-24_openai_blog_3367",
              "url": "https://openai.com/blog/2026/01/24/open_interpreter_core_kv_cache",
              "published": "2026-01-24",
              "snippet": "Under the hood, open-interpreter Core implements KV Cache Optimization for improved efficiency..."
            },
            {
              "docId": "2026-01-24_the_gradient_6581",
              "url": "https://thegradient.pub/2026/01/24/open_interpreter_core_kv_cache",
              "published": "2026-01-24",
              "snippet": "Under the hood, open-interpreter Core implements KV Cache Optimization for improved efficiency..."
            },
            {
              "docId": "2026-01-24_microsoft_resea_3872",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/open_interpreter_core_kv_cache",
              "published": "2026-01-24",
              "snippet": "Under the hood, open-interpreter Core implements KV Cache Optimization for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_edge_palm_3_next_integrates_with",
          "source": "tool:autogpt_edge",
          "target": "model:palm_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_7359",
              "url": "https://blog.google/technology/ai/2026/01/22/autogpt_edge_palm_3_next",
              "published": "2026-01-22",
              "snippet": "AutoGPT Edge now supports PaLM 3 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_whisper_v4_lite_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:whisper_v4_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-23_microsoft_resea_7147",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Direct Preference Optimization Pro achieves 98% on Whisper v4 Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_whisper_v4_core_integrates_with",
          "source": "tool:mlflow",
          "target": "model:whisper_v4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:codex_2_gpt_4o_mini_2_v2_depends_on",
          "source": "model:codex_2",
          "target": "model:gpt_4o_mini_2_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:litellm_max_whisper_v4_ultra_integrates_with",
          "source": "tool:litellm_max",
          "target": "model:whisper_v4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-25_the_verge_6826",
              "url": "https://theverge.com/2025/12/25/litellm_max_whisper_v4_ultra",
              "published": "2025-12-25",
              "snippet": "The latest release of LiteLLM Max adds native Whisper v4 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_edge_sparse_attention_pro_uses_tech",
          "source": "model:claude_opus_45_edge",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-11-26_anthropic_blog_6348",
              "url": "https://anthropic.com/news/2025/11/26/claude_opus_45_edge_sparse_att",
              "published": "2025-11-26",
              "snippet": "Claude Opus 4.5 Edge leverages Sparse Attention Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-11-28_bloomberg_9207",
              "url": "https://bloomberg.com/technology/2025/11/28/claude_opus_45_edge_sparse_att",
              "published": "2025-11-28",
              "snippet": "Under the hood, Claude Opus 4.5 Edge implements Sparse Attention Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_wikipedia_dump_2025_pro_trained_on",
          "source": "model:grok_3_plus",
          "target": "dataset:wikipedia_dump_2025_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_v2_laion_5b_next_trained_on",
          "source": "model:command_r_plus_v2",
          "target": "dataset:laion_5b_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2026-01-03_the_gradient_4416",
              "url": "https://thegradient.pub/2026/01/03/command_r_plus_v2_laion_5b_nex",
              "published": "2026-01-03",
              "snippet": "Command R+ v2 was trained on LAION-5B Next comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_dall_e_4_edge_depends_on",
          "source": "model:claude_opus_45_pro",
          "target": "model:dall_e_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_group_query_attention_core_uses_tech",
          "source": "repo:autogpt_plus",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-19_techcrunch_9079",
              "url": "https://techcrunch.com/2026/01/19/autogpt_plus_group_query_atten",
              "published": "2026-01-19",
              "snippet": "Technical details reveal AutoGPT Plus relies heavily on Group Query Attention Core..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_8921",
              "url": "https://blogs.nvidia.com/2026/01/19/autogpt_plus_group_query_atten",
              "published": "2026-01-19",
              "snippet": "AutoGPT Plus leverages Group Query Attention Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-21_hugging_face_bl_1171",
              "url": "https://huggingface.co/blog/2026/01/21/autogpt_plus_group_query_atten",
              "published": "2026-01-21",
              "snippet": "Technical details reveal AutoGPT Plus relies heavily on Group Query Attention Core..."
            },
            {
              "docId": "2026-01-22_the_verge_4420",
              "url": "https://theverge.com/2026/01/22/autogpt_plus_group_query_atten",
              "published": "2026-01-22",
              "snippet": "AutoGPT Plus leverages Group Query Attention Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_plus_gguf_uses_tech",
          "source": "tool:llamaindex_plus",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_flash_attention_uses_tech",
          "source": "repo:ollama_ultra",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-14_arxiv_9822",
              "url": "https://arxiv.org/abs/2026/01/14/ollama_ultra_flash_attention",
              "published": "2026-01-14",
              "snippet": "Technical details reveal ollama Ultra relies heavily on Flash Attention..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_max_chain_of_thought_plus_uses_tech",
          "source": "dataset:c4_max",
          "target": "tech:chain_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_edge_cody_edge_integrates_with",
          "source": "tool:weights_and_biases_edge",
          "target": "tool:cody_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:arc_agi_phi_4_pro_measures",
          "source": "benchmark:arc_agi",
          "target": "model:phi_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-11_techcrunch_4699",
              "url": "https://techcrunch.com/2026/01/11/arc_agi_phi_4_pro",
              "published": "2026-01-11",
              "snippet": "ARC-AGI provides standardized evaluation of Phi-4 Pro..."
            },
            {
              "docId": "2026-01-23_hugging_face_bl_2572",
              "url": "https://huggingface.co/blog/2026/01/23/arc_agi_phi_4_pro",
              "published": "2026-01-23",
              "snippet": "ARC-AGI has become the standard for evaluating Phi-4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_ultra_redpajama_v2_plus_trained_on",
          "source": "model:phi_4_ultra",
          "target": "dataset:redpajama_v2_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-13_techcrunch_6354",
              "url": "https://techcrunch.com/2025/12/13/phi_4_ultra_redpajama_v2_plus",
              "published": "2025-12-13",
              "snippet": "Phi-4 Ultra utilized RedPajama v2 Plus as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_mmlu_plus_evaluated_on",
          "source": "model:nemotron_5_next",
          "target": "benchmark:mmlu_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_lite_whisper_v4_mini_depends_on",
          "source": "model:stable_diffusion_4_lite",
          "target": "model:whisper_v4_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_retrieval_augmented_generation_lite_uses_tech",
          "source": "repo:autogpt_v2",
          "target": "tech:retrieval_augmented_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-22_the_verge_5444",
              "url": "https://theverge.com/2026/01/22/autogpt_v2_retrieval_augmented",
              "published": "2026-01-22",
              "snippet": "Under the hood, AutoGPT v2 implements Retrieval-Augmented Generation Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_gemini_ultra_2_core_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "model:gemini_ultra_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_5790",
              "url": "https://nextgov.com/2026/01/24/textbooks_are_all_you_need_nex",
              "published": "2026-01-24",
              "snippet": "Textbooks Are All You Need Next achieves 96% on Gemini Ultra 2 Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_pro_bigbench_hard_core_evaluated_on",
          "source": "model:nemotron_5_pro",
          "target": "benchmark:bigbench_hard_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-13_arxiv_8522",
              "url": "https://arxiv.org/abs/2026/01/13/nemotron_5_pro_bigbench_hard_c",
              "published": "2026-01-13",
              "snippet": "On the BigBench Hard Core benchmark, Nemotron-5 Pro scored 90%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_speculative_decoding_core_uses_tech",
          "source": "repo:ollama_core",
          "target": "tech:speculative_decoding_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-14_bloomberg_5586",
              "url": "https://bloomberg.com/technology/2026/01/14/ollama_core_speculative_decodi",
              "published": "2026-01-14",
              "snippet": "ollama Core leverages Speculative Decoding Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-20_google_ai_blog_2833",
              "url": "https://blog.google/technology/ai/2026/01/20/ollama_core_speculative_decodi",
              "published": "2026-01-20",
              "snippet": "Technical details reveal ollama Core relies heavily on Speculative Decoding Core..."
            },
            {
              "docId": "2026-01-20_nextgov_3427",
              "url": "https://nextgov.com/2026/01/20/ollama_core_speculative_decodi",
              "published": "2026-01-20",
              "snippet": "ollama Core leverages Speculative Decoding Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_bloomberg_6906",
              "url": "https://bloomberg.com/technology/2026/01/24/ollama_core_speculative_decodi",
              "published": "2026-01-24",
              "snippet": "ollama Core leverages Speculative Decoding Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_plus_gemma_3_integrates_with",
          "source": "tool:tensorrt_llm_plus",
          "target": "model:gemma_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-03_bloomberg_1430",
              "url": "https://bloomberg.com/technology/2026/01/03/tensorrt_llm_plus_gemma_3",
              "published": "2026-01-03",
              "snippet": "TensorRT-LLM Plus now supports Gemma 3 with full feature parity..."
            },
            {
              "docId": "2026-01-12_google_ai_blog_6743",
              "url": "https://blog.google/technology/ai/2026/01/12/tensorrt_llm_plus_gemma_3",
              "published": "2026-01-12",
              "snippet": "TensorRT-LLM Plus now supports Gemma 3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_quantization_ultra_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:quantization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:jamba_2_ultra_the_stack_v2_pro_trained_on",
          "source": "model:jamba_2_ultra",
          "target": "dataset:the_stack_v2_pro",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.55,
          "evidence": [
            {
              "docId": "2026-01-13_the_verge_3673",
              "url": "https://theverge.com/2026/01/13/jamba_2_ultra_the_stack_v2_pro",
              "published": "2026-01-13",
              "snippet": "Jamba 2 Ultra was trained on The Stack v2 Pro comprising billions of tokens..."
            },
            {
              "docId": "2026-01-18_ars_technica_5825",
              "url": "https://arstechnica.com/2026/01/18/jamba_2_ultra_the_stack_v2_pro",
              "published": "2026-01-18",
              "snippet": "Jamba 2 Ultra was trained on The Stack v2 Pro comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_qlora_mini_uses_tech",
          "source": "model:claude_opus_45_pro",
          "target": "tech:qlora_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_midjourney_v7_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:midjourney_v7",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_5848",
              "url": "https://reuters.com/technology/2026/01/24/chain_of_thought_prompting_eli",
              "published": "2026-01-24",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning achieves 87% on Midjourney V7, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_sora_2_pro_depends_on",
          "source": "model:gemma_3_mini",
          "target": "model:sora_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_quantization_max_uses_tech",
          "source": "paper:llm_agents:_a_survey",
          "target": "tech:quantization_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_plus_palm_3_core_depends_on",
          "source": "model:claude_sonnet_4_plus",
          "target": "model:palm_3_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:c4_ultra_lora_v2_uses_tech",
          "source": "dataset:c4_ultra",
          "target": "tech:lora_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_gemini_ultra_2_depends_on",
          "source": "model:claude_opus_45_core",
          "target": "model:gemini_ultra_2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_whisper_v4_ultra_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:whisper_v4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-08_bloomberg_9707",
              "url": "https://bloomberg.com/technology/2026/01/08/toolformer:_language_models_ca",
              "published": "2026-01-08",
              "snippet": "Evaluation results show Toolformer: Language Models Can Teach Themselves to Use Tools reaching 79% on Whisper v4 Ultra..."
            },
            {
              "docId": "2026-01-09_hugging_face_bl_9903",
              "url": "https://huggingface.co/blog/2026/01/09/toolformer:_language_models_ca",
              "published": "2026-01-09",
              "snippet": "Toolformer: Language Models Can Teach Themselves to Use Tools achieves 86% on Whisper v4 Ultra, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_chain_of_thought_edge_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:chain_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-29_hugging_face_bl_9859",
              "url": "https://huggingface.co/blog/2025/12/29/direct_preference_optimization",
              "published": "2025-12-29",
              "snippet": "Direct Preference Optimization v2 leverages Chain-of-Thought Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-03_anthropic_blog_5952",
              "url": "https://anthropic.com/news/2026/01/03/direct_preference_optimization",
              "published": "2026-01-03",
              "snippet": "Under the hood, Direct Preference Optimization v2 implements Chain-of-Thought Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-11_venturebeat_3304",
              "url": "https://venturebeat.com/2026/01/11/direct_preference_optimization",
              "published": "2026-01-11",
              "snippet": "Direct Preference Optimization v2 leverages Chain-of-Thought Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_constitutional_ai_max_uses_tech",
          "source": "tool:ollama_edge",
          "target": "tech:constitutional_ai_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-12_bloomberg_9287",
              "url": "https://bloomberg.com/technology/2025/12/12/ollama_edge_constitutional_ai_",
              "published": "2025-12-12",
              "snippet": "Ollama Edge leverages Constitutional AI Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-09_openai_blog_4131",
              "url": "https://openai.com/blog/2026/01/09/ollama_edge_constitutional_ai_",
              "published": "2026-01-09",
              "snippet": "Under the hood, Ollama Edge implements Constitutional AI Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_max_whisper_v4_pro_depends_on",
          "source": "model:midjourney_v7_max",
          "target": "model:whisper_v4_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:phi_4_ultra_slimpajama_plus_trained_on",
          "source": "model:phi_4_ultra",
          "target": "dataset:slimpajama_plus",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:slimpajama_tree_of_thought_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:langchain_midjourney_v7_integrates_with",
          "source": "repo:langchain",
          "target": "model:midjourney_v7",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-24_google_ai_blog_4157",
              "url": "https://blog.google/technology/ai/2025/12/24/langchain_midjourney_v7",
              "published": "2025-12-24",
              "snippet": "langchain announced official support for Midjourney V7..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_edge_synthetic_data_generation_edge_uses_tech",
          "source": "model:command_r_plus_edge",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:fineweb_pro_distillation_mini_uses_tech",
          "source": "dataset:fineweb_pro",
          "target": "tech:distillation_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:ollama_mixture_of_experts_plus_uses_tech",
          "source": "repo:ollama",
          "target": "tech:mixture_of_experts_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-02_google_ai_blog_9329",
              "url": "https://blog.google/technology/ai/2026/01/02/ollama_mixture_of_experts_plus",
              "published": "2026-01-02",
              "snippet": "ollama leverages Mixture of Experts Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_dall_e_4_core_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:dall_e_4_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:codex_2_pro_gpqa_plus_evaluated_on",
          "source": "model:codex_2_pro",
          "target": "benchmark:gpqa_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:localai_v2_tree_of_thought_lite_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:tree_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_mini_codex_2_mini_measures",
          "source": "benchmark:alpacaeval_2_mini",
          "target": "model:codex_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-19_nvidia_blog_8016",
              "url": "https://blogs.nvidia.com/2026/01/19/alpacaeval_2_mini_codex_2_mini",
              "published": "2026-01-19",
              "snippet": "AlpacaEval 2 Mini has become the standard for evaluating Codex 2 Mini..."
            },
            {
              "docId": "2026-01-20_weights_and_bia_9214",
              "url": "https://wandb.ai/articles/2026/01/20/alpacaeval_2_mini_codex_2_mini",
              "published": "2026-01-20",
              "snippet": "AlpacaEval 2 Mini provides standardized evaluation of Codex 2 Mini..."
            },
            {
              "docId": "2026-01-24_techcrunch_4304",
              "url": "https://techcrunch.com/2026/01/24/alpacaeval_2_mini_codex_2_mini",
              "published": "2026-01-24",
              "snippet": "AlpacaEval 2 Mini has become the standard for evaluating Codex 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_tool_use_uses_tech",
          "source": "tool:ollama_v2",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-16_venturebeat_2685",
              "url": "https://venturebeat.com/2026/01/16/ollama_v2_tool_use",
              "published": "2026-01-16",
              "snippet": "Technical details reveal Ollama v2 relies heavily on Tool Use..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_chain_of_thought_ultra_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_kv_cache_optimization_pro_uses_tech",
          "source": "model:grok_3_next",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_nemotron_5_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:nemotron_5",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_pro_retrieval_augmented_generation_ultra_uses_tech",
          "source": "model:claude_opus_45_pro",
          "target": "tech:retrieval_augmented_generation_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:vllm_dpo_max_uses_tech",
          "source": "repo:vllm",
          "target": "tech:dpo_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-17_google_ai_blog_5021",
              "url": "https://blog.google/technology/ai/2026/01/17/vllm_dpo_max",
              "published": "2026-01-17",
              "snippet": "Technical details reveal vllm relies heavily on DPO Max..."
            },
            {
              "docId": "2026-01-23_meta_ai_blog_1701",
              "url": "https://ai.meta.com/blog/2026/01/23/vllm_dpo_max",
              "published": "2026-01-23",
              "snippet": "Under the hood, vllm implements DPO Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_core_tree_of_thought_core_uses_tech",
          "source": "tool:localai_core",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:mlflow_qwen_3_core_integrates_with",
          "source": "tool:mlflow",
          "target": "model:qwen_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:aya_3_next_stable_diffusion_4_max_depends_on",
          "source": "model:aya_3_next",
          "target": "model:stable_diffusion_4_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_grok_3_integrates_with",
          "source": "repo:ollama_v2",
          "target": "model:grok_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-12_anthropic_blog_6047",
              "url": "https://anthropic.com/news/2025/12/12/ollama_v2_grok_3",
              "published": "2025-12-12",
              "snippet": "ollama v2 now supports Grok-3 with full feature parity..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_6668",
              "url": "https://anthropic.com/news/2026/01/25/ollama_v2_grok_3",
              "published": "2026-01-25",
              "snippet": "ollama v2 announced official support for Grok-3..."
            },
            {
              "docId": "2026-01-25_nextgov_1602",
              "url": "https://nextgov.com/2026/01/25/ollama_v2_grok_3",
              "published": "2026-01-25",
              "snippet": "ollama v2 now supports Grok-3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_llama_4_v2_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:llama_4_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_synthetic_data_generation_mini_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-07_meta_ai_blog_1825",
              "url": "https://ai.meta.com/blog/2026/01/07/direct_preference_optimization",
              "published": "2026-01-07",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Synthetic Data Generation Mini..."
            },
            {
              "docId": "2026-01-08_google_ai_blog_1381",
              "url": "https://blog.google/technology/ai/2026/01/08/direct_preference_optimization",
              "published": "2026-01-08",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Synthetic Data Generation Mini..."
            },
            {
              "docId": "2026-01-21_venturebeat_8586",
              "url": "https://venturebeat.com/2026/01/21/direct_preference_optimization",
              "published": "2026-01-21",
              "snippet": "Direct Preference Optimization leverages Synthetic Data Generation Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_alpacaeval_2_mini_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:alpacaeval_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_sparse_attention_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-17_reuters_1102",
              "url": "https://reuters.com/technology/2026/01/17/llamacpp_mini_sparse_attention",
              "published": "2026-01-17",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Sparse Attention..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_flash_attention_edge_uses_tech",
          "source": "model:stable_diffusion_4_max",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:slimpajama_core_tree_of_thought_v2_uses_tech",
          "source": "dataset:slimpajama_core",
          "target": "tech:tree_of_thought_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:localai_lite_rlhf_max_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:litellm_v2_haystack_plus_integrates_with",
          "source": "tool:litellm_v2",
          "target": "tool:haystack_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:transformers_tree_of_thought_next_uses_tech",
          "source": "repo:transformers",
          "target": "tech:tree_of_thought_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-06_venturebeat_5618",
              "url": "https://venturebeat.com/2025/12/06/transformers_tree_of_thought_n",
              "published": "2025-12-06",
              "snippet": "Technical details reveal transformers relies heavily on Tree of Thought Next..."
            },
            {
              "docId": "2025-12-20_weights_and_bia_4998",
              "url": "https://wandb.ai/articles/2025/12/20/transformers_tree_of_thought_n",
              "published": "2025-12-20",
              "snippet": "Technical details reveal transformers relies heavily on Tree of Thought Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_midjourney_v7_lite_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:midjourney_v7_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:dify_v2_kv_cache_optimization_ultra_uses_tech",
          "source": "tool:dify_v2",
          "target": "tech:kv_cache_optimization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:ollama_next_crewai_v2_integrates_with",
          "source": "tool:ollama_next",
          "target": "tool:crewai_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-23_weights_and_bia_6630",
              "url": "https://wandb.ai/articles/2026/01/23/ollama_next_crewai_v2",
              "published": "2026-01-23",
              "snippet": "Ollama Next announced official support for CrewAI v2..."
            },
            {
              "docId": "2026-01-23_arxiv_9037",
              "url": "https://arxiv.org/abs/2026/01/23/ollama_next_crewai_v2",
              "published": "2026-01-23",
              "snippet": "The latest release of Ollama Next adds native CrewAI v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_plus_falcon_3_ultra_depends_on",
          "source": "model:nemotron_5_plus",
          "target": "model:falcon_3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_tree_of_thought_v2_uses_tech",
          "source": "paper:direct_preference_optimization_edge",
          "target": "tech:tree_of_thought_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_9396",
              "url": "https://techcrunch.com/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Direct Preference Optimization Edge relies heavily on Tree of Thought v2..."
            },
            {
              "docId": "2026-01-23_ars_technica_4833",
              "url": "https://arstechnica.com/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Direct Preference Optimization Edge relies heavily on Tree of Thought v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_dpo_v2_uses_tech",
          "source": "model:jamba_2_mini",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_gsm8k_ultra_evaluated_on",
          "source": "model:nemotron_5_next",
          "target": "benchmark:gsm8k_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-04_meta_ai_blog_7857",
              "url": "https://ai.meta.com/blog/2025/12/04/nemotron_5_next_gsm8k_ultra",
              "published": "2025-12-04",
              "snippet": "Nemotron-5 Next achieves 85% on GSM8K Ultra, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_vllm_pro_integrates_with",
          "source": "tool:flowise_plus",
          "target": "tool:vllm_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_7214",
              "url": "https://huggingface.co/blog/2026/01/22/flowise_plus_vllm_pro",
              "published": "2026-01-22",
              "snippet": "Flowise Plus now supports vLLM Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_dall_e_4_edge_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:dall_e_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-16_wired_1898",
              "url": "https://wired.com/2026/01/16/llamacpp_mini_dall_e_4_edge",
              "published": "2026-01-16",
              "snippet": "llama.cpp Mini announced official support for DALL-E 4 Edge..."
            },
            {
              "docId": "2026-01-20_nvidia_blog_5978",
              "url": "https://blogs.nvidia.com/2026/01/20/llamacpp_mini_dall_e_4_edge",
              "published": "2026-01-20",
              "snippet": "llama.cpp Mini now supports DALL-E 4 Edge with full feature parity..."
            },
            {
              "docId": "2026-01-20_anthropic_blog_4109",
              "url": "https://anthropic.com/news/2026/01/20/llamacpp_mini_dall_e_4_edge",
              "published": "2026-01-20",
              "snippet": "llama.cpp Mini announced official support for DALL-E 4 Edge..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_2991",
              "url": "https://anthropic.com/news/2026/01/25/llamacpp_mini_dall_e_4_edge",
              "published": "2026-01-25",
              "snippet": "The latest release of llama.cpp Mini adds native DALL-E 4 Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_max_localai_next_integrates_with",
          "source": "tool:crewai_max",
          "target": "tool:localai_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_mixture_of_experts_next_uses_tech",
          "source": "model:midjourney_v7_core",
          "target": "tech:mixture_of_experts_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:haystack_lite_vllm_integrates_with",
          "source": "tool:haystack_lite",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_constitutional_ai_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "tech:constitutional_ai_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:localai_v2_flash_attention_lite_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:flash_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:codex_2_edge_tokenizer_bpe_lite_uses_tech",
          "source": "model:codex_2_edge",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-11_langchain_blog_2990",
              "url": "https://blog.langchain.dev/2026/01/11/codex_2_edge_tokenizer_bpe_lit",
              "published": "2026-01-11",
              "snippet": "Codex 2 Edge leverages Tokenizer BPE Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_dpo_max_uses_tech",
          "source": "model:stable_diffusion_4_plus",
          "target": "tech:dpo_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_edge_autogpt_edge_integrates_with",
          "source": "tool:tensorrt_llm_edge",
          "target": "tool:autogpt_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:mc4_edge_constitutional_ai_next_uses_tech",
          "source": "dataset:mc4_edge",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_core_nemotron_5_plus_depends_on",
          "source": "model:gemini_ultra_2_core",
          "target": "model:nemotron_5_plus",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:dify_edge_mixture_of_experts_uses_tech",
          "source": "tool:dify_edge",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-09_ars_technica_6288",
              "url": "https://arstechnica.com/2026/01/09/dify_edge_mixture_of_experts",
              "published": "2026-01-09",
              "snippet": "Under the hood, Dify Edge implements Mixture of Experts for improved efficiency..."
            },
            {
              "docId": "2026-01-20_google_ai_blog_8301",
              "url": "https://blog.google/technology/ai/2026/01/20/dify_edge_mixture_of_experts",
              "published": "2026-01-20",
              "snippet": "Dify Edge leverages Mixture of Experts to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_red_teaming_v2_uses_tech",
          "source": "model:jamba_2_core",
          "target": "tech:red_teaming_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-26_the_gradient_5281",
              "url": "https://thegradient.pub/2025/12/26/jamba_2_core_red_teaming_v2",
              "published": "2025-12-26",
              "snippet": "Jamba 2 Core leverages Red Teaming v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-30_mit_technology__9950",
              "url": "https://technologyreview.com/2025/12/30/jamba_2_core_red_teaming_v2",
              "published": "2025-12-30",
              "snippet": "Jamba 2 Core leverages Red Teaming v2 to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-31_mit_technology__2734",
              "url": "https://technologyreview.com/2025/12/31/jamba_2_core_red_teaming_v2",
              "published": "2025-12-31",
              "snippet": "Technical details reveal Jamba 2 Core relies heavily on Red Teaming v2..."
            },
            {
              "docId": "2026-01-03_openai_blog_5464",
              "url": "https://openai.com/blog/2026/01/03/jamba_2_core_red_teaming_v2",
              "published": "2026-01-03",
              "snippet": "Jamba 2 Core leverages Red Teaming v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_group_query_attention_mini_uses_tech",
          "source": "tool:vllm",
          "target": "tech:group_query_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-19_nvidia_blog_2554",
              "url": "https://blogs.nvidia.com/2025/12/19/vllm_group_query_attention_min",
              "published": "2025-12-19",
              "snippet": "vLLM leverages Group Query Attention Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-28_openai_blog_7237",
              "url": "https://openai.com/blog/2025/12/28/vllm_group_query_attention_min",
              "published": "2025-12-28",
              "snippet": "Technical details reveal vLLM relies heavily on Group Query Attention Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_mixture_of_experts_uses_tech",
          "source": "tool:cursor_mini",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-06_techcrunch_8996",
              "url": "https://techcrunch.com/2026/01/06/cursor_mini_mixture_of_experts",
              "published": "2026-01-06",
              "snippet": "Technical details reveal Cursor Mini relies heavily on Mixture of Experts..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_sora_2_mini_depends_on",
          "source": "model:gpt_5",
          "target": "model:sora_2_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:cody_plus_tree_of_thought_lite_uses_tech",
          "source": "tool:cody_plus",
          "target": "tech:tree_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_grok_3_integrates_with",
          "source": "repo:llamacpp_v2",
          "target": "model:grok_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-12_anthropic_blog_4504",
              "url": "https://anthropic.com/news/2026/01/12/llamacpp_v2_grok_3",
              "published": "2026-01-12",
              "snippet": "llama.cpp v2 announced official support for Grok-3..."
            },
            {
              "docId": "2026-01-18_techcrunch_4448",
              "url": "https://techcrunch.com/2026/01/18/llamacpp_v2_grok_3",
              "published": "2026-01-18",
              "snippet": "llama.cpp v2 now supports Grok-3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_palm_3_v2_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "model:palm_3_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_rlhf_mini_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-12-08_microsoft_resea_2007",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/08/retrieval_augmented_generation",
              "published": "2025-12-08",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP leverages RLHF Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_ars_technica_5258",
              "url": "https://arstechnica.com/2026/01/25/retrieval_augmented_generation",
              "published": "2026-01-25",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP leverages RLHF Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_core_gpt_5_v2_integrates_with",
          "source": "tool:localai_core",
          "target": "model:gpt_5_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_5353",
              "url": "https://blog.langchain.dev/2026/01/17/localai_core_gpt_5_v2",
              "published": "2026-01-17",
              "snippet": "LocalAI Core announced official support for GPT-5 v2..."
            },
            {
              "docId": "2026-01-24_mit_technology__5530",
              "url": "https://technologyreview.com/2026/01/24/localai_core_gpt_5_v2",
              "published": "2026-01-24",
              "snippet": "LocalAI Core now supports GPT-5 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_1496",
              "url": "https://anthropic.com/news/2026/01/25/localai_core_gpt_5_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of LocalAI Core adds native GPT-5 v2 integration..."
            },
            {
              "docId": "2026-01-25_techcrunch_2584",
              "url": "https://techcrunch.com/2026/01/25/localai_core_gpt_5_v2",
              "published": "2026-01-25",
              "snippet": "LocalAI Core now supports GPT-5 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_red_teaming_edge_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:red_teaming_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-06_techcrunch_6955",
              "url": "https://techcrunch.com/2026/01/06/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-06",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models leverages Red Teaming Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_falcon_3_max_integrates_with",
          "source": "tool:gradio",
          "target": "model:falcon_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-27_meta_ai_blog_7477",
              "url": "https://ai.meta.com/blog/2025/12/27/gradio_falcon_3_max",
              "published": "2025-12-27",
              "snippet": "Gradio announced official support for Falcon 3 Max..."
            },
            {
              "docId": "2026-01-02_microsoft_resea_8364",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/02/gradio_falcon_3_max",
              "published": "2026-01-02",
              "snippet": "Gradio announced official support for Falcon 3 Max..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_8410",
              "url": "https://huggingface.co/blog/2026/01/25/gradio_falcon_3_max",
              "published": "2026-01-25",
              "snippet": "Gradio now supports Falcon 3 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_sliding_window_attention_lite_uses_tech",
          "source": "dataset:fineweb",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_gsm8k_core_evaluated_on",
          "source": "model:jamba_2_lite",
          "target": "benchmark:gsm8k_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__3168",
              "url": "https://technologyreview.com/2026/01/24/jamba_2_lite_gsm8k_core",
              "published": "2026-01-24",
              "snippet": "Jamba 2 Lite achieves 82% on GSM8K Core, setting a new record..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_2644",
              "url": "https://wandb.ai/articles/2026/01/25/jamba_2_lite_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Jamba 2 Lite reaching 78% on GSM8K Core..."
            },
            {
              "docId": "2026-01-25_ars_technica_3085",
              "url": "https://arstechnica.com/2026/01/25/jamba_2_lite_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Jamba 2 Lite reaching 83% on GSM8K Core..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_2576",
              "url": "https://anthropic.com/news/2026/01/25/jamba_2_lite_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "Jamba 2 Lite achieves 70% on GSM8K Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_core_tensorrt_llm_plus_integrates_with",
          "source": "tool:localai_core",
          "target": "tool:tensorrt_llm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_nemotron_5_depends_on",
          "source": "model:gemma_3_lite",
          "target": "model:nemotron_5",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_transformer_architecture_edge_uses_tech",
          "source": "tool:langchain_plus",
          "target": "tech:transformer_architecture_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:autogpt_ultra_falcon_3_ultra_integrates_with",
          "source": "tool:autogpt_ultra",
          "target": "model:falcon_3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_pro_gpt_4o_mini_2_depends_on",
          "source": "model:command_r_plus_pro",
          "target": "model:gpt_4o_mini_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_constitutional_ai_edge_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-11-25_arxiv_7291",
              "url": "https://arxiv.org/abs/2025/11/25/attention_is_all_you_need_v2_p",
              "published": "2025-11-25",
              "snippet": "Under the hood, Attention Is All You Need v2 Plus implements Constitutional AI Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-24_reuters_7916",
              "url": "https://reuters.com/technology/2026/01/24/attention_is_all_you_need_v2_p",
              "published": "2026-01-24",
              "snippet": "Under the hood, Attention Is All You Need v2 Plus implements Constitutional AI Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-24_venturebeat_1235",
              "url": "https://venturebeat.com/2026/01/24/attention_is_all_you_need_v2_p",
              "published": "2026-01-24",
              "snippet": "Attention Is All You Need v2 Plus leverages Constitutional AI Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_the_verge_9674",
              "url": "https://theverge.com/2026/01/24/attention_is_all_you_need_v2_p",
              "published": "2026-01-24",
              "snippet": "Under the hood, Attention Is All You Need v2 Plus implements Constitutional AI Edge for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_kv_cache_optimization_pro_uses_tech",
          "source": "model:codex_2",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:llamacpp_claude_opus_45_plus_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:claude_opus_45_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:phi_4_pro_jamba_2_max_depends_on",
          "source": "model:phi_4_pro",
          "target": "model:jamba_2_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.52
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_max_stable_diffusion_4_next_measures",
          "source": "benchmark:alpacaeval_2_max",
          "target": "model:stable_diffusion_4_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:codex_2_edge_gguf_max_uses_tech",
          "source": "model:codex_2_edge",
          "target": "tech:gguf_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-11_anthropic_blog_1696",
              "url": "https://anthropic.com/news/2026/01/11/codex_2_edge_gguf_max",
              "published": "2026-01-11",
              "snippet": "Technical details reveal Codex 2 Edge relies heavily on GGUF Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_constitutional_ai_pro_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "tech:constitutional_ai_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_5160",
              "url": "https://huggingface.co/blog/2026/01/22/attention_is_all_you_need_v2_p",
              "published": "2026-01-22",
              "snippet": "Under the hood, Attention Is All You Need v2 Plus implements Constitutional AI Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-25_arxiv_7817",
              "url": "https://arxiv.org/abs/2026/01/25/attention_is_all_you_need_v2_p",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Plus leverages Constitutional AI Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_venturebeat_7643",
              "url": "https://venturebeat.com/2026/01/25/attention_is_all_you_need_v2_p",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Attention Is All You Need v2 Plus relies heavily on Constitutional AI Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_kv_cache_optimization_uses_tech",
          "source": "repo:vllm",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_max_quantization_lite_uses_tech",
          "source": "model:dall_e_4_max",
          "target": "tech:quantization_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:sora_2_mini_dall_e_4_ultra_depends_on",
          "source": "model:sora_2_mini",
          "target": "model:dall_e_4_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_gpt_5_lite_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:gpt_5_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:dify_transformer_architecture_uses_tech",
          "source": "tool:dify",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-19_the_verge_8859",
              "url": "https://theverge.com/2026/01/19/dify_transformer_architecture",
              "published": "2026-01-19",
              "snippet": "Dify leverages Transformer Architecture to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_4586",
              "url": "https://huggingface.co/blog/2026/01/25/dify_transformer_architecture",
              "published": "2026-01-25",
              "snippet": "Under the hood, Dify implements Transformer Architecture for improved efficiency..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_5731",
              "url": "https://ai.meta.com/blog/2026/01/25/dify_transformer_architecture",
              "published": "2026-01-25",
              "snippet": "Under the hood, Dify implements Transformer Architecture for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_v2_deepseek_v3_plus_integrates_with",
          "source": "tool:crewai_v2",
          "target": "model:deepseek_v3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-23_reuters_9004",
              "url": "https://reuters.com/technology/2026/01/23/crewai_v2_deepseek_v3_plus",
              "published": "2026-01-23",
              "snippet": "CrewAI v2 now supports DeepSeek-V3 Plus with full feature parity..."
            },
            {
              "docId": "2026-01-24_mit_technology__8081",
              "url": "https://technologyreview.com/2026/01/24/crewai_v2_deepseek_v3_plus",
              "published": "2026-01-24",
              "snippet": "CrewAI v2 now supports DeepSeek-V3 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_edge_tree_of_thought_edge_uses_tech",
          "source": "tool:dify_edge",
          "target": "tech:tree_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-15_arxiv_3154",
              "url": "https://arxiv.org/abs/2026/01/15/dify_edge_tree_of_thought_edge",
              "published": "2026-01-15",
              "snippet": "Technical details reveal Dify Edge relies heavily on Tree of Thought Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_mbpp_lite_evaluated_on",
          "source": "model:codex_2",
          "target": "benchmark:mbpp_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_reuters_1285",
              "url": "https://reuters.com/technology/2026/01/25/codex_2_mbpp_lite",
              "published": "2026-01-25",
              "snippet": "On the MBPP Lite benchmark, Codex 2 scored 84%..."
            },
            {
              "docId": "2026-01-25_the_gradient_1054",
              "url": "https://thegradient.pub/2026/01/25/codex_2_mbpp_lite",
              "published": "2026-01-25",
              "snippet": "On the MBPP Lite benchmark, Codex 2 scored 84%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_kv_cache_optimization_core_uses_tech",
          "source": "repo:localai",
          "target": "tech:kv_cache_optimization_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-11_wired_1678",
              "url": "https://wired.com/2025/12/11/localai_kv_cache_optimization_",
              "published": "2025-12-11",
              "snippet": "Under the hood, LocalAI implements KV Cache Optimization Core for improved efficiency..."
            },
            {
              "docId": "2025-12-20_openai_blog_9072",
              "url": "https://openai.com/blog/2025/12/20/localai_kv_cache_optimization_",
              "published": "2025-12-20",
              "snippet": "Technical details reveal LocalAI relies heavily on KV Cache Optimization Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_edge_litellm_plus_integrates_with",
          "source": "tool:localai_edge",
          "target": "tool:litellm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_8312",
              "url": "https://arxiv.org/abs/2026/01/24/localai_edge_litellm_plus",
              "published": "2026-01-24",
              "snippet": "The latest release of LocalAI Edge adds native LiteLLM Plus integration..."
            },
            {
              "docId": "2026-01-24_venturebeat_4659",
              "url": "https://venturebeat.com/2026/01/24/localai_edge_litellm_plus",
              "published": "2026-01-24",
              "snippet": "The latest release of LocalAI Edge adds native LiteLLM Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_flash_attention_ultra_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:flash_attention_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_6464",
              "url": "https://huggingface.co/blog/2026/01/22/llamacpp_mini_flash_attention_",
              "published": "2026-01-22",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Flash Attention Ultra..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_7872",
              "url": "https://wandb.ai/articles/2026/01/25/llamacpp_mini_flash_attention_",
              "published": "2026-01-25",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Flash Attention Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_lite_command_r_plus_edge_integrates_with",
          "source": "tool:semantic_kernel_lite",
          "target": "model:command_r_plus_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-11-22_wired_8269",
              "url": "https://wired.com/2025/11/22/semantic_kernel_lite_command_r",
              "published": "2025-11-22",
              "snippet": "Semantic Kernel Lite announced official support for Command R+ Edge..."
            },
            {
              "docId": "2025-12-21_bloomberg_5427",
              "url": "https://bloomberg.com/technology/2025/12/21/semantic_kernel_lite_command_r",
              "published": "2025-12-21",
              "snippet": "Semantic Kernel Lite announced official support for Command R+ Edge..."
            },
            {
              "docId": "2025-12-26_bloomberg_1619",
              "url": "https://bloomberg.com/technology/2025/12/26/semantic_kernel_lite_command_r",
              "published": "2025-12-26",
              "snippet": "Semantic Kernel Lite now supports Command R+ Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_tool_use_core_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-13_bloomberg_1636",
              "url": "https://bloomberg.com/technology/2025/12/13/flash_attention:_fast_and_memo",
              "published": "2025-12-13",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention leverages Tool Use Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_qwen_3_core_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:qwen_3_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-22_reuters_4554",
              "url": "https://reuters.com/technology/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Direct Preference Optimization Pro reaching 91% on Qwen-3 Core..."
            },
            {
              "docId": "2026-01-25_the_gradient_8025",
              "url": "https://thegradient.pub/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "On the Qwen-3 Core benchmark, Direct Preference Optimization Pro scored 83%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_core_chain_of_thought_core_uses_tech",
          "source": "dataset:c4_core",
          "target": "tech:chain_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_swe_bench_next_evaluated_on",
          "source": "model:dall_e_4",
          "target": "benchmark:swe_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:gemma_3_pro_mmlu_edge_evaluated_on",
          "source": "model:gemma_3_pro",
          "target": "benchmark:mmlu_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-08_nvidia_blog_1156",
              "url": "https://blogs.nvidia.com/2026/01/08/gemma_3_pro_mmlu_edge",
              "published": "2026-01-08",
              "snippet": "On the MMLU Edge benchmark, Gemma 3 Pro scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_speculative_decoding_pro_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:speculative_decoding_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-31_venturebeat_9506",
              "url": "https://venturebeat.com/2025/12/31/textbooks_are_all_you_need_min",
              "published": "2025-12-31",
              "snippet": "Textbooks Are All You Need Mini leverages Speculative Decoding Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_pro_speculative_decoding_uses_tech",
          "source": "dataset:redpajama_v2_pro",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.4
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_mixtral_8x22b_integrates_with",
          "source": "repo:autogpt_v2",
          "target": "model:mixtral_8x22b",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-24_google_ai_blog_3092",
              "url": "https://blog.google/technology/ai/2026/01/24/autogpt_v2_mixtral_8x22b",
              "published": "2026-01-24",
              "snippet": "AutoGPT v2 announced official support for Mixtral 8x22B..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_aya_3_integrates_with",
          "source": "tool:langchain_edge",
          "target": "model:aya_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-01_openai_blog_5290",
              "url": "https://openai.com/blog/2026/01/01/langchain_edge_aya_3",
              "published": "2026-01-01",
              "snippet": "LangChain Edge now supports Aya 3 with full feature parity..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_9024",
              "url": "https://blog.google/technology/ai/2026/01/24/langchain_edge_aya_3",
              "published": "2026-01-24",
              "snippet": "LangChain Edge announced official support for Aya 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_stable_diffusion_4_plus_integrates_with",
          "source": "repo:autogpt_plus",
          "target": "model:stable_diffusion_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_9264",
              "url": "https://arstechnica.com/2026/01/25/autogpt_plus_stable_diffusion_",
              "published": "2026-01-25",
              "snippet": "The latest release of AutoGPT Plus adds native Stable Diffusion 4 Plus integration..."
            },
            {
              "docId": "2026-01-25_the_gradient_5663",
              "url": "https://thegradient.pub/2026/01/25/autogpt_plus_stable_diffusion_",
              "published": "2026-01-25",
              "snippet": "AutoGPT Plus now supports Stable Diffusion 4 Plus with full feature parity..."
            },
            {
              "docId": "2026-01-25_bloomberg_4477",
              "url": "https://bloomberg.com/technology/2026/01/25/autogpt_plus_stable_diffusion_",
              "published": "2026-01-25",
              "snippet": "The latest release of AutoGPT Plus adds native Stable Diffusion 4 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_sora_2_edge_depends_on",
          "source": "model:claude_opus_45",
          "target": "model:sora_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:refinedweb_ultra_sliding_window_attention_lite_uses_tech",
          "source": "dataset:refinedweb_ultra",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:laion_5b_core_rlhf_core_uses_tech",
          "source": "dataset:laion_5b_core",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_stable_diffusion_4_next_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "model:stable_diffusion_4_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-13_hugging_face_bl_7826",
              "url": "https://huggingface.co/blog/2026/01/13/attention_is_all_you_need_v2_m",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Attention Is All You Need v2 Max reaching 84% on Stable Diffusion 4 Next..."
            },
            {
              "docId": "2026-01-15_the_gradient_1200",
              "url": "https://thegradient.pub/2026/01/15/attention_is_all_you_need_v2_m",
              "published": "2026-01-15",
              "snippet": "On the Stable Diffusion 4 Next benchmark, Attention Is All You Need v2 Max scored 77%..."
            },
            {
              "docId": "2026-01-23_google_ai_blog_7842",
              "url": "https://blog.google/technology/ai/2026/01/23/attention_is_all_you_need_v2_m",
              "published": "2026-01-23",
              "snippet": "Attention Is All You Need v2 Max achieves 97% on Stable Diffusion 4 Next, setting a new record..."
            },
            {
              "docId": "2026-01-24_ars_technica_6326",
              "url": "https://arstechnica.com/2026/01/24/attention_is_all_you_need_v2_m",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Attention Is All You Need v2 Max reaching 91% on Stable Diffusion 4 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_whisper_v4_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:whisper_v4",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:llamaindex_max_dify_core_integrates_with",
          "source": "tool:llamaindex_max",
          "target": "tool:dify_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_redpajama_v2_core_trained_on",
          "source": "model:claude_sonnet_4_next",
          "target": "dataset:redpajama_v2_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__red_teaming_lite_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_c4_next_trained_on",
          "source": "model:stable_diffusion_4_max",
          "target": "dataset:c4_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:haystack_pro_gpt_4o_mini_2_integrates_with",
          "source": "tool:haystack_pro",
          "target": "model:gpt_4o_mini_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:gpt_5_max_flash_attention_max_uses_tech",
          "source": "model:gpt_5_max",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:qwen_3_max_redpajama_v2_edge_trained_on",
          "source": "model:qwen_3_max",
          "target": "dataset:redpajama_v2_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_max_fineweb_lite_trained_on",
          "source": "model:claude_sonnet_4_max",
          "target": "dataset:fineweb_lite",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-17_ars_technica_5929",
              "url": "https://arstechnica.com/2026/01/17/claude_sonnet_4_max_fineweb_li",
              "published": "2026-01-17",
              "snippet": "The training corpus for Claude Sonnet 4 Max includes FineWeb Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mbpp_lite_llama_4_v2_measures",
          "source": "benchmark:mbpp_lite",
          "target": "model:llama_4_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_6230",
              "url": "https://blog.google/technology/ai/2026/01/25/mbpp_lite_llama_4_v2",
              "published": "2026-01-25",
              "snippet": "MBPP Lite has become the standard for evaluating Llama 4 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_jamba_2_max_integrates_with",
          "source": "repo:langchain",
          "target": "model:jamba_2_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-25_langchain_blog_6152",
              "url": "https://blog.langchain.dev/2026/01/25/langchain_jamba_2_max",
              "published": "2026-01-25",
              "snippet": "langchain announced official support for Jamba 2 Max..."
            },
            {
              "docId": "2026-01-25_reuters_6364",
              "url": "https://reuters.com/technology/2026/01/25/langchain_jamba_2_max",
              "published": "2026-01-25",
              "snippet": "langchain now supports Jamba 2 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_math_next_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:math_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_gemma_3_pro_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:gemma_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_codex_2_pro_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:codex_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-15_techcrunch_8302",
              "url": "https://techcrunch.com/2026/01/15/open_interpreter_pro_codex_2_p",
              "published": "2026-01-15",
              "snippet": "open-interpreter Pro now supports Codex 2 Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_phi_4_evaluated_on",
          "source": "paper:direct_preference_optimization_next",
          "target": "model:phi_4",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:autogpt_red_teaming_ultra_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_edge_claude_sonnet_4_v2_measures",
          "source": "benchmark:lmsys_chatbot_arena_edge",
          "target": "model:claude_sonnet_4_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2025-12-05_openai_blog_4172",
              "url": "https://openai.com/blog/2025/12/05/lmsys_chatbot_arena_edge_claud",
              "published": "2025-12-05",
              "snippet": "The LMSYS Chatbot Arena Edge benchmark measures Claude Sonnet 4 v2 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_gemini_ultra_2_edge_integrates_with",
          "source": "tool:ollama",
          "target": "model:gemini_ultra_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-18_google_ai_blog_9028",
              "url": "https://blog.google/technology/ai/2026/01/18/ollama_gemini_ultra_2_edge",
              "published": "2026-01-18",
              "snippet": "Ollama now supports Gemini Ultra 2 Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_plus_swe_bench_lite_evaluated_on",
          "source": "model:claude_opus_45_plus",
          "target": "benchmark:swe_bench_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_meta_ai_blog_8556",
              "url": "https://ai.meta.com/blog/2026/01/24/claude_opus_45_plus_swe_bench_",
              "published": "2026-01-24",
              "snippet": "On the SWE-bench Lite benchmark, Claude Opus 4.5 Plus scored 72%..."
            },
            {
              "docId": "2026-01-25_arxiv_3978",
              "url": "https://arxiv.org/abs/2026/01/25/claude_opus_45_plus_swe_bench_",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 Plus achieves 80% on SWE-bench Lite, setting a new record..."
            },
            {
              "docId": "2026-01-25_techcrunch_5761",
              "url": "https://techcrunch.com/2026/01/25/claude_opus_45_plus_swe_bench_",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 Plus achieves 97% on SWE-bench Lite, setting a new record..."
            },
            {
              "docId": "2026-01-25_mit_technology__2887",
              "url": "https://technologyreview.com/2026/01/25/claude_opus_45_plus_swe_bench_",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 Plus achieves 73% on SWE-bench Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_qwen_3_mini_integrates_with",
          "source": "repo:llamacpp_v2",
          "target": "model:qwen_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-14_hugging_face_bl_2159",
              "url": "https://huggingface.co/blog/2025/12/14/llamacpp_v2_qwen_3_mini",
              "published": "2025-12-14",
              "snippet": "llama.cpp v2 now supports Qwen-3 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_mmlu_ultra_evaluated_on",
          "source": "model:deepseek_v3_mini",
          "target": "benchmark:mmlu_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-08_anthropic_blog_9240",
              "url": "https://anthropic.com/news/2026/01/08/deepseek_v3_mini_mmlu_ultra",
              "published": "2026-01-08",
              "snippet": "On the MMLU Ultra benchmark, DeepSeek-V3 Mini scored 87%..."
            },
            {
              "docId": "2026-01-14_the_verge_1019",
              "url": "https://theverge.com/2026/01/14/deepseek_v3_mini_mmlu_ultra",
              "published": "2026-01-14",
              "snippet": "DeepSeek-V3 Mini achieves 88% on MMLU Ultra, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_redpajama_v2_pro_trained_on",
          "source": "model:stable_diffusion_4_next",
          "target": "dataset:redpajama_v2_pro",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-19_the_verge_4381",
              "url": "https://theverge.com/2026/01/19/stable_diffusion_4_next_redpaj",
              "published": "2026-01-19",
              "snippet": "Stable Diffusion 4 Next utilized RedPajama v2 Pro as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_next_slimpajama_v2_trained_on",
          "source": "model:gpt_4o_mini_2_next",
          "target": "dataset:slimpajama_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-10_bloomberg_2853",
              "url": "https://bloomberg.com/technology/2026/01/10/gpt_4o_mini_2_next_slimpajama_",
              "published": "2026-01-10",
              "snippet": "GPT-4o Mini 2 Next was trained on SlimPajama v2 comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_ultra_synthetic_data_generation_edge_uses_tech",
          "source": "dataset:wikipedia_dump_2025_ultra",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_falcon_3_v2_depends_on",
          "source": "model:claude_sonnet_4",
          "target": "model:falcon_3_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:flowise_multimodal_fusion_edge_uses_tech",
          "source": "tool:flowise",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:gpqa_plus_midjourney_v7_mini_measures",
          "source": "benchmark:gpqa_plus",
          "target": "model:midjourney_v7_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_laion_5b_ultra_trained_on",
          "source": "model:command_r_plus",
          "target": "dataset:laion_5b_ultra",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:jamba_2_edge_distillation_lite_uses_tech",
          "source": "model:jamba_2_edge",
          "target": "tech:distillation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-10_the_verge_5247",
              "url": "https://theverge.com/2026/01/10/jamba_2_edge_distillation_lite",
              "published": "2026-01-10",
              "snippet": "Jamba 2 Edge leverages Distillation Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_edge_claude_opus_45_v2_integrates_with",
          "source": "tool:crewai_edge",
          "target": "model:claude_opus_45_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:gpt_5_core_humaneval_next_evaluated_on",
          "source": "model:gpt_5_core",
          "target": "benchmark:humaneval_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-19_reuters_8209",
              "url": "https://reuters.com/technology/2026/01/19/gpt_5_core_humaneval_next",
              "published": "2026-01-19",
              "snippet": "GPT-5 Core achieves 88% on HumanEval Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_max_dpo_v2_uses_tech",
          "source": "repo:autogpt_max",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-13_nextgov_7561",
              "url": "https://nextgov.com/2026/01/13/autogpt_max_dpo_v2",
              "published": "2026-01-13",
              "snippet": "Technical details reveal AutoGPT Max relies heavily on DPO v2..."
            },
            {
              "docId": "2026-01-22_bloomberg_4165",
              "url": "https://bloomberg.com/technology/2026/01/22/autogpt_max_dpo_v2",
              "published": "2026-01-22",
              "snippet": "Technical details reveal AutoGPT Max relies heavily on DPO v2..."
            },
            {
              "docId": "2026-01-23_the_gradient_9240",
              "url": "https://thegradient.pub/2026/01/23/autogpt_max_dpo_v2",
              "published": "2026-01-23",
              "snippet": "AutoGPT Max leverages DPO v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_distillation_lite_uses_tech",
          "source": "model:command_r_plus",
          "target": "tech:distillation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2026-01-14_venturebeat_1539",
              "url": "https://venturebeat.com/2026/01/14/command_r_plus_distillation_li",
              "published": "2026-01-14",
              "snippet": "Under the hood, Command R+ implements Distillation Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_plus_mixtral_8x22b_edge_integrates_with",
          "source": "tool:tensorrt_llm_plus",
          "target": "model:mixtral_8x22b_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-31_google_ai_blog_9343",
              "url": "https://blog.google/technology/ai/2025/12/31/tensorrt_llm_plus_mixtral_8x22",
              "published": "2025-12-31",
              "snippet": "The latest release of TensorRT-LLM Plus adds native Mixtral 8x22B Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_mini_jamba_2_next_integrates_with",
          "source": "tool:tensorrt_llm_mini",
          "target": "model:jamba_2_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-07_anthropic_blog_1079",
              "url": "https://anthropic.com/news/2026/01/07/tensorrt_llm_mini_jamba_2_next",
              "published": "2026-01-07",
              "snippet": "TensorRT-LLM Mini now supports Jamba 2 Next with full feature parity..."
            },
            {
              "docId": "2026-01-09_reuters_6452",
              "url": "https://reuters.com/technology/2026/01/09/tensorrt_llm_mini_jamba_2_next",
              "published": "2026-01-09",
              "snippet": "The latest release of TensorRT-LLM Mini adds native Jamba 2 Next integration..."
            },
            {
              "docId": "2026-01-21_reuters_9132",
              "url": "https://reuters.com/technology/2026/01/21/tensorrt_llm_mini_jamba_2_next",
              "published": "2026-01-21",
              "snippet": "TensorRT-LLM Mini announced official support for Jamba 2 Next..."
            },
            {
              "docId": "2026-01-21_arxiv_2163",
              "url": "https://arxiv.org/abs/2026/01/21/tensorrt_llm_mini_jamba_2_next",
              "published": "2026-01-21",
              "snippet": "TensorRT-LLM Mini now supports Jamba 2 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_sora_2_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:sora_2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-14_langchain_blog_3377",
              "url": "https://blog.langchain.dev/2026/01/14/textbooks_are_all_you_need_max",
              "published": "2026-01-14",
              "snippet": "Textbooks Are All You Need Max achieves 89% on Sora 2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_lite_distillation_next_uses_tech",
          "source": "dataset:openwebtext2_lite",
          "target": "tech:distillation_next",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_multimodal_fusion_edge_uses_tech",
          "source": "model:phi_4_lite",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-25_arxiv_7308",
              "url": "https://arxiv.org/abs/2026/01/25/phi_4_lite_multimodal_fusion_e",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Lite relies heavily on Multimodal Fusion Edge..."
            },
            {
              "docId": "2026-01-25_bloomberg_1406",
              "url": "https://bloomberg.com/technology/2026/01/25/phi_4_lite_multimodal_fusion_e",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Lite relies heavily on Multimodal Fusion Edge..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_8264",
              "url": "https://huggingface.co/blog/2026/01/25/phi_4_lite_multimodal_fusion_e",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Lite relies heavily on Multimodal Fusion Edge..."
            },
            {
              "docId": "2026-01-25_bloomberg_8884",
              "url": "https://bloomberg.com/technology/2026/01/25/phi_4_lite_multimodal_fusion_e",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Phi-4 Lite relies heavily on Multimodal Fusion Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_lora_max_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:math_v2_stable_diffusion_4_plus_measures",
          "source": "benchmark:math_v2",
          "target": "model:stable_diffusion_4_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-20_ars_technica_5191",
              "url": "https://arstechnica.com/2026/01/20/math_v2_stable_diffusion_4_plu",
              "published": "2026-01-20",
              "snippet": "MATH v2 provides standardized evaluation of Stable Diffusion 4 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_rotary_position_embedding_ultra_uses_tech",
          "source": "repo:llamacpp_ultra",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-18_anthropic_blog_1194",
              "url": "https://anthropic.com/news/2026/01/18/llamacpp_ultra_rotary_position",
              "published": "2026-01-18",
              "snippet": "Under the hood, llama.cpp Ultra implements Rotary Position Embedding Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-21_wired_9224",
              "url": "https://wired.com/2026/01/21/llamacpp_ultra_rotary_position",
              "published": "2026-01-21",
              "snippet": "Technical details reveal llama.cpp Ultra relies heavily on Rotary Position Embedding Ultra..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_6913",
              "url": "https://huggingface.co/blog/2026/01/25/llamacpp_ultra_rotary_position",
              "published": "2026-01-25",
              "snippet": "Under the hood, llama.cpp Ultra implements Rotary Position Embedding Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_v2_codex_2_core_measures",
          "source": "benchmark:gsm8k_v2",
          "target": "model:codex_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-18_meta_ai_blog_8584",
              "url": "https://ai.meta.com/blog/2026/01/18/gsm8k_v2_codex_2_core",
              "published": "2026-01-18",
              "snippet": "The GSM8K v2 benchmark measures Codex 2 Core across multiple tasks..."
            },
            {
              "docId": "2026-01-22_the_gradient_6523",
              "url": "https://thegradient.pub/2026/01/22/gsm8k_v2_codex_2_core",
              "published": "2026-01-22",
              "snippet": "The GSM8K v2 benchmark measures Codex 2 Core across multiple tasks..."
            },
            {
              "docId": "2026-01-24_reuters_4199",
              "url": "https://reuters.com/technology/2026/01/24/gsm8k_v2_codex_2_core",
              "published": "2026-01-24",
              "snippet": "GSM8K v2 provides standardized evaluation of Codex 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_flowise_max_integrates_with",
          "source": "tool:ollama_lite",
          "target": "tool:flowise_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-05_meta_ai_blog_5201",
              "url": "https://ai.meta.com/blog/2026/01/05/ollama_lite_flowise_max",
              "published": "2026-01-05",
              "snippet": "The latest release of Ollama Lite adds native Flowise Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_ultra_synthetic_data_generation_plus_uses_tech",
          "source": "model:deepseek_v3_ultra",
          "target": "tech:synthetic_data_generation_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:jamba_2_ultra_dpo_lite_uses_tech",
          "source": "model:jamba_2_ultra",
          "target": "tech:dpo_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:falcon_3_core_phi_4_max_depends_on",
          "source": "model:falcon_3_core",
          "target": "model:phi_4_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_gguf_core_uses_tech",
          "source": "model:grok_3_plus",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:gpt_5_ultra_slimpajama_plus_trained_on",
          "source": "model:gpt_5_ultra",
          "target": "dataset:slimpajama_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_grok_3_next_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:grok_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-09_google_ai_blog_9796",
              "url": "https://blog.google/technology/ai/2026/01/09/langchain_plus_grok_3_next",
              "published": "2026-01-09",
              "snippet": "langchain Plus announced official support for Grok-3 Next..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_3610",
              "url": "https://blogs.nvidia.com/2026/01/24/langchain_plus_grok_3_next",
              "published": "2026-01-24",
              "snippet": "langchain Plus announced official support for Grok-3 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_pro_copilot_plus_integrates_with",
          "source": "tool:autogpt_pro",
          "target": "tool:copilot_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_2013",
              "url": "https://nextgov.com/2026/01/25/autogpt_pro_copilot_plus",
              "published": "2026-01-25",
              "snippet": "AutoGPT Pro now supports Copilot Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_falcon_3_ultra_depends_on",
          "source": "model:jamba_2",
          "target": "model:falcon_3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:gradio_mini_speculative_decoding_core_uses_tech",
          "source": "tool:gradio_mini",
          "target": "tech:speculative_decoding_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_tool_use_uses_tech",
          "source": "model:palm_3_lite",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:mbpp_next_gpt_5_next_measures",
          "source": "benchmark:mbpp_next",
          "target": "model:gpt_5_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.97
        }
      },
      {
        "data": {
          "id": "e:llamaindex_edge_cody_ultra_integrates_with",
          "source": "tool:llamaindex_edge",
          "target": "tool:cody_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-31_arxiv_1568",
              "url": "https://arxiv.org/abs/2025/12/31/llamaindex_edge_cody_ultra",
              "published": "2025-12-31",
              "snippet": "LlamaIndex Edge now supports Cody Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_pro_gemma_3_lite_depends_on",
          "source": "model:aya_3_pro",
          "target": "model:gemma_3_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_slimpajama_pro_trained_on",
          "source": "model:nemotron_5",
          "target": "dataset:slimpajama_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_lora_core_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:lora_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-21_nextgov_9504",
              "url": "https://nextgov.com/2025/12/21/lora:_low_rank_adaptation_of_l",
              "published": "2025-12-21",
              "snippet": "Under the hood, LoRA: Low-Rank Adaptation of Large Language Models implements LoRA Core for improved efficiency..."
            },
            {
              "docId": "2026-01-14_weights_and_bia_5473",
              "url": "https://wandb.ai/articles/2026/01/14/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-14",
              "snippet": "Under the hood, LoRA: Low-Rank Adaptation of Large Language Models implements LoRA Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_sparse_attention_ultra_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:sparse_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_grok_3_lite_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:grok_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-30_microsoft_resea_6924",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/30/tensorrt_llm_grok_3_lite",
              "published": "2025-12-30",
              "snippet": "TensorRT-LLM announced official support for Grok-3 Lite..."
            },
            {
              "docId": "2026-01-13_the_verge_9373",
              "url": "https://theverge.com/2026/01/13/tensorrt_llm_grok_3_lite",
              "published": "2026-01-13",
              "snippet": "TensorRT-LLM announced official support for Grok-3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_dpo_plus_uses_tech",
          "source": "model:nemotron_5_next",
          "target": "tech:dpo_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_max_command_r_plus_max_depends_on",
          "source": "model:claude_opus_45_max",
          "target": "model:command_r_plus_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_chain_of_thought_next_uses_tech",
          "source": "model:llama_4_plus",
          "target": "tech:chain_of_thought_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-24_openai_blog_6671",
              "url": "https://openai.com/blog/2026/01/24/llama_4_plus_chain_of_thought_",
              "published": "2026-01-24",
              "snippet": "Under the hood, Llama 4 Plus implements Chain-of-Thought Next for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_rlhf_uses_tech",
          "source": "tool:cursor_mini",
          "target": "tech:rlhf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-15_nvidia_blog_7377",
              "url": "https://blogs.nvidia.com/2026/01/15/cursor_mini_rlhf",
              "published": "2026-01-15",
              "snippet": "Technical details reveal Cursor Mini relies heavily on RLHF..."
            },
            {
              "docId": "2026-01-23_bloomberg_2115",
              "url": "https://bloomberg.com/technology/2026/01/23/cursor_mini_rlhf",
              "published": "2026-01-23",
              "snippet": "Cursor Mini leverages RLHF to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_transformer_architecture_mini_uses_tech",
          "source": "repo:open_interpreter_pro",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-19_the_gradient_8843",
              "url": "https://thegradient.pub/2026/01/19/open_interpreter_pro_transform",
              "published": "2026-01-19",
              "snippet": "Under the hood, open-interpreter Pro implements Transformer Architecture Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_7045",
              "url": "https://blogs.nvidia.com/2026/01/19/open_interpreter_pro_transform",
              "published": "2026-01-19",
              "snippet": "Under the hood, open-interpreter Pro implements Transformer Architecture Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_3572",
              "url": "https://blogs.nvidia.com/2026/01/25/open_interpreter_pro_transform",
              "published": "2026-01-25",
              "snippet": "open-interpreter Pro leverages Transformer Architecture Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_1830",
              "url": "https://huggingface.co/blog/2026/01/25/open_interpreter_pro_transform",
              "published": "2026-01-25",
              "snippet": "Under the hood, open-interpreter Pro implements Transformer Architecture Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_fineweb_plus_trained_on",
          "source": "model:falcon_3",
          "target": "dataset:fineweb_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_palm_3_integrates_with",
          "source": "repo:text_generation_webui_pro",
          "target": "model:palm_3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_red_teaming_pro_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:red_teaming_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-12-31_nvidia_blog_3485",
              "url": "https://blogs.nvidia.com/2025/12/31/textbooks_are_all_you_need_min",
              "published": "2025-12-31",
              "snippet": "Technical details reveal Textbooks Are All You Need Mini relies heavily on Red Teaming Pro..."
            },
            {
              "docId": "2026-01-01_meta_ai_blog_7071",
              "url": "https://ai.meta.com/blog/2026/01/01/textbooks_are_all_you_need_min",
              "published": "2026-01-01",
              "snippet": "Technical details reveal Textbooks Are All You Need Mini relies heavily on Red Teaming Pro..."
            },
            {
              "docId": "2026-01-18_microsoft_resea_9619",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/textbooks_are_all_you_need_min",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Textbooks Are All You Need Mini relies heavily on Red Teaming Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_truthfulqa_pro_evaluated_on",
          "source": "model:sora_2_plus",
          "target": "benchmark:truthfulqa_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_2195",
              "url": "https://anthropic.com/news/2026/01/25/sora_2_plus_truthfulqa_pro",
              "published": "2026-01-25",
              "snippet": "Sora 2 Plus achieves 89% on TruthfulQA Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_haystack_next_integrates_with",
          "source": "tool:semantic_kernel_v2",
          "target": "tool:haystack_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-18_openai_blog_8343",
              "url": "https://openai.com/blog/2026/01/18/semantic_kernel_v2_haystack_ne",
              "published": "2026-01-18",
              "snippet": "The latest release of Semantic Kernel v2 adds native Haystack Next integration..."
            },
            {
              "docId": "2026-01-22_reuters_4061",
              "url": "https://reuters.com/technology/2026/01/22/semantic_kernel_v2_haystack_ne",
              "published": "2026-01-22",
              "snippet": "The latest release of Semantic Kernel v2 adds native Haystack Next integration..."
            },
            {
              "docId": "2026-01-22_wired_3147",
              "url": "https://wired.com/2026/01/22/semantic_kernel_v2_haystack_ne",
              "published": "2026-01-22",
              "snippet": "Semantic Kernel v2 now supports Haystack Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_core_kv_cache_optimization_ultra_uses_tech",
          "source": "tool:gradio_core",
          "target": "tech:kv_cache_optimization_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:common_crawl_mini_transformer_architecture_lite_uses_tech",
          "source": "dataset:common_crawl_mini",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_tool_use_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_dpo_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:dpo",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_falcon_3_pro_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "model:falcon_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_8459",
              "url": "https://openai.com/blog/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Mini achieves 82% on Falcon 3 Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_core_gpt_5_v2_integrates_with",
          "source": "tool:haystack_core",
          "target": "model:gpt_5_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-17_the_verge_8390",
              "url": "https://theverge.com/2026/01/17/haystack_core_gpt_5_v2",
              "published": "2026-01-17",
              "snippet": "Haystack Core announced official support for GPT-5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_pro_qlora_plus_uses_tech",
          "source": "model:whisper_v4_pro",
          "target": "tech:qlora_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_claude_opus_45_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:claude_opus_45",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:sora_2_pro_alpacaeval_2_evaluated_on",
          "source": "model:sora_2_pro",
          "target": "benchmark:alpacaeval_2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-03_bloomberg_3076",
              "url": "https://bloomberg.com/technology/2026/01/03/sora_2_pro_alpacaeval_2",
              "published": "2026-01-03",
              "snippet": "Sora 2 Pro achieves 97% on AlpacaEval 2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_openwebtext2_max_trained_on",
          "source": "model:qwen_3_next",
          "target": "dataset:openwebtext2_max",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_flash_attention_ultra_uses_tech",
          "source": "model:grok_3_next",
          "target": "tech:flash_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_stable_diffusion_4_lite_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:stable_diffusion_4_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:gpqa_pro_midjourney_v7_ultra_measures",
          "source": "benchmark:gpqa_pro",
          "target": "model:midjourney_v7_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-15_openai_blog_1112",
              "url": "https://openai.com/blog/2026/01/15/gpqa_pro_midjourney_v7_ultra",
              "published": "2026-01-15",
              "snippet": "GPQA Pro has become the standard for evaluating Midjourney V7 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_jamba_2_plus_integrates_with",
          "source": "repo:langchain",
          "target": "model:jamba_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-05_the_gradient_6879",
              "url": "https://thegradient.pub/2026/01/05/langchain_jamba_2_plus",
              "published": "2026-01-05",
              "snippet": "The latest release of langchain adds native Jamba 2 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_edge_kv_cache_optimization_core_uses_tech",
          "source": "dataset:mc4_edge",
          "target": "tech:kv_cache_optimization_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:autogpt_edge_phi_4_pro_integrates_with",
          "source": "tool:autogpt_edge",
          "target": "model:phi_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-22_nextgov_3737",
              "url": "https://nextgov.com/2026/01/22/autogpt_edge_phi_4_pro",
              "published": "2026-01-22",
              "snippet": "The latest release of AutoGPT Edge adds native Phi-4 Pro integration..."
            },
            {
              "docId": "2026-01-24_ars_technica_9448",
              "url": "https://arstechnica.com/2026/01/24/autogpt_edge_phi_4_pro",
              "published": "2026-01-24",
              "snippet": "AutoGPT Edge announced official support for Phi-4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_claude_opus_45_core_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:claude_opus_45_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-18_nvidia_blog_1582",
              "url": "https://blogs.nvidia.com/2025/12/18/transformers_pro_claude_opus_4",
              "published": "2025-12-18",
              "snippet": "transformers Pro now supports Claude Opus 4.5 Core with full feature parity..."
            },
            {
              "docId": "2026-01-20_techcrunch_6713",
              "url": "https://techcrunch.com/2026/01/20/transformers_pro_claude_opus_4",
              "published": "2026-01-20",
              "snippet": "The latest release of transformers Pro adds native Claude Opus 4.5 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_core_quantization_lite_uses_tech",
          "source": "model:gemini_ultra_2_core",
          "target": "tech:quantization_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:roots_max_chain_of_thought_plus_uses_tech",
          "source": "dataset:roots_max",
          "target": "tech:chain_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:palm_3_max_dpo_lite_uses_tech",
          "source": "model:palm_3_max",
          "target": "tech:dpo_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-20_techcrunch_7140",
              "url": "https://techcrunch.com/2025/12/20/palm_3_max_dpo_lite",
              "published": "2025-12-20",
              "snippet": "Technical details reveal PaLM 3 Max relies heavily on DPO Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_midjourney_v7_max_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:midjourney_v7_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-12_techcrunch_3472",
              "url": "https://techcrunch.com/2026/01/12/llm_agents:_a_survey_midjourne",
              "published": "2026-01-12",
              "snippet": "On the Midjourney V7 Max benchmark, LLM Agents: A Survey scored 72%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_pro_claude_opus_45_pro_integrates_with",
          "source": "repo:llamacpp_pro",
          "target": "model:claude_opus_45_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mixture_of_experts_uses_tech",
          "source": "model:mixtral_8x22b",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-23_the_gradient_7715",
              "url": "https://thegradient.pub/2026/01/23/mixtral_8x22b_mixture_of_exper",
              "published": "2026-01-23",
              "snippet": "Under the hood, Mixtral 8x22B implements Mixture of Experts for improved efficiency..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_3628",
              "url": "https://huggingface.co/blog/2026/01/25/mixtral_8x22b_mixture_of_exper",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Mixtral 8x22B relies heavily on Mixture of Experts..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_max_openwebtext2_edge_trained_on",
          "source": "model:gpt_4o_mini_2_max",
          "target": "dataset:openwebtext2_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:ollama_constitutional_ai_lite_uses_tech",
          "source": "repo:ollama",
          "target": "tech:constitutional_ai_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2025-12-19_nextgov_6711",
              "url": "https://nextgov.com/2025/12/19/ollama_constitutional_ai_lite",
              "published": "2025-12-19",
              "snippet": "ollama leverages Constitutional AI Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_next_claude_opus_45_plus_measures",
          "source": "benchmark:mmlu_next",
          "target": "model:claude_opus_45_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-07_venturebeat_6897",
              "url": "https://venturebeat.com/2026/01/07/mmlu_next_claude_opus_45_plus",
              "published": "2026-01-07",
              "snippet": "MMLU Next provides standardized evaluation of Claude Opus 4.5 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_quantization_mini_uses_tech",
          "source": "repo:vllm",
          "target": "tech:quantization_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-18_google_ai_blog_3841",
              "url": "https://blog.google/technology/ai/2026/01/18/vllm_quantization_mini",
              "published": "2026-01-18",
              "snippet": "Under the hood, vllm implements Quantization Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-22_weights_and_bia_9542",
              "url": "https://wandb.ai/articles/2026/01/22/vllm_quantization_mini",
              "published": "2026-01-22",
              "snippet": "vllm leverages Quantization Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_meta_ai_blog_1017",
              "url": "https://ai.meta.com/blog/2026/01/24/vllm_quantization_mini",
              "published": "2026-01-24",
              "snippet": "vllm leverages Quantization Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_phi_4_pro_depends_on",
          "source": "model:dall_e_4_core",
          "target": "model:phi_4_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__red_teaming_edge_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:red_teaming_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:vllm_plus_dify_mini_integrates_with",
          "source": "tool:vllm_plus",
          "target": "tool:dify_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-21_the_gradient_8382",
              "url": "https://thegradient.pub/2026/01/21/vllm_plus_dify_mini",
              "published": "2026-01-21",
              "snippet": "The latest release of vLLM Plus adds native Dify Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_pro_synthetic_data_generation_edge_uses_tech",
          "source": "model:codex_2_pro",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:streamlit_edge_sliding_window_attention_plus_uses_tech",
          "source": "tool:streamlit_edge",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_sora_2_max_depends_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "model:sora_2_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.4
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__yi_large_next_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:yi_large_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-10_ars_technica_8431",
              "url": "https://arstechnica.com/2026/01/10/scaling_laws_for_neural_langua",
              "published": "2026-01-10",
              "snippet": "Scaling Laws for Neural Language Models (2025) achieves 98% on Yi-Large Next, setting a new record..."
            },
            {
              "docId": "2026-01-13_ars_technica_5826",
              "url": "https://arstechnica.com/2026/01/13/scaling_laws_for_neural_langua",
              "published": "2026-01-13",
              "snippet": "On the Yi-Large Next benchmark, Scaling Laws for Neural Language Models (2025) scored 91%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_midjourney_v7_ultra_integrates_with",
          "source": "repo:ollama_ultra",
          "target": "model:midjourney_v7_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-30_openai_blog_2132",
              "url": "https://openai.com/blog/2025/12/30/ollama_ultra_midjourney_v7_ult",
              "published": "2025-12-30",
              "snippet": "ollama Ultra announced official support for Midjourney V7 Ultra..."
            },
            {
              "docId": "2026-01-14_mit_technology__8706",
              "url": "https://technologyreview.com/2026/01/14/ollama_ultra_midjourney_v7_ult",
              "published": "2026-01-14",
              "snippet": "ollama Ultra now supports Midjourney V7 Ultra with full feature parity..."
            },
            {
              "docId": "2026-01-15_reuters_3799",
              "url": "https://reuters.com/technology/2026/01/15/ollama_ultra_midjourney_v7_ult",
              "published": "2026-01-15",
              "snippet": "ollama Ultra announced official support for Midjourney V7 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_plus_yi_large_mini_measures",
          "source": "benchmark:humaneval_plus",
          "target": "model:yi_large_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-05_openai_blog_3761",
              "url": "https://openai.com/blog/2026/01/05/humaneval_plus_yi_large_mini",
              "published": "2026-01-05",
              "snippet": "HumanEval Plus has become the standard for evaluating Yi-Large Mini..."
            },
            {
              "docId": "2026-01-23_the_verge_2887",
              "url": "https://theverge.com/2026/01/23/humaneval_plus_yi_large_mini",
              "published": "2026-01-23",
              "snippet": "The HumanEval Plus benchmark measures Yi-Large Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-23_microsoft_resea_5168",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/humaneval_plus_yi_large_mini",
              "published": "2026-01-23",
              "snippet": "HumanEval Plus has become the standard for evaluating Yi-Large Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_mini_chain_of_thought_ultra_uses_tech",
          "source": "model:nemotron_5_mini",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:gpt_5_mmlu_evaluated_on",
          "source": "model:gpt_5",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-13_ars_technica_2556",
              "url": "https://arstechnica.com/2026/01/13/gpt_5_mmlu",
              "published": "2026-01-13",
              "snippet": "On the MMLU benchmark, GPT-5 scored 76%..."
            },
            {
              "docId": "2026-01-14_venturebeat_9255",
              "url": "https://venturebeat.com/2026/01/14/gpt_5_mmlu",
              "published": "2026-01-14",
              "snippet": "GPT-5 achieves 79% on MMLU, setting a new record..."
            },
            {
              "docId": "2026-01-22_the_gradient_1791",
              "url": "https://thegradient.pub/2026/01/22/gpt_5_mmlu",
              "published": "2026-01-22",
              "snippet": "Evaluation results show GPT-5 reaching 98% on MMLU..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_rlhf_core_uses_tech",
          "source": "tool:litellm",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_nemotron_5_pro_measures",
          "source": "benchmark:alpacaeval_2",
          "target": "model:nemotron_5_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-06_hugging_face_bl_1417",
              "url": "https://huggingface.co/blog/2026/01/06/alpacaeval_2_nemotron_5_pro",
              "published": "2026-01-06",
              "snippet": "AlpacaEval 2 provides standardized evaluation of Nemotron-5 Pro..."
            },
            {
              "docId": "2026-01-15_microsoft_resea_1678",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/15/alpacaeval_2_nemotron_5_pro",
              "published": "2026-01-15",
              "snippet": "The AlpacaEval 2 benchmark measures Nemotron-5 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_pro_sparse_attention_pro_uses_tech",
          "source": "repo:llamacpp_pro",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-22_arxiv_5185",
              "url": "https://arxiv.org/abs/2026/01/22/llamacpp_pro_sparse_attention_",
              "published": "2026-01-22",
              "snippet": "Under the hood, llama.cpp Pro implements Sparse Attention Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_next_gemini_ultra_2_core_integrates_with",
          "source": "tool:streamlit_next",
          "target": "model:gemini_ultra_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:transformers_codex_2_mini_integrates_with",
          "source": "repo:transformers",
          "target": "model:codex_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-19_wired_3078",
              "url": "https://wired.com/2026/01/19/transformers_codex_2_mini",
              "published": "2026-01-19",
              "snippet": "transformers now supports Codex 2 Mini with full feature parity..."
            },
            {
              "docId": "2026-01-19_microsoft_resea_6087",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/19/transformers_codex_2_mini",
              "published": "2026-01-19",
              "snippet": "The latest release of transformers adds native Codex 2 Mini integration..."
            },
            {
              "docId": "2026-01-22_the_verge_7014",
              "url": "https://theverge.com/2026/01/22/transformers_codex_2_mini",
              "published": "2026-01-22",
              "snippet": "The latest release of transformers adds native Codex 2 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_plus_retrieval_augmented_generation_uses_tech",
          "source": "repo:transformers_plus",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:fineweb_next_rlhf_max_uses_tech",
          "source": "dataset:fineweb_next",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_aya_3_pro_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "model:aya_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__3066",
              "url": "https://technologyreview.com/2026/01/24/attention_is_all_you_need_v2_v",
              "published": "2026-01-24",
              "snippet": "On the Aya 3 Pro benchmark, Attention Is All You Need v2 v2 scored 88%..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_5500",
              "url": "https://wandb.ai/articles/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 v2 achieves 82% on Aya 3 Pro, setting a new record..."
            },
            {
              "docId": "2026-01-25_arxiv_7524",
              "url": "https://arxiv.org/abs/2026/01/25/attention_is_all_you_need_v2_v",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 v2 achieves 91% on Aya 3 Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_v2_gpqa_max_evaluated_on",
          "source": "model:nemotron_5_v2",
          "target": "benchmark:gpqa_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_starcoder_data_trained_on",
          "source": "model:claude_opus_45_core",
          "target": "dataset:starcoder_data",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_rotary_position_embedding_lite_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:rotary_position_embedding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-08_arxiv_1958",
              "url": "https://arxiv.org/abs/2026/01/08/textbooks_are_all_you_need_min",
              "published": "2026-01-08",
              "snippet": "Under the hood, Textbooks Are All You Need Mini implements Rotary Position Embedding Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_constitutional_ai_edge_uses_tech",
          "source": "dataset:common_crawl",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_qwen_3_core_depends_on",
          "source": "model:deepseek_v3_max",
          "target": "model:qwen_3_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_speculative_decoding_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-18_hugging_face_bl_7487",
              "url": "https://huggingface.co/blog/2026/01/18/retrieval_augmented_generation",
              "published": "2026-01-18",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP leverages Speculative Decoding to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_rotary_position_embedding_max_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:rotary_position_embedding_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-13_mit_technology__8742",
              "url": "https://technologyreview.com/2026/01/13/retrieval_augmented_generation",
              "published": "2026-01-13",
              "snippet": "Technical details reveal Retrieval-Augmented Generation for Knowledge-Intensive NLP relies heavily on Rotary Position Embedding Max..."
            },
            {
              "docId": "2026-01-24_nextgov_6932",
              "url": "https://nextgov.com/2026/01/24/retrieval_augmented_generation",
              "published": "2026-01-24",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP leverages Rotary Position Embedding Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_ultra_distillation_mini_uses_tech",
          "source": "repo:langchain_ultra",
          "target": "tech:distillation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__7511",
              "url": "https://technologyreview.com/2026/01/24/langchain_ultra_distillation_m",
              "published": "2026-01-24",
              "snippet": "Technical details reveal langchain Ultra relies heavily on Distillation Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_langchain_plus_integrates_with",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tool:langchain_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:localai_v2_chain_of_thought_ultra_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_sliding_window_attention_mini_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:sliding_window_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_stable_diffusion_4_max_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:stable_diffusion_4_max",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:cody_deepseek_v3_mini_integrates_with",
          "source": "tool:cody",
          "target": "model:deepseek_v3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-11_langchain_blog_3640",
              "url": "https://blog.langchain.dev/2026/01/11/cody_deepseek_v3_mini",
              "published": "2026-01-11",
              "snippet": "Cody announced official support for DeepSeek-V3 Mini..."
            },
            {
              "docId": "2026-01-15_openai_blog_3422",
              "url": "https://openai.com/blog/2026/01/15/cody_deepseek_v3_mini",
              "published": "2026-01-15",
              "snippet": "Cody now supports DeepSeek-V3 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_transformer_architecture_mini_uses_tech",
          "source": "repo:ollama_mini",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2025-12-29_the_gradient_1668",
              "url": "https://thegradient.pub/2025/12/29/ollama_mini_transformer_archit",
              "published": "2025-12-29",
              "snippet": "Under the hood, ollama Mini implements Transformer Architecture Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-22_the_verge_3548",
              "url": "https://theverge.com/2026/01/22/ollama_mini_transformer_archit",
              "published": "2026-01-22",
              "snippet": "Under the hood, ollama Mini implements Transformer Architecture Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_qlora_ultra_uses_tech",
          "source": "repo:vllm_edge",
          "target": "tech:qlora_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-02_arxiv_7765",
              "url": "https://arxiv.org/abs/2026/01/02/vllm_edge_qlora_ultra",
              "published": "2026-01-02",
              "snippet": "vllm Edge leverages QLoRA Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_deepseek_v3_max_measures",
          "source": "benchmark:arc_agi",
          "target": "model:deepseek_v3_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-14_ars_technica_2149",
              "url": "https://arstechnica.com/2026/01/14/arc_agi_deepseek_v3_max",
              "published": "2026-01-14",
              "snippet": "ARC-AGI provides standardized evaluation of DeepSeek-V3 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_max_mt_bench_next_evaluated_on",
          "source": "model:command_r_plus_max",
          "target": "benchmark:mt_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2025-12-12_venturebeat_9281",
              "url": "https://venturebeat.com/2025/12/12/command_r_plus_max_mt_bench_ne",
              "published": "2025-12-12",
              "snippet": "On the MT-Bench Next benchmark, Command R+ Max scored 80%..."
            },
            {
              "docId": "2025-12-22_google_ai_blog_7481",
              "url": "https://blog.google/technology/ai/2025/12/22/command_r_plus_max_mt_bench_ne",
              "published": "2025-12-22",
              "snippet": "Evaluation results show Command R+ Max reaching 78% on MT-Bench Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_v2_ollama_lite_integrates_with",
          "source": "tool:llamaindex_v2",
          "target": "tool:ollama_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_9146",
              "url": "https://blogs.nvidia.com/2026/01/25/llamaindex_v2_ollama_lite",
              "published": "2026-01-25",
              "snippet": "LlamaIndex v2 announced official support for Ollama Lite..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_5150",
              "url": "https://blogs.nvidia.com/2026/01/25/llamaindex_v2_ollama_lite",
              "published": "2026-01-25",
              "snippet": "LlamaIndex v2 announced official support for Ollama Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_claude_sonnet_4_ultra_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "model:claude_sonnet_4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-19_wired_9313",
              "url": "https://wired.com/2026/01/19/attention_is_all_you_need_v2_v",
              "published": "2026-01-19",
              "snippet": "Attention Is All You Need v2 v2 achieves 96% on Claude Sonnet 4 Ultra, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:refinedweb_mini_transformer_architecture_core_uses_tech",
          "source": "dataset:refinedweb_mini",
          "target": "tech:transformer_architecture_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:vllm_v2_tokenizer_bpe_lite_uses_tech",
          "source": "repo:vllm_v2",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-13_hugging_face_bl_6666",
              "url": "https://huggingface.co/blog/2025/12/13/vllm_v2_tokenizer_bpe_lite",
              "published": "2025-12-13",
              "snippet": "Technical details reveal vllm v2 relies heavily on Tokenizer BPE Lite..."
            },
            {
              "docId": "2026-01-12_microsoft_resea_2092",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/12/vllm_v2_tokenizer_bpe_lite",
              "published": "2026-01-12",
              "snippet": "Under the hood, vllm v2 implements Tokenizer BPE Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_5900",
              "url": "https://blogs.nvidia.com/2026/01/22/vllm_v2_tokenizer_bpe_lite",
              "published": "2026-01-22",
              "snippet": "Technical details reveal vllm v2 relies heavily on Tokenizer BPE Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_dpo_core_uses_tech",
          "source": "model:deepseek_v3_max",
          "target": "tech:dpo_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:dify_mini_cody_ultra_integrates_with",
          "source": "tool:dify_mini",
          "target": "tool:cody_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_llama_4_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:llama_4",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-18_anthropic_blog_1169",
              "url": "https://anthropic.com/news/2025/12/18/tree_of_thoughts:_deliberate_p",
              "published": "2025-12-18",
              "snippet": "On the Llama 4 benchmark, Tree of Thoughts: Deliberate Problem Solving with LLMs scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_kv_cache_optimization_v2_uses_tech",
          "source": "repo:open_interpreter",
          "target": "tech:kv_cache_optimization_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-12_langchain_blog_5226",
              "url": "https://blog.langchain.dev/2026/01/12/open_interpreter_kv_cache_opti",
              "published": "2026-01-12",
              "snippet": "Technical details reveal open-interpreter relies heavily on KV Cache Optimization v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_aya_3_v2_integrates_with",
          "source": "tool:flowise",
          "target": "model:aya_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_grok_3_integrates_with",
          "source": "tool:litellm_mini",
          "target": "model:grok_3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_kv_cache_optimization_ultra_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:kv_cache_optimization_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-14_nvidia_blog_4688",
              "url": "https://blogs.nvidia.com/2025/12/14/ollama_v2_kv_cache_optimizatio",
              "published": "2025-12-14",
              "snippet": "Under the hood, ollama v2 implements KV Cache Optimization Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_gpt_5_lite_integrates_with",
          "source": "tool:dify",
          "target": "model:gpt_5_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_venturebeat_6513",
              "url": "https://venturebeat.com/2026/01/25/dify_gpt_5_lite",
              "published": "2026-01-25",
              "snippet": "Dify now supports GPT-5 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_lite_cody_plus_integrates_with",
          "source": "tool:dify_lite",
          "target": "tool:cody_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_chain_of_thought_max_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:chain_of_thought_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-23_langchain_blog_9588",
              "url": "https://blog.langchain.dev/2026/01/23/llm_agents:_a_survey_pro_chain",
              "published": "2026-01-23",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Chain-of-Thought Max for improved efficiency..."
            },
            {
              "docId": "2026-01-24_openai_blog_8120",
              "url": "https://openai.com/blog/2026/01/24/llm_agents:_a_survey_pro_chain",
              "published": "2026-01-24",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Chain-of-Thought Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_sparse_attention_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-06_langchain_blog_5762",
              "url": "https://blog.langchain.dev/2026/01/06/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-06",
              "snippet": "Under the hood, LoRA: Low-Rank Adaptation of Large Language Models implements Sparse Attention for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_codex_2_integrates_with",
          "source": "repo:langchain",
          "target": "model:codex_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-25_arxiv_8646",
              "url": "https://arxiv.org/abs/2026/01/25/langchain_codex_2",
              "published": "2026-01-25",
              "snippet": "langchain announced official support for Codex 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_v2_constitutional_ai_pro_uses_tech",
          "source": "dataset:mc4_v2",
          "target": "tech:constitutional_ai_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_dolma_v2_trained_on",
          "source": "model:claude_opus_45",
          "target": "dataset:dolma_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_7085",
              "url": "https://arstechnica.com/2026/01/25/claude_opus_45_dolma_v2",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 was trained on Dolma v2 comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_constitutional_ai_lite_uses_tech",
          "source": "repo:open_interpreter_core",
          "target": "tech:constitutional_ai_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-24_bloomberg_6230",
              "url": "https://bloomberg.com/technology/2026/01/24/open_interpreter_core_constitu",
              "published": "2026-01-24",
              "snippet": "Technical details reveal open-interpreter Core relies heavily on Constitutional AI Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_lora_lite_uses_tech",
          "source": "model:grok_3_next",
          "target": "tech:lora_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_c4_edge_trained_on",
          "source": "model:mixtral_8x22b_plus",
          "target": "dataset:c4_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:autogpt_max_midjourney_v7_edge_integrates_with",
          "source": "repo:autogpt_max",
          "target": "model:midjourney_v7_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_gguf_edge_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-24_openai_blog_7762",
              "url": "https://openai.com/blog/2025/12/24/attention_is_all_you_need_v2_v",
              "published": "2025-12-24",
              "snippet": "Under the hood, Attention Is All You Need v2 v2 implements GGUF Edge for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_dolma_next_trained_on",
          "source": "model:claude_sonnet_4_next",
          "target": "dataset:dolma_next",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_plus_gemini_ultra_2_core_measures",
          "source": "benchmark:bigbench_hard_plus",
          "target": "model:gemini_ultra_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-23_the_verge_4590",
              "url": "https://theverge.com/2026/01/23/bigbench_hard_plus_gemini_ultr",
              "published": "2026-01-23",
              "snippet": "BigBench Hard Plus provides standardized evaluation of Gemini Ultra 2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_command_r_plus_edge_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:command_r_plus_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-08_openai_blog_9710",
              "url": "https://openai.com/blog/2026/01/08/retrieval_augmented_generation",
              "published": "2026-01-08",
              "snippet": "Evaluation results show Retrieval-Augmented Generation for Knowledge-Intensive NLP reaching 87% on Command R+ Edge..."
            },
            {
              "docId": "2026-01-17_hugging_face_bl_8386",
              "url": "https://huggingface.co/blog/2026/01/17/retrieval_augmented_generation",
              "published": "2026-01-17",
              "snippet": "On the Command R+ Edge benchmark, Retrieval-Augmented Generation for Knowledge-Intensive NLP scored 86%..."
            },
            {
              "docId": "2026-01-22_bloomberg_3772",
              "url": "https://bloomberg.com/technology/2026/01/22/retrieval_augmented_generation",
              "published": "2026-01-22",
              "snippet": "On the Command R+ Edge benchmark, Retrieval-Augmented Generation for Knowledge-Intensive NLP scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_dall_e_4_integrates_with",
          "source": "repo:autogpt",
          "target": "model:dall_e_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-14_google_ai_blog_1875",
              "url": "https://blog.google/technology/ai/2026/01/14/autogpt_dall_e_4",
              "published": "2026-01-14",
              "snippet": "AutoGPT now supports DALL-E 4 with full feature parity..."
            },
            {
              "docId": "2026-01-24_wired_5671",
              "url": "https://wired.com/2026/01/24/autogpt_dall_e_4",
              "published": "2026-01-24",
              "snippet": "AutoGPT announced official support for DALL-E 4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_grok_3_plus_integrates_with",
          "source": "repo:ollama_mini",
          "target": "model:grok_3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-22_weights_and_bia_5210",
              "url": "https://wandb.ai/articles/2026/01/22/ollama_mini_grok_3_plus",
              "published": "2026-01-22",
              "snippet": "The latest release of ollama Mini adds native Grok-3 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_lora_v2_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:lora_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-19_reuters_2821",
              "url": "https://reuters.com/technology/2026/01/19/llm_agents:_a_survey_mini_lora",
              "published": "2026-01-19",
              "snippet": "Technical details reveal LLM Agents: A Survey Mini relies heavily on LoRA v2..."
            },
            {
              "docId": "2026-01-24_ars_technica_4359",
              "url": "https://arstechnica.com/2026/01/24/llm_agents:_a_survey_mini_lora",
              "published": "2026-01-24",
              "snippet": "LLM Agents: A Survey Mini leverages LoRA v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_mini_claude_sonnet_4_plus_integrates_with",
          "source": "tool:semantic_kernel_mini",
          "target": "model:claude_sonnet_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_pro_tokenizer_bpe_edge_uses_tech",
          "source": "model:gpt_4o_mini_2_pro",
          "target": "tech:tokenizer_bpe_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_roots_max_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:roots_max",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_7480",
              "url": "https://techcrunch.com/2026/01/25/gemma_3_mini_roots_max",
              "published": "2026-01-25",
              "snippet": "The training corpus for Gemma 3 Mini includes ROOTS Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_redpajama_v2_pro_trained_on",
          "source": "model:command_r_plus",
          "target": "dataset:redpajama_v2_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_v2_claude_sonnet_4_ultra_measures",
          "source": "benchmark:bigbench_hard_v2",
          "target": "model:claude_sonnet_4_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-24_hugging_face_bl_3287",
              "url": "https://huggingface.co/blog/2026/01/24/bigbench_hard_v2_claude_sonnet",
              "published": "2026-01-24",
              "snippet": "BigBench Hard v2 provides standardized evaluation of Claude Sonnet 4 Ultra..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_2839",
              "url": "https://blogs.nvidia.com/2026/01/24/bigbench_hard_v2_claude_sonnet",
              "published": "2026-01-24",
              "snippet": "BigBench Hard v2 provides standardized evaluation of Claude Sonnet 4 Ultra..."
            },
            {
              "docId": "2026-01-24_ars_technica_1268",
              "url": "https://arstechnica.com/2026/01/24/bigbench_hard_v2_claude_sonnet",
              "published": "2026-01-24",
              "snippet": "BigBench Hard v2 provides standardized evaluation of Claude Sonnet 4 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_core_yi_large_lite_integrates_with",
          "source": "tool:gradio_core",
          "target": "model:yi_large_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-01_the_verge_7286",
              "url": "https://theverge.com/2026/01/01/gradio_core_yi_large_lite",
              "published": "2026-01-01",
              "snippet": "Gradio Core announced official support for Yi-Large Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_plus_falcon_3_max_depends_on",
          "source": "model:nemotron_5_plus",
          "target": "model:falcon_3_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_tree_of_thought_plus_uses_tech",
          "source": "model:phi_4_lite",
          "target": "tech:tree_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_plus_red_teaming_lite_uses_tech",
          "source": "dataset:redpajama_v2_plus",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_plus_slimpajama_mini_trained_on",
          "source": "model:claude_opus_45_plus",
          "target": "dataset:slimpajama_mini",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.53,
          "evidence": [
            {
              "docId": "2026-01-04_arxiv_9198",
              "url": "https://arxiv.org/abs/2026/01/04/claude_opus_45_plus_slimpajama",
              "published": "2026-01-04",
              "snippet": "The training corpus for Claude Opus 4.5 Plus includes SlimPajama Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_plus_math_edge_evaluated_on",
          "source": "model:command_r_plus_plus",
          "target": "benchmark:math_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-12-24_nvidia_blog_1633",
              "url": "https://blogs.nvidia.com/2025/12/24/command_r_plus_plus_math_edge",
              "published": "2025-12-24",
              "snippet": "Command R+ Plus achieves 94% on MATH Edge, setting a new record..."
            },
            {
              "docId": "2026-01-25_bloomberg_8267",
              "url": "https://bloomberg.com/technology/2026/01/25/command_r_plus_plus_math_edge",
              "published": "2026-01-25",
              "snippet": "Command R+ Plus achieves 95% on MATH Edge, setting a new record..."
            },
            {
              "docId": "2026-01-25_langchain_blog_2459",
              "url": "https://blog.langchain.dev/2026/01/25/command_r_plus_plus_math_edge",
              "published": "2026-01-25",
              "snippet": "Command R+ Plus achieves 92% on MATH Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_gpt_4o_mini_2_max_integrates_with",
          "source": "repo:ollama_core",
          "target": "model:gpt_4o_mini_2_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-24_hugging_face_bl_8468",
              "url": "https://huggingface.co/blog/2026/01/24/ollama_core_gpt_4o_mini_2_max",
              "published": "2026-01-24",
              "snippet": "The latest release of ollama Core adds native GPT-4o Mini 2 Max integration..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_4789",
              "url": "https://wandb.ai/articles/2026/01/24/ollama_core_gpt_4o_mini_2_max",
              "published": "2026-01-24",
              "snippet": "ollama Core now supports GPT-4o Mini 2 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_refinedweb_lite_trained_on",
          "source": "model:grok_3_plus",
          "target": "dataset:refinedweb_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_synthetic_data_generation_pro_uses_tech",
          "source": "model:jamba_2_lite",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_mixtral_8x22b_ultra_integrates_with",
          "source": "repo:text_generation_webui_pro",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-04_hugging_face_bl_7490",
              "url": "https://huggingface.co/blog/2025/12/04/text_generation_webui_pro_mixt",
              "published": "2025-12-04",
              "snippet": "text-generation-webui Pro now supports Mixtral 8x22B Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_edge_claude_sonnet_4_integrates_with",
          "source": "tool:llamaindex_edge",
          "target": "model:claude_sonnet_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-17_nvidia_blog_4900",
              "url": "https://blogs.nvidia.com/2026/01/17/llamaindex_edge_claude_sonnet_",
              "published": "2026-01-17",
              "snippet": "LlamaIndex Edge now supports Claude Sonnet 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_max_constitutional_ai_plus_uses_tech",
          "source": "tool:gradio_max",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:vllm_max_lora_uses_tech",
          "source": "repo:vllm_max",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-18_nextgov_3989",
              "url": "https://nextgov.com/2026/01/18/vllm_max_lora",
              "published": "2026-01-18",
              "snippet": "Technical details reveal vllm Max relies heavily on LoRA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_midjourney_v7_max_depends_on",
          "source": "model:gemma_3_lite",
          "target": "model:midjourney_v7_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:refinedweb_next_multimodal_fusion_uses_tech",
          "source": "dataset:refinedweb_next",
          "target": "tech:multimodal_fusion",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_pro_qlora_edge_uses_tech",
          "source": "dataset:starcoder_data_pro",
          "target": "tech:qlora_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_edge_sora_2_ultra_evaluated_on",
          "source": "paper:direct_preference_optimization_edge",
          "target": "model:sora_2_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_chain_of_thought_uses_tech",
          "source": "repo:open_interpreter_core",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-23_arxiv_8352",
              "url": "https://arxiv.org/abs/2026/01/23/open_interpreter_core_chain_of",
              "published": "2026-01-23",
              "snippet": "Under the hood, open-interpreter Core implements Chain-of-Thought for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_palm_3_edge_integrates_with",
          "source": "tool:llamaindex_pro",
          "target": "model:palm_3_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-22_microsoft_resea_6906",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/22/llamaindex_pro_palm_3_edge",
              "published": "2026-01-22",
              "snippet": "LlamaIndex Pro now supports PaLM 3 Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_core_llama_4_core_measures",
          "source": "benchmark:math_core",
          "target": "model:llama_4_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-30_weights_and_bia_5068",
              "url": "https://wandb.ai/articles/2025/12/30/math_core_llama_4_core",
              "published": "2025-12-30",
              "snippet": "MATH Core provides standardized evaluation of Llama 4 Core..."
            },
            {
              "docId": "2026-01-13_hugging_face_bl_4041",
              "url": "https://huggingface.co/blog/2026/01/13/math_core_llama_4_core",
              "published": "2026-01-13",
              "snippet": "The MATH Core benchmark measures Llama 4 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:swe_bench_gemma_3_lite_measures",
          "source": "benchmark:swe_bench",
          "target": "model:gemma_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-24_reuters_9082",
              "url": "https://reuters.com/technology/2025/12/24/swe_bench_gemma_3_lite",
              "published": "2025-12-24",
              "snippet": "SWE-bench has become the standard for evaluating Gemma 3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_command_r_plus_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:command_r_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-10_microsoft_resea_2313",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/10/llm_agents:_a_survey_next_comm",
              "published": "2026-01-10",
              "snippet": "LLM Agents: A Survey Next achieves 96% on Command R+, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_rlhf_ultra_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-06_google_ai_blog_2185",
              "url": "https://blog.google/technology/ai/2026/01/06/gpt4all_pro_rlhf_ultra",
              "published": "2026-01-06",
              "snippet": "Technical details reveal gpt4all Pro relies heavily on RLHF Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_gemini_ultra_2_plus_integrates_with",
          "source": "repo:transformers_edge",
          "target": "model:gemini_ultra_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_7979",
              "url": "https://nextgov.com/2026/01/24/transformers_edge_gemini_ultra",
              "published": "2026-01-24",
              "snippet": "transformers Edge now supports Gemini Ultra 2 Plus with full feature parity..."
            },
            {
              "docId": "2026-01-24_nextgov_3621",
              "url": "https://nextgov.com/2026/01/24/transformers_edge_gemini_ultra",
              "published": "2026-01-24",
              "snippet": "The latest release of transformers Edge adds native Gemini Ultra 2 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_qwen_3_v2_integrates_with",
          "source": "tool:litellm_pro",
          "target": "model:qwen_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-01_anthropic_blog_8673",
              "url": "https://anthropic.com/news/2025/12/01/litellm_pro_qwen_3_v2",
              "published": "2025-12-01",
              "snippet": "LiteLLM Pro now supports Qwen-3 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_plus_synthetic_data_generation_max_uses_tech",
          "source": "dataset:the_pile_plus",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:fineweb_lite_rotary_position_embedding_v2_uses_tech",
          "source": "dataset:fineweb_lite",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_red_teaming_edge_uses_tech",
          "source": "model:palm_3_lite",
          "target": "tech:red_teaming_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:vllm_max_copilot_core_integrates_with",
          "source": "tool:vllm_max",
          "target": "tool:copilot_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2025-12-04_bloomberg_9472",
              "url": "https://bloomberg.com/technology/2025/12/04/vllm_max_copilot_core",
              "published": "2025-12-04",
              "snippet": "vLLM Max now supports Copilot Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_red_teaming_lite_uses_tech",
          "source": "repo:llamacpp_ultra",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_claude_sonnet_4_core_depends_on",
          "source": "model:midjourney_v7_core",
          "target": "model:claude_sonnet_4_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:localai_lite_falcon_3_lite_integrates_with",
          "source": "repo:localai_lite",
          "target": "model:falcon_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-15_google_ai_blog_8778",
              "url": "https://blog.google/technology/ai/2025/12/15/localai_lite_falcon_3_lite",
              "published": "2025-12-15",
              "snippet": "LocalAI Lite now supports Falcon 3 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_swe_bench_next_evaluated_on",
          "source": "model:phi_4",
          "target": "benchmark:swe_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-23_hugging_face_bl_2338",
              "url": "https://huggingface.co/blog/2026/01/23/phi_4_swe_bench_next",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Phi-4 reaching 98% on SWE-bench Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_pro_gpqa_evaluated_on",
          "source": "model:mixtral_8x22b_pro",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-02_techcrunch_8926",
              "url": "https://techcrunch.com/2026/01/02/mixtral_8x22b_pro_gpqa",
              "published": "2026-01-02",
              "snippet": "Mixtral 8x22B Pro achieves 86% on GPQA, setting a new record..."
            },
            {
              "docId": "2026-01-10_techcrunch_9369",
              "url": "https://techcrunch.com/2026/01/10/mixtral_8x22b_pro_gpqa",
              "published": "2026-01-10",
              "snippet": "Mixtral 8x22B Pro achieves 78% on GPQA, setting a new record..."
            },
            {
              "docId": "2026-01-11_bloomberg_3400",
              "url": "https://bloomberg.com/technology/2026/01/11/mixtral_8x22b_pro_gpqa",
              "published": "2026-01-11",
              "snippet": "Mixtral 8x22B Pro achieves 80% on GPQA, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_grok_3_lite_integrates_with",
          "source": "repo:langchain_edge",
          "target": "model:grok_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_max_codex_2_integrates_with",
          "source": "tool:tensorrt_llm_max",
          "target": "model:codex_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_6795",
              "url": "https://nextgov.com/2026/01/25/tensorrt_llm_max_codex_2",
              "published": "2026-01-25",
              "snippet": "The latest release of TensorRT-LLM Max adds native Codex 2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_qwen_3_integrates_with",
          "source": "tool:litellm_mini",
          "target": "model:qwen_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-25_wired_5521",
              "url": "https://wired.com/2026/01/25/litellm_mini_qwen_3",
              "published": "2026-01-25",
              "snippet": "LiteLLM Mini now supports Qwen-3 with full feature parity..."
            },
            {
              "docId": "2026-01-25_wired_5577",
              "url": "https://wired.com/2026/01/25/litellm_mini_qwen_3",
              "published": "2026-01-25",
              "snippet": "The latest release of LiteLLM Mini adds native Qwen-3 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_multimodal_fusion_pro_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:multimodal_fusion_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-09_bloomberg_2857",
              "url": "https://bloomberg.com/technology/2026/01/09/self_play_fine_tuning_for_lang",
              "published": "2026-01-09",
              "snippet": "Technical details reveal Self-Play Fine-Tuning for Language Models relies heavily on Multimodal Fusion Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_next_command_r_plus_mini_depends_on",
          "source": "model:codex_2_next",
          "target": "model:command_r_plus_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_localai_edge_integrates_with",
          "source": "tool:gradio_plus",
          "target": "tool:localai_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:falcon_3_plus_gguf_edge_uses_tech",
          "source": "model:falcon_3_plus",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_alpacaeval_2_mini_evaluated_on",
          "source": "model:phi_4_lite",
          "target": "benchmark:alpacaeval_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_venturebeat_7749",
              "url": "https://venturebeat.com/2026/01/25/phi_4_lite_alpacaeval_2_mini",
              "published": "2026-01-25",
              "snippet": "Phi-4 Lite achieves 85% on AlpacaEval 2 Mini, setting a new record..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_9855",
              "url": "https://blogs.nvidia.com/2026/01/25/phi_4_lite_alpacaeval_2_mini",
              "published": "2026-01-25",
              "snippet": "Phi-4 Lite achieves 90% on AlpacaEval 2 Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_ultra_rotary_position_embedding_v2_uses_tech",
          "source": "model:gpt_4o_mini_2_ultra",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_next_streamlit_edge_integrates_with",
          "source": "tool:weights_and_biases_next",
          "target": "tool:streamlit_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-04_anthropic_blog_4425",
              "url": "https://anthropic.com/news/2026/01/04/weights_and_biases_next_stream",
              "published": "2026-01-04",
              "snippet": "Weights & Biases Next announced official support for Streamlit Edge..."
            },
            {
              "docId": "2026-01-13_wired_7884",
              "url": "https://wired.com/2026/01/13/weights_and_biases_next_stream",
              "published": "2026-01-13",
              "snippet": "Weights & Biases Next now supports Streamlit Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_edge_refinedweb_pro_trained_on",
          "source": "model:palm_3_edge",
          "target": "dataset:refinedweb_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_gpt_5_v2_depends_on",
          "source": "model:command_r_plus",
          "target": "model:gpt_5_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:llamaindex_quantization_max_uses_tech",
          "source": "tool:llamaindex",
          "target": "tech:quantization_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_v2_winogrande_ultra_evaluated_on",
          "source": "model:gpt_4o_mini_2_v2",
          "target": "benchmark:winogrande_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-18_nvidia_blog_7155",
              "url": "https://blogs.nvidia.com/2026/01/18/gpt_4o_mini_2_v2_winogrande_ul",
              "published": "2026-01-18",
              "snippet": "On the WinoGrande Ultra benchmark, GPT-4o Mini 2 v2 scored 71%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_palm_3_v2_measures",
          "source": "benchmark:math",
          "target": "model:palm_3_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-11-19_bloomberg_2859",
              "url": "https://bloomberg.com/technology/2025/11/19/math_palm_3_v2",
              "published": "2025-11-19",
              "snippet": "MATH has become the standard for evaluating PaLM 3 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_plus_tree_of_thought_core_uses_tech",
          "source": "model:claude_opus_45_plus",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_v2_jamba_2_v2_depends_on",
          "source": "model:midjourney_v7_v2",
          "target": "model:jamba_2_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_deepseek_v3_pro_depends_on",
          "source": "model:whisper_v4_core",
          "target": "model:deepseek_v3_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_core_stable_diffusion_4_edge_depends_on",
          "source": "model:nemotron_5_core",
          "target": "model:stable_diffusion_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:codex_2_plus_math_evaluated_on",
          "source": "model:codex_2_plus",
          "target": "benchmark:math",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-11-18_the_gradient_1084",
              "url": "https://thegradient.pub/2025/11/18/codex_2_plus_math",
              "published": "2025-11-18",
              "snippet": "Evaluation results show Codex 2 Plus reaching 97% on MATH..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_next_claude_sonnet_4_v2_integrates_with",
          "source": "tool:haystack_next",
          "target": "model:claude_sonnet_4_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_llama_4_pro_integrates_with",
          "source": "repo:text_generation_webui_next",
          "target": "model:llama_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_3840",
              "url": "https://anthropic.com/news/2026/01/24/text_generation_webui_next_lla",
              "published": "2026-01-24",
              "snippet": "The latest release of text-generation-webui Next adds native Llama 4 Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_gguf_core_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_qwen_3_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:qwen_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_8518",
              "url": "https://blogs.nvidia.com/2026/01/25/transformers_pro_qwen_3",
              "published": "2026-01-25",
              "snippet": "transformers Pro announced official support for Qwen-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_gemini_ultra_2_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:gemini_ultra_2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-09_hugging_face_bl_9128",
              "url": "https://huggingface.co/blog/2026/01/09/llm_agents:_a_survey_gemini_ul",
              "published": "2026-01-09",
              "snippet": "LLM Agents: A Survey achieves 78% on Gemini Ultra 2, setting a new record..."
            },
            {
              "docId": "2026-01-22_anthropic_blog_9491",
              "url": "https://anthropic.com/news/2026/01/22/llm_agents:_a_survey_gemini_ul",
              "published": "2026-01-22",
              "snippet": "On the Gemini Ultra 2 benchmark, LLM Agents: A Survey scored 86%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_pro_retrieval_augmented_generation_ultra_uses_tech",
          "source": "dataset:fineweb_pro",
          "target": "tech:retrieval_augmented_generation_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.4
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_constitutional_ai_edge_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:constitutional_ai_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-13_the_verge_7446",
              "url": "https://theverge.com/2026/01/13/llamacpp_mini_constitutional_a",
              "published": "2026-01-13",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Constitutional AI Edge..."
            },
            {
              "docId": "2026-01-13_google_ai_blog_5903",
              "url": "https://blog.google/technology/ai/2026/01/13/llamacpp_mini_constitutional_a",
              "published": "2026-01-13",
              "snippet": "Under the hood, llama.cpp Mini implements Constitutional AI Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-15_the_verge_6088",
              "url": "https://theverge.com/2026/01/15/llamacpp_mini_constitutional_a",
              "published": "2026-01-15",
              "snippet": "Under the hood, llama.cpp Mini implements Constitutional AI Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-20_nvidia_blog_7694",
              "url": "https://blogs.nvidia.com/2026/01/20/llamacpp_mini_constitutional_a",
              "published": "2026-01-20",
              "snippet": "llama.cpp Mini leverages Constitutional AI Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_next_tool_use_mini_uses_tech",
          "source": "dataset:wikipedia_dump_2025_next",
          "target": "tech:tool_use_mini",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:vllm_core_sliding_window_attention_v2_uses_tech",
          "source": "repo:vllm_core",
          "target": "tech:sliding_window_attention_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-15_bloomberg_9244",
              "url": "https://bloomberg.com/technology/2026/01/15/vllm_core_sliding_window_atten",
              "published": "2026-01-15",
              "snippet": "Technical details reveal vllm Core relies heavily on Sliding Window Attention v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_deepseek_v3_next_depends_on",
          "source": "model:qwen_3_next",
          "target": "model:deepseek_v3_next",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_alpacaeval_2_lite_evaluated_on",
          "source": "model:codex_2_core",
          "target": "benchmark:alpacaeval_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-12_reuters_6935",
              "url": "https://reuters.com/technology/2026/01/12/codex_2_core_alpacaeval_2_lite",
              "published": "2026-01-12",
              "snippet": "Evaluation results show Codex 2 Core reaching 93% on AlpacaEval 2 Lite..."
            },
            {
              "docId": "2026-01-18_mit_technology__4943",
              "url": "https://technologyreview.com/2026/01/18/codex_2_core_alpacaeval_2_lite",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Codex 2 Core reaching 95% on AlpacaEval 2 Lite..."
            },
            {
              "docId": "2026-01-24_wired_4439",
              "url": "https://wired.com/2026/01/24/codex_2_core_alpacaeval_2_lite",
              "published": "2026-01-24",
              "snippet": "Codex 2 Core achieves 97% on AlpacaEval 2 Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_v2_laion_5b_ultra_trained_on",
          "source": "model:deepseek_v3_v2",
          "target": "dataset:laion_5b_ultra",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-02_nvidia_blog_1416",
              "url": "https://blogs.nvidia.com/2026/01/02/deepseek_v3_v2_laion_5b_ultra",
              "published": "2026-01-02",
              "snippet": "The training corpus for DeepSeek-V3 v2 includes LAION-5B Ultra..."
            },
            {
              "docId": "2026-01-05_nextgov_4390",
              "url": "https://nextgov.com/2026/01/05/deepseek_v3_v2_laion_5b_ultra",
              "published": "2026-01-05",
              "snippet": "DeepSeek-V3 v2 was trained on LAION-5B Ultra comprising billions of tokens..."
            },
            {
              "docId": "2026-01-19_ars_technica_7120",
              "url": "https://arstechnica.com/2026/01/19/deepseek_v3_v2_laion_5b_ultra",
              "published": "2026-01-19",
              "snippet": "DeepSeek-V3 v2 utilized LAION-5B Ultra as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_rotary_position_embedding_plus_uses_tech",
          "source": "model:gemini_ultra_2_pro",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.64,
          "evidence": [
            {
              "docId": "2025-12-28_techcrunch_6460",
              "url": "https://techcrunch.com/2025/12/28/gemini_ultra_2_pro_rotary_posi",
              "published": "2025-12-28",
              "snippet": "Technical details reveal Gemini Ultra 2 Pro relies heavily on Rotary Position Embedding Plus..."
            },
            {
              "docId": "2026-01-06_the_gradient_7884",
              "url": "https://thegradient.pub/2026/01/06/gemini_ultra_2_pro_rotary_posi",
              "published": "2026-01-06",
              "snippet": "Under the hood, Gemini Ultra 2 Pro implements Rotary Position Embedding Plus for improved efficiency..."
            },
            {
              "docId": "2026-01-12_microsoft_resea_2844",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/12/gemini_ultra_2_pro_rotary_posi",
              "published": "2026-01-12",
              "snippet": "Under the hood, Gemini Ultra 2 Pro implements Rotary Position Embedding Plus for improved efficiency..."
            },
            {
              "docId": "2026-01-21_weights_and_bia_1732",
              "url": "https://wandb.ai/articles/2026/01/21/gemini_ultra_2_pro_rotary_posi",
              "published": "2026-01-21",
              "snippet": "Gemini Ultra 2 Pro leverages Rotary Position Embedding Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_edge_rotary_position_embedding_core_uses_tech",
          "source": "model:gemma_3_edge",
          "target": "tech:rotary_position_embedding_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:llamaindex_max_semantic_kernel_max_integrates_with",
          "source": "tool:llamaindex_max",
          "target": "tool:semantic_kernel_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-09_bloomberg_5334",
              "url": "https://bloomberg.com/technology/2026/01/09/llamaindex_max_semantic_kernel",
              "published": "2026-01-09",
              "snippet": "LlamaIndex Max announced official support for Semantic Kernel Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_max_mc4_plus_trained_on",
          "source": "model:midjourney_v7_max",
          "target": "dataset:mc4_plus",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_v2_group_query_attention_uses_tech",
          "source": "dataset:wikipedia_dump_2025_v2",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_command_r_plus_edge_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "model:command_r_plus_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-20_venturebeat_7227",
              "url": "https://venturebeat.com/2026/01/20/attention_is_all_you_need_v2_n",
              "published": "2026-01-20",
              "snippet": "Attention Is All You Need v2 Next achieves 70% on Command R+ Edge, setting a new record..."
            },
            {
              "docId": "2026-01-23_microsoft_resea_8465",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/attention_is_all_you_need_v2_n",
              "published": "2026-01-23",
              "snippet": "Attention Is All You Need v2 Next achieves 78% on Command R+ Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_core_command_r_plus_core_integrates_with",
          "source": "tool:cursor_core",
          "target": "model:command_r_plus_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_3025",
              "url": "https://anthropic.com/news/2026/01/24/cursor_core_command_r_plus_cor",
              "published": "2026-01-24",
              "snippet": "Cursor Core announced official support for Command R+ Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_lite_openwebtext2_edge_trained_on",
          "source": "model:command_r_plus_lite",
          "target": "dataset:openwebtext2_edge",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_deepseek_v3_v2_integrates_with",
          "source": "tool:tensorrt_llm_v2",
          "target": "model:deepseek_v3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_edge_sora_2_mini_measures",
          "source": "benchmark:truthfulqa_edge",
          "target": "model:sora_2_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.96
        }
      },
      {
        "data": {
          "id": "e:llama_4_ultra_math_next_evaluated_on",
          "source": "model:llama_4_ultra",
          "target": "benchmark:math_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-22_weights_and_bia_4494",
              "url": "https://wandb.ai/articles/2025/12/22/llama_4_ultra_math_next",
              "published": "2025-12-22",
              "snippet": "On the MATH Next benchmark, Llama 4 Ultra scored 85%..."
            },
            {
              "docId": "2026-01-02_google_ai_blog_9422",
              "url": "https://blog.google/technology/ai/2026/01/02/llama_4_ultra_math_next",
              "published": "2026-01-02",
              "snippet": "On the MATH Next benchmark, Llama 4 Ultra scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_plus_speculative_decoding_ultra_uses_tech",
          "source": "dataset:fineweb_plus",
          "target": "tech:speculative_decoding_ultra",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_gguf_pro_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:gguf_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__deepseek_v3_pro_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:deepseek_v3_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_edge_gemma_3_lite_integrates_with",
          "source": "tool:tensorrt_llm_edge",
          "target": "model:gemma_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_mt_bench_next_evaluated_on",
          "source": "model:nemotron_5",
          "target": "benchmark:mt_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:dify_core_command_r_plus_integrates_with",
          "source": "tool:dify_core",
          "target": "model:command_r_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-08_anthropic_blog_1489",
              "url": "https://anthropic.com/news/2025/12/08/dify_core_command_r_plus",
              "published": "2025-12-08",
              "snippet": "The latest release of Dify Core adds native Command R+ integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_max_synthetic_data_generation_core_uses_tech",
          "source": "dataset:the_stack_v2_max",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:copilot_core_gpt_4o_mini_2_lite_integrates_with",
          "source": "tool:copilot_core",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-16_mit_technology__4920",
              "url": "https://technologyreview.com/2025/12/16/copilot_core_gpt_4o_mini_2_lit",
              "published": "2025-12-16",
              "snippet": "Copilot Core now supports GPT-4o Mini 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_synthetic_data_generation_lite_uses_tech",
          "source": "repo:gpt4all",
          "target": "tech:synthetic_data_generation_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_distillation_lite_uses_tech",
          "source": "paper:direct_preference_optimization_pro",
          "target": "tech:distillation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-20_reuters_6713",
              "url": "https://reuters.com/technology/2026/01/20/direct_preference_optimization",
              "published": "2026-01-20",
              "snippet": "Direct Preference Optimization Pro leverages Distillation Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_arxiv_6580",
              "url": "https://arxiv.org/abs/2026/01/24/direct_preference_optimization",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Direct Preference Optimization Pro relies heavily on Distillation Lite..."
            },
            {
              "docId": "2026-01-25_techcrunch_2854",
              "url": "https://techcrunch.com/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Direct Preference Optimization Pro relies heavily on Distillation Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_edge_yi_large_mini_measures",
          "source": "benchmark:math_edge",
          "target": "model:yi_large_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-07_nvidia_blog_6971",
              "url": "https://blogs.nvidia.com/2026/01/07/math_edge_yi_large_mini",
              "published": "2026-01-07",
              "snippet": "MATH Edge has become the standard for evaluating Yi-Large Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_edge_nemotron_5_lite_measures",
          "source": "benchmark:mt_bench_edge",
          "target": "model:nemotron_5_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-12-24_nvidia_blog_8498",
              "url": "https://blogs.nvidia.com/2025/12/24/mt_bench_edge_nemotron_5_lite",
              "published": "2025-12-24",
              "snippet": "MT-Bench Edge has become the standard for evaluating Nemotron-5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_edge_gsm8k_v2_evaluated_on",
          "source": "model:nemotron_5_edge",
          "target": "benchmark:gsm8k_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_nvidia_blog_8792",
              "url": "https://blogs.nvidia.com/2026/01/25/nemotron_5_edge_gsm8k_v2",
              "published": "2026-01-25",
              "snippet": "Nemotron-5 Edge achieves 85% on GSM8K v2, setting a new record..."
            },
            {
              "docId": "2026-01-25_the_gradient_1459",
              "url": "https://thegradient.pub/2026/01/25/nemotron_5_edge_gsm8k_v2",
              "published": "2026-01-25",
              "snippet": "On the GSM8K v2 benchmark, Nemotron-5 Edge scored 83%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_flash_attention_edge_uses_tech",
          "source": "model:aya_3",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:mlflow_ultra_weights_and_biases_edge_integrates_with",
          "source": "tool:mlflow_ultra",
          "target": "tool:weights_and_biases_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:mlflow_plus_dall_e_4_mini_integrates_with",
          "source": "tool:mlflow_plus",
          "target": "model:dall_e_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-10_openai_blog_9473",
              "url": "https://openai.com/blog/2026/01/10/mlflow_plus_dall_e_4_mini",
              "published": "2026-01-10",
              "snippet": "MLflow Plus now supports DALL-E 4 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_whisper_v4_lite_integrates_with",
          "source": "repo:vllm_core",
          "target": "model:whisper_v4_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-19_techcrunch_7870",
              "url": "https://techcrunch.com/2026/01/19/vllm_core_whisper_v4_lite",
              "published": "2026-01-19",
              "snippet": "vllm Core now supports Whisper v4 Lite with full feature parity..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_9292",
              "url": "https://ai.meta.com/blog/2026/01/25/vllm_core_whisper_v4_lite",
              "published": "2026-01-25",
              "snippet": "vllm Core announced official support for Whisper v4 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_max_sora_2_edge_measures",
          "source": "benchmark:arc_agi_max",
          "target": "model:sora_2_edge",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_command_r_plus_core_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:command_r_plus_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_speculative_decoding_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-05_weights_and_bia_4234",
              "url": "https://wandb.ai/articles/2026/01/05/llm_agents:_a_survey_pro_specu",
              "published": "2026-01-05",
              "snippet": "LLM Agents: A Survey Pro leverages Speculative Decoding to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_techcrunch_7461",
              "url": "https://techcrunch.com/2026/01/23/llm_agents:_a_survey_pro_specu",
              "published": "2026-01-23",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Speculative Decoding for improved efficiency..."
            },
            {
              "docId": "2026-01-23_wired_9586",
              "url": "https://wired.com/2026/01/23/llm_agents:_a_survey_pro_specu",
              "published": "2026-01-23",
              "snippet": "Technical details reveal LLM Agents: A Survey Pro relies heavily on Speculative Decoding..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_group_query_attention_core_uses_tech",
          "source": "model:jamba_2_lite",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_multimodal_fusion_edge_uses_tech",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-03_the_gradient_9198",
              "url": "https://thegradient.pub/2025/12/03/llm_agents:_a_survey_mini_mult",
              "published": "2025-12-03",
              "snippet": "LLM Agents: A Survey Mini leverages Multimodal Fusion Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-16_the_verge_3204",
              "url": "https://theverge.com/2025/12/16/llm_agents:_a_survey_mini_mult",
              "published": "2025-12-16",
              "snippet": "Technical details reveal LLM Agents: A Survey Mini relies heavily on Multimodal Fusion Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_edge_alpacaeval_2_plus_evaluated_on",
          "source": "model:gemma_3_edge",
          "target": "benchmark:alpacaeval_2_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2026-01-18_bloomberg_7130",
              "url": "https://bloomberg.com/technology/2026/01/18/gemma_3_edge_alpacaeval_2_plus",
              "published": "2026-01-18",
              "snippet": "On the AlpacaEval 2 Plus benchmark, Gemma 3 Edge scored 70%..."
            },
            {
              "docId": "2026-01-23_weights_and_bia_8991",
              "url": "https://wandb.ai/articles/2026/01/23/gemma_3_edge_alpacaeval_2_plus",
              "published": "2026-01-23",
              "snippet": "Gemma 3 Edge achieves 89% on AlpacaEval 2 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_sora_2_core_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:sora_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-22_nvidia_blog_4328",
              "url": "https://blogs.nvidia.com/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Direct Preference Optimization Pro reaching 71% on Sora 2 Core..."
            },
            {
              "docId": "2026-01-23_the_verge_5992",
              "url": "https://theverge.com/2026/01/23/direct_preference_optimization",
              "published": "2026-01-23",
              "snippet": "Direct Preference Optimization Pro achieves 77% on Sora 2 Core, setting a new record..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_5204",
              "url": "https://blogs.nvidia.com/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "On the Sora 2 Core benchmark, Direct Preference Optimization Pro scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_mlflow_max_integrates_with",
          "source": "tool:tensorrt_llm_v2",
          "target": "tool:mlflow_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-23_nextgov_5241",
              "url": "https://nextgov.com/2026/01/23/tensorrt_llm_v2_mlflow_max",
              "published": "2026-01-23",
              "snippet": "TensorRT-LLM v2 announced official support for MLflow Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_max_gpqa_pro_evaluated_on",
          "source": "model:falcon_3_max",
          "target": "benchmark:gpqa_pro",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:langchain_rotary_position_embedding_v2_uses_tech",
          "source": "repo:langchain",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-26_anthropic_blog_4148",
              "url": "https://anthropic.com/news/2025/12/26/langchain_rotary_position_embe",
              "published": "2025-12-26",
              "snippet": "Technical details reveal langchain relies heavily on Rotary Position Embedding v2..."
            },
            {
              "docId": "2026-01-05_wired_4591",
              "url": "https://wired.com/2026/01/05/langchain_rotary_position_embe",
              "published": "2026-01-05",
              "snippet": "Under the hood, langchain implements Rotary Position Embedding v2 for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_core_transformer_architecture_mini_uses_tech",
          "source": "dataset:the_pile_core",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_codex_2_integrates_with",
          "source": "repo:langchain_edge",
          "target": "model:codex_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_meta_ai_blog_1374",
              "url": "https://ai.meta.com/blog/2026/01/25/langchain_edge_codex_2",
              "published": "2026-01-25",
              "snippet": "langchain Edge now supports Codex 2 with full feature parity..."
            },
            {
              "docId": "2026-01-25_techcrunch_6094",
              "url": "https://techcrunch.com/2026/01/25/langchain_edge_codex_2",
              "published": "2026-01-25",
              "snippet": "langchain Edge announced official support for Codex 2..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_2564",
              "url": "https://blogs.nvidia.com/2026/01/25/langchain_edge_codex_2",
              "published": "2026-01-25",
              "snippet": "langchain Edge announced official support for Codex 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_sliding_window_attention_mini_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:sliding_window_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:mbpp_lite_sora_2_mini_measures",
          "source": "benchmark:mbpp_lite",
          "target": "model:sora_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_2092",
              "url": "https://nextgov.com/2026/01/24/mbpp_lite_sora_2_mini",
              "published": "2026-01-24",
              "snippet": "The MBPP Lite benchmark measures Sora 2 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-24_wired_8454",
              "url": "https://wired.com/2026/01/24/mbpp_lite_sora_2_mini",
              "published": "2026-01-24",
              "snippet": "MBPP Lite has become the standard for evaluating Sora 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_sliding_window_attention_next_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__sliding_window_attention_ultra_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:sliding_window_attention_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-15_weights_and_bia_8312",
              "url": "https://wandb.ai/articles/2026/01/15/scaling_laws_for_neural_langua",
              "published": "2026-01-15",
              "snippet": "Scaling Laws for Neural Language Models (2025) leverages Sliding Window Attention Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-17_microsoft_resea_3539",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/17/scaling_laws_for_neural_langua",
              "published": "2026-01-17",
              "snippet": "Technical details reveal Scaling Laws for Neural Language Models (2025) relies heavily on Sliding Window Attention Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_pro_tool_use_mini_uses_tech",
          "source": "repo:text_generation_webui_pro",
          "target": "tech:tool_use_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_6496",
              "url": "https://openai.com/blog/2026/01/25/text_generation_webui_pro_tool",
              "published": "2026-01-25",
              "snippet": "Under the hood, text-generation-webui Pro implements Tool Use Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_sparse_attention_mini_uses_tech",
          "source": "tool:langchain",
          "target": "tech:sparse_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_core_kv_cache_optimization_pro_uses_tech",
          "source": "dataset:redpajama_v2_core",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_claude_opus_45_edge_depends_on",
          "source": "model:stable_diffusion_4_next",
          "target": "model:claude_opus_45_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_dall_e_4_core_measures",
          "source": "benchmark:truthfulqa",
          "target": "model:dall_e_4_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2025-12-11_microsoft_resea_7780",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/11/truthfulqa_dall_e_4_core",
              "published": "2025-12-11",
              "snippet": "TruthfulQA has become the standard for evaluating DALL-E 4 Core..."
            },
            {
              "docId": "2026-01-08_the_gradient_2373",
              "url": "https://thegradient.pub/2026/01/08/truthfulqa_dall_e_4_core",
              "published": "2026-01-08",
              "snippet": "The TruthfulQA benchmark measures DALL-E 4 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_next_tokenizer_bpe_core_uses_tech",
          "source": "dataset:laion_5b_next",
          "target": "tech:tokenizer_bpe_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:copilot_next_weights_and_biases_next_integrates_with",
          "source": "tool:copilot_next",
          "target": "tool:weights_and_biases_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:gpqa_pro_phi_4_pro_measures",
          "source": "benchmark:gpqa_pro",
          "target": "model:phi_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-12_mit_technology__2081",
              "url": "https://technologyreview.com/2025/12/12/gpqa_pro_phi_4_pro",
              "published": "2025-12-12",
              "snippet": "GPQA Pro provides standardized evaluation of Phi-4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_gemma_3_plus_integrates_with",
          "source": "repo:ollama",
          "target": "model:gemma_3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_7622",
              "url": "https://nextgov.com/2026/01/24/ollama_gemma_3_plus",
              "published": "2026-01-24",
              "snippet": "ollama announced official support for Gemma 3 Plus..."
            },
            {
              "docId": "2026-01-25_mit_technology__4695",
              "url": "https://technologyreview.com/2026/01/25/ollama_gemma_3_plus",
              "published": "2026-01-25",
              "snippet": "ollama now supports Gemma 3 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_tree_of_thought_core_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:tree_of_thought_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_tree_of_thought_mini_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:tree_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-21_wired_5198",
              "url": "https://wired.com/2026/01/21/llm_agents:_a_survey_pro_tree_",
              "published": "2026-01-21",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Tree of Thought Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_9763",
              "url": "https://blogs.nvidia.com/2026/01/22/llm_agents:_a_survey_pro_tree_",
              "published": "2026-01-22",
              "snippet": "Technical details reveal LLM Agents: A Survey Pro relies heavily on Tree of Thought Mini..."
            },
            {
              "docId": "2026-01-24_techcrunch_8208",
              "url": "https://techcrunch.com/2026/01/24/llm_agents:_a_survey_pro_tree_",
              "published": "2026-01-24",
              "snippet": "Technical details reveal LLM Agents: A Survey Pro relies heavily on Tree of Thought Mini..."
            },
            {
              "docId": "2026-01-25_the_gradient_8669",
              "url": "https://thegradient.pub/2026/01/25/llm_agents:_a_survey_pro_tree_",
              "published": "2026-01-25",
              "snippet": "LLM Agents: A Survey Pro leverages Tree of Thought Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_speculative_decoding_max_uses_tech",
          "source": "repo:langchain",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_cody_pro_integrates_with",
          "source": "tool:ollama_lite",
          "target": "tool:cody_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-18_nvidia_blog_2969",
              "url": "https://blogs.nvidia.com/2026/01/18/ollama_lite_cody_pro",
              "published": "2026-01-18",
              "snippet": "The latest release of Ollama Lite adds native Cody Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_mixtral_8x22b_edge_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:mixtral_8x22b_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-22_langchain_blog_2046",
              "url": "https://blog.langchain.dev/2025/12/22/vllm_pro_mixtral_8x22b_edge",
              "published": "2025-12-22",
              "snippet": "The latest release of vllm Pro adds native Mixtral 8x22B Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:refinedweb_ultra_tokenizer_bpe_next_uses_tech",
          "source": "dataset:refinedweb_ultra",
          "target": "tech:tokenizer_bpe_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_rotary_position_embedding_plus_uses_tech",
          "source": "repo:open_interpreter",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-23_ars_technica_7707",
              "url": "https://arstechnica.com/2026/01/23/open_interpreter_rotary_positi",
              "published": "2026-01-23",
              "snippet": "Technical details reveal open-interpreter relies heavily on Rotary Position Embedding Plus..."
            },
            {
              "docId": "2026-01-23_wired_1687",
              "url": "https://wired.com/2026/01/23/open_interpreter_rotary_positi",
              "published": "2026-01-23",
              "snippet": "Technical details reveal open-interpreter relies heavily on Rotary Position Embedding Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_lite_arc_agi_core_evaluated_on",
          "source": "model:yi_large_lite",
          "target": "benchmark:arc_agi_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-05_reuters_3475",
              "url": "https://reuters.com/technology/2026/01/05/yi_large_lite_arc_agi_core",
              "published": "2026-01-05",
              "snippet": "Evaluation results show Yi-Large Lite reaching 92% on ARC-AGI Core..."
            },
            {
              "docId": "2026-01-13_arxiv_9111",
              "url": "https://arxiv.org/abs/2026/01/13/yi_large_lite_arc_agi_core",
              "published": "2026-01-13",
              "snippet": "On the ARC-AGI Core benchmark, Yi-Large Lite scored 91%..."
            },
            {
              "docId": "2026-01-25_the_gradient_8063",
              "url": "https://thegradient.pub/2026/01/25/yi_large_lite_arc_agi_core",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Yi-Large Lite reaching 71% on ARC-AGI Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_codex_2_mini_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:codex_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:math_edge_stable_diffusion_4_lite_measures",
          "source": "benchmark:math_edge",
          "target": "model:stable_diffusion_4_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-18_openai_blog_5365",
              "url": "https://openai.com/blog/2026/01/18/math_edge_stable_diffusion_4_l",
              "published": "2026-01-18",
              "snippet": "MATH Edge has become the standard for evaluating Stable Diffusion 4 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_dpo_max_uses_tech",
          "source": "model:sora_2_plus",
          "target": "tech:dpo_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:copilot_max_rlhf_pro_uses_tech",
          "source": "tool:copilot_max",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-22_nvidia_blog_9579",
              "url": "https://blogs.nvidia.com/2026/01/22/copilot_max_rlhf_pro",
              "published": "2026-01-22",
              "snippet": "Copilot Max leverages RLHF Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-22_meta_ai_blog_8223",
              "url": "https://ai.meta.com/blog/2026/01/22/copilot_max_rlhf_pro",
              "published": "2026-01-22",
              "snippet": "Under the hood, Copilot Max implements RLHF Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-23_hugging_face_bl_4047",
              "url": "https://huggingface.co/blog/2026/01/23/copilot_max_rlhf_pro",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Copilot Max relies heavily on RLHF Pro..."
            },
            {
              "docId": "2026-01-23_arxiv_2014",
              "url": "https://arxiv.org/abs/2026/01/23/copilot_max_rlhf_pro",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Copilot Max relies heavily on RLHF Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_ultra_group_query_attention_lite_uses_tech",
          "source": "dataset:c4_ultra",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:yi_large_pro_jamba_2_next_depends_on",
          "source": "model:yi_large_pro",
          "target": "model:jamba_2_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:arc_agi_core_falcon_3_core_measures",
          "source": "benchmark:arc_agi_core",
          "target": "model:falcon_3_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_9092",
              "url": "https://blog.google/technology/ai/2026/01/22/arc_agi_core_falcon_3_core",
              "published": "2026-01-22",
              "snippet": "The ARC-AGI Core benchmark measures Falcon 3 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_core_vllm_plus_integrates_with",
          "source": "tool:haystack_core",
          "target": "tool:vllm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-11-20_weights_and_bia_9356",
              "url": "https://wandb.ai/articles/2025/11/20/haystack_core_vllm_plus",
              "published": "2025-11-20",
              "snippet": "Haystack Core now supports vLLM Plus with full feature parity..."
            },
            {
              "docId": "2025-12-27_techcrunch_9063",
              "url": "https://techcrunch.com/2025/12/27/haystack_core_vllm_plus",
              "published": "2025-12-27",
              "snippet": "Haystack Core announced official support for vLLM Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_rotary_position_embedding_lite_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:rotary_position_embedding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:arc_agi_edge_llama_4_plus_measures",
          "source": "benchmark:arc_agi_edge",
          "target": "model:llama_4_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-18_hugging_face_bl_9800",
              "url": "https://huggingface.co/blog/2026/01/18/arc_agi_edge_llama_4_plus",
              "published": "2026-01-18",
              "snippet": "The ARC-AGI Edge benchmark measures Llama 4 Plus across multiple tasks..."
            },
            {
              "docId": "2026-01-24_techcrunch_1048",
              "url": "https://techcrunch.com/2026/01/24/arc_agi_edge_llama_4_plus",
              "published": "2026-01-24",
              "snippet": "The ARC-AGI Edge benchmark measures Llama 4 Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_redpajama_v2_plus_trained_on",
          "source": "model:palm_3_lite",
          "target": "dataset:redpajama_v2_plus",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_the_gradient_3757",
              "url": "https://thegradient.pub/2026/01/22/palm_3_lite_redpajama_v2_plus",
              "published": "2026-01-22",
              "snippet": "PaLM 3 Lite was trained on RedPajama v2 Plus comprising billions of tokens..."
            },
            {
              "docId": "2026-01-22_techcrunch_4356",
              "url": "https://techcrunch.com/2026/01/22/palm_3_lite_redpajama_v2_plus",
              "published": "2026-01-22",
              "snippet": "PaLM 3 Lite was trained on RedPajama v2 Plus comprising billions of tokens..."
            },
            {
              "docId": "2026-01-23_venturebeat_4194",
              "url": "https://venturebeat.com/2026/01/23/palm_3_lite_redpajama_v2_plus",
              "published": "2026-01-23",
              "snippet": "PaLM 3 Lite was trained on RedPajama v2 Plus comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_v2_deepseek_v3_ultra_depends_on",
          "source": "model:gpt_4o_mini_2_v2",
          "target": "model:deepseek_v3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_synthetic_data_generation_mini_uses_tech",
          "source": "model:llama_4_plus",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:c4_core_kv_cache_optimization_core_uses_tech",
          "source": "dataset:c4_core",
          "target": "tech:kv_cache_optimization_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_mixtral_8x22b_edge_evaluated_on",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "model:mixtral_8x22b_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_gemini_ultra_2_core_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:gemini_ultra_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:slimpajama_plus_group_query_attention_lite_uses_tech",
          "source": "dataset:slimpajama_plus",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_v2_grok_3_plus_integrates_with",
          "source": "tool:weights_and_biases_v2",
          "target": "model:grok_3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-11-22_reuters_2686",
              "url": "https://reuters.com/technology/2025/11/22/weights_and_biases_v2_grok_3_p",
              "published": "2025-11-22",
              "snippet": "Weights & Biases v2 now supports Grok-3 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_group_query_attention_uses_tech",
          "source": "dataset:starcoder_data",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_ultra_dolma_core_trained_on",
          "source": "model:dall_e_4_ultra",
          "target": "dataset:dolma_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_codex_2_pro_depends_on",
          "source": "model:deepseek_v3_mini",
          "target": "model:codex_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_speculative_decoding_max_uses_tech",
          "source": "model:stable_diffusion_4_next",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_next_tokenizer_bpe_edge_uses_tech",
          "source": "dataset:redpajama_v2_next",
          "target": "tech:tokenizer_bpe_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_gguf_core_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:palm_3_v2_truthfulqa_pro_evaluated_on",
          "source": "model:palm_3_v2",
          "target": "benchmark:truthfulqa_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-24_mit_technology__4333",
              "url": "https://technologyreview.com/2025/12/24/palm_3_v2_truthfulqa_pro",
              "published": "2025-12-24",
              "snippet": "On the TruthfulQA Pro benchmark, PaLM 3 v2 scored 91%..."
            },
            {
              "docId": "2025-12-27_ars_technica_3818",
              "url": "https://arstechnica.com/2025/12/27/palm_3_v2_truthfulqa_pro",
              "published": "2025-12-27",
              "snippet": "PaLM 3 v2 achieves 85% on TruthfulQA Pro, setting a new record..."
            },
            {
              "docId": "2026-01-25_ars_technica_4310",
              "url": "https://arstechnica.com/2026/01/25/palm_3_v2_truthfulqa_pro",
              "published": "2026-01-25",
              "snippet": "PaLM 3 v2 achieves 90% on TruthfulQA Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_the_pile_next_trained_on",
          "source": "model:yi_large_next",
          "target": "dataset:the_pile_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:mlflow_llamaindex_core_integrates_with",
          "source": "tool:mlflow",
          "target": "tool:llamaindex_core",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_constitutional_ai_plus_uses_tech",
          "source": "repo:llamacpp_v2",
          "target": "tech:constitutional_ai_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_v2_wikipedia_dump_2025_ultra_trained_on",
          "source": "model:gemini_ultra_2_v2",
          "target": "dataset:wikipedia_dump_2025_ultra",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-12_wired_1796",
              "url": "https://wired.com/2026/01/12/gemini_ultra_2_v2_wikipedia_du",
              "published": "2026-01-12",
              "snippet": "The training corpus for Gemini Ultra 2 v2 includes Wikipedia Dump 2025 Ultra..."
            },
            {
              "docId": "2026-01-15_the_gradient_1277",
              "url": "https://thegradient.pub/2026/01/15/gemini_ultra_2_v2_wikipedia_du",
              "published": "2026-01-15",
              "snippet": "The training corpus for Gemini Ultra 2 v2 includes Wikipedia Dump 2025 Ultra..."
            },
            {
              "docId": "2026-01-16_hugging_face_bl_4385",
              "url": "https://huggingface.co/blog/2026/01/16/gemini_ultra_2_v2_wikipedia_du",
              "published": "2026-01-16",
              "snippet": "Gemini Ultra 2 v2 was trained on Wikipedia Dump 2025 Ultra comprising billions of tokens..."
            },
            {
              "docId": "2026-01-25_the_gradient_2694",
              "url": "https://thegradient.pub/2026/01/25/gemini_ultra_2_v2_wikipedia_du",
              "published": "2026-01-25",
              "snippet": "The training corpus for Gemini Ultra 2 v2 includes Wikipedia Dump 2025 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_next_gpt_4o_mini_2_next_measures",
          "source": "benchmark:humaneval_next",
          "target": "model:gpt_4o_mini_2_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-06_hugging_face_bl_1878",
              "url": "https://huggingface.co/blog/2026/01/06/humaneval_next_gpt_4o_mini_2_n",
              "published": "2026-01-06",
              "snippet": "The HumanEval Next benchmark measures GPT-4o Mini 2 Next across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_dpo_v2_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-24_the_verge_3440",
              "url": "https://theverge.com/2026/01/24/llm_agents:_a_survey_pro_dpo_v",
              "published": "2026-01-24",
              "snippet": "LLM Agents: A Survey Pro leverages DPO v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_core_group_query_attention_core_uses_tech",
          "source": "tool:langchain_core",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-19_arxiv_3065",
              "url": "https://arxiv.org/abs/2026/01/19/langchain_core_group_query_att",
              "published": "2026-01-19",
              "snippet": "Under the hood, LangChain Core implements Group Query Attention Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_mini_stable_diffusion_4_next_integrates_with",
          "source": "tool:cody_mini",
          "target": "model:stable_diffusion_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-21_meta_ai_blog_9747",
              "url": "https://ai.meta.com/blog/2026/01/21/cody_mini_stable_diffusion_4_n",
              "published": "2026-01-21",
              "snippet": "Cody Mini now supports Stable Diffusion 4 Next with full feature parity..."
            },
            {
              "docId": "2026-01-24_the_verge_2577",
              "url": "https://theverge.com/2026/01/24/cody_mini_stable_diffusion_4_n",
              "published": "2026-01-24",
              "snippet": "Cody Mini announced official support for Stable Diffusion 4 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_phi_4_pro_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:phi_4_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-25_arxiv_5883",
              "url": "https://arxiv.org/abs/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Chain-of-Thought Prompting Elicits Reasoning reaching 93% on Phi-4 Pro..."
            },
            {
              "docId": "2026-01-25_techcrunch_7667",
              "url": "https://techcrunch.com/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "On the Phi-4 Pro benchmark, Chain-of-Thought Prompting Elicits Reasoning scored 74%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_next_flowise_plus_integrates_with",
          "source": "tool:vllm_next",
          "target": "tool:flowise_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_yi_large_next_depends_on",
          "source": "model:dall_e_4_core",
          "target": "model:yi_large_next",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:refinedweb_mini_synthetic_data_generation_edge_uses_tech",
          "source": "dataset:refinedweb_mini",
          "target": "tech:synthetic_data_generation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_qlora_core_uses_tech",
          "source": "model:stable_diffusion_4_next",
          "target": "tech:qlora_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-23_openai_blog_6236",
              "url": "https://openai.com/blog/2026/01/23/stable_diffusion_4_next_qlora_",
              "published": "2026-01-23",
              "snippet": "Under the hood, Stable Diffusion 4 Next implements QLoRA Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_next_math_max_evaluated_on",
          "source": "model:llama_4_next",
          "target": "benchmark:math_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-11_openai_blog_1180",
              "url": "https://openai.com/blog/2026/01/11/llama_4_next_math_max",
              "published": "2026-01-11",
              "snippet": "Evaluation results show Llama 4 Next reaching 91% on MATH Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_command_r_plus_plus_integrates_with",
          "source": "repo:open_interpreter",
          "target": "model:command_r_plus_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_7352",
              "url": "https://blog.google/technology/ai/2026/01/25/open_interpreter_command_r_plu",
              "published": "2026-01-25",
              "snippet": "open-interpreter now supports Command R+ Plus with full feature parity..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_8567",
              "url": "https://anthropic.com/news/2026/01/25/open_interpreter_command_r_plu",
              "published": "2026-01-25",
              "snippet": "open-interpreter announced official support for Command R+ Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_tree_of_thought_plus_uses_tech",
          "source": "model:command_r_plus",
          "target": "tech:tree_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:sora_2_ultra_dolma_pro_trained_on",
          "source": "model:sora_2_ultra",
          "target": "dataset:dolma_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:mlflow_ultra_yi_large_max_integrates_with",
          "source": "tool:mlflow_ultra",
          "target": "model:yi_large_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_edge_qlora_pro_uses_tech",
          "source": "model:whisper_v4_edge",
          "target": "tech:qlora_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_tool_use_max_uses_tech",
          "source": "model:palm_3_lite",
          "target": "tech:tool_use_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_gpt_5_plus_integrates_with",
          "source": "repo:vllm_edge",
          "target": "model:gpt_5_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-20_nextgov_4773",
              "url": "https://nextgov.com/2026/01/20/vllm_edge_gpt_5_plus",
              "published": "2026-01-20",
              "snippet": "The latest release of vllm Edge adds native GPT-5 Plus integration..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_8093",
              "url": "https://anthropic.com/news/2026/01/25/vllm_edge_gpt_5_plus",
              "published": "2026-01-25",
              "snippet": "vllm Edge announced official support for GPT-5 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_nemotron_5_core_depends_on",
          "source": "model:jamba_2_mini",
          "target": "model:nemotron_5_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:dolma_v2_quantization_v2_uses_tech",
          "source": "dataset:dolma_v2",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:arc_agi_max_gemini_ultra_2_pro_measures",
          "source": "benchmark:arc_agi_max",
          "target": "model:gemini_ultra_2_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_7921",
              "url": "https://huggingface.co/blog/2026/01/25/arc_agi_max_gemini_ultra_2_pro",
              "published": "2026-01-25",
              "snippet": "The ARC-AGI Max benchmark measures Gemini Ultra 2 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_constitutional_ai_lite_uses_tech",
          "source": "tool:mlflow_mini",
          "target": "tech:constitutional_ai_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_claude_sonnet_4_ultra_integrates_with",
          "source": "tool:ollama_edge",
          "target": "model:claude_sonnet_4_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-14_google_ai_blog_9851",
              "url": "https://blog.google/technology/ai/2026/01/14/ollama_edge_claude_sonnet_4_ul",
              "published": "2026-01-14",
              "snippet": "Ollama Edge now supports Claude Sonnet 4 Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_plus_yi_large_max_integrates_with",
          "source": "repo:llamacpp_plus",
          "target": "model:yi_large_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-01_microsoft_resea_4049",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/01/llamacpp_plus_yi_large_max",
              "published": "2026-01-01",
              "snippet": "llama.cpp Plus announced official support for Yi-Large Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_speculative_decoding_max_uses_tech",
          "source": "repo:ollama_core",
          "target": "tech:speculative_decoding_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-19_the_verge_1853",
              "url": "https://theverge.com/2026/01/19/ollama_core_speculative_decodi",
              "published": "2026-01-19",
              "snippet": "Under the hood, ollama Core implements Speculative Decoding Max for improved efficiency..."
            },
            {
              "docId": "2026-01-24_techcrunch_9773",
              "url": "https://techcrunch.com/2026/01/24/ollama_core_speculative_decodi",
              "published": "2026-01-24",
              "snippet": "ollama Core leverages Speculative Decoding Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_max_gemini_ultra_2_pro_depends_on",
          "source": "model:stable_diffusion_4_max",
          "target": "model:gemini_ultra_2_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_edge_gsm8k_v2_evaluated_on",
          "source": "model:claude_opus_45_edge",
          "target": "benchmark:gsm8k_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_3515",
              "url": "https://blog.google/technology/ai/2026/01/22/claude_opus_45_edge_gsm8k_v2",
              "published": "2026-01-22",
              "snippet": "Evaluation results show Claude Opus 4.5 Edge reaching 85% on GSM8K v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_edge_yi_large_mini_integrates_with",
          "source": "tool:haystack_edge",
          "target": "model:yi_large_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-20_microsoft_resea_8204",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/haystack_edge_yi_large_mini",
              "published": "2026-01-20",
              "snippet": "Haystack Edge now supports Yi-Large Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_edge_palm_3_max_depends_on",
          "source": "model:whisper_v4_edge",
          "target": "model:palm_3_max",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_synthetic_data_generation_max_uses_tech",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "tech:synthetic_data_generation_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-13_anthropic_blog_5053",
              "url": "https://anthropic.com/news/2026/01/13/llm_agents:_a_survey_next_synt",
              "published": "2026-01-13",
              "snippet": "Technical details reveal LLM Agents: A Survey Next relies heavily on Synthetic Data Generation Max..."
            },
            {
              "docId": "2026-01-25_nextgov_9410",
              "url": "https://nextgov.com/2026/01/25/llm_agents:_a_survey_next_synt",
              "published": "2026-01-25",
              "snippet": "LLM Agents: A Survey Next leverages Synthetic Data Generation Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_gemma_3_lite_integrates_with",
          "source": "tool:langchain",
          "target": "model:gemma_3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:jamba_2_max_aya_3_pro_depends_on",
          "source": "model:jamba_2_max",
          "target": "model:aya_3_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_swe_bench_pro_evaluated_on",
          "source": "model:whisper_v4_core",
          "target": "benchmark:swe_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-16_meta_ai_blog_7880",
              "url": "https://ai.meta.com/blog/2026/01/16/whisper_v4_core_swe_bench_pro",
              "published": "2026-01-16",
              "snippet": "Evaluation results show Whisper v4 Core reaching 74% on SWE-bench Pro..."
            },
            {
              "docId": "2026-01-21_the_verge_2171",
              "url": "https://theverge.com/2026/01/21/whisper_v4_core_swe_bench_pro",
              "published": "2026-01-21",
              "snippet": "On the SWE-bench Pro benchmark, Whisper v4 Core scored 85%..."
            },
            {
              "docId": "2026-01-22_wired_5286",
              "url": "https://wired.com/2026/01/22/whisper_v4_core_swe_bench_pro",
              "published": "2026-01-22",
              "snippet": "Whisper v4 Core achieves 99% on SWE-bench Pro, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_edge_weights_and_biases_pro_integrates_with",
          "source": "tool:dify_edge",
          "target": "tool:weights_and_biases_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-19_openai_blog_1473",
              "url": "https://openai.com/blog/2026/01/19/dify_edge_weights_and_biases_p",
              "published": "2026-01-19",
              "snippet": "Dify Edge announced official support for Weights & Biases Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_sora_2_ultra_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:sora_2_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_roots_max_trained_on",
          "source": "model:jamba_2_core",
          "target": "dataset:roots_max",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.53,
          "evidence": [
            {
              "docId": "2026-01-14_weights_and_bia_8799",
              "url": "https://wandb.ai/articles/2026/01/14/jamba_2_core_roots_max",
              "published": "2026-01-14",
              "snippet": "Jamba 2 Core was trained on ROOTS Max comprising billions of tokens..."
            },
            {
              "docId": "2026-01-24_weights_and_bia_9671",
              "url": "https://wandb.ai/articles/2026/01/24/jamba_2_core_roots_max",
              "published": "2026-01-24",
              "snippet": "Jamba 2 Core was trained on ROOTS Max comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_starcoder_data_lite_trained_on",
          "source": "model:sora_2_edge",
          "target": "dataset:starcoder_data_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_flash_attention_mini_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-18_weights_and_bia_3153",
              "url": "https://wandb.ai/articles/2026/01/18/textbooks_are_all_you_need_min",
              "published": "2026-01-18",
              "snippet": "Under the hood, Textbooks Are All You Need Mini implements Flash Attention Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_next_mixture_of_experts_max_uses_tech",
          "source": "model:aya_3_next",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:vllm_edge_sora_2_max_integrates_with",
          "source": "repo:vllm_edge",
          "target": "model:sora_2_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:autogpt_tokenizer_bpe_lite_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-12_reuters_5757",
              "url": "https://reuters.com/technology/2026/01/12/autogpt_tokenizer_bpe_lite",
              "published": "2026-01-12",
              "snippet": "AutoGPT leverages Tokenizer BPE Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-17_mit_technology__3846",
              "url": "https://technologyreview.com/2026/01/17/autogpt_tokenizer_bpe_lite",
              "published": "2026-01-17",
              "snippet": "Technical details reveal AutoGPT relies heavily on Tokenizer BPE Lite..."
            },
            {
              "docId": "2026-01-25_arxiv_7000",
              "url": "https://arxiv.org/abs/2026/01/25/autogpt_tokenizer_bpe_lite",
              "published": "2026-01-25",
              "snippet": "Technical details reveal AutoGPT relies heavily on Tokenizer BPE Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_constitutional_ai_mini_uses_tech",
          "source": "repo:ollama_mini",
          "target": "tech:constitutional_ai_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-10_ars_technica_1234",
              "url": "https://arstechnica.com/2026/01/10/ollama_mini_constitutional_ai_",
              "published": "2026-01-10",
              "snippet": "ollama Mini leverages Constitutional AI Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_pro_dolma_lite_trained_on",
          "source": "model:mixtral_8x22b_pro",
          "target": "dataset:dolma_lite",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_flash_attention_core_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:flash_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_3876",
              "url": "https://anthropic.com/news/2026/01/24/llm_agents:_a_survey_pro_flash",
              "published": "2026-01-24",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Flash Attention Core for improved efficiency..."
            },
            {
              "docId": "2026-01-24_anthropic_blog_5405",
              "url": "https://anthropic.com/news/2026/01/24/llm_agents:_a_survey_pro_flash",
              "published": "2026-01-24",
              "snippet": "Technical details reveal LLM Agents: A Survey Pro relies heavily on Flash Attention Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_mini_aya_3_core_measures",
          "source": "benchmark:gsm8k_mini",
          "target": "model:aya_3_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_8290",
              "url": "https://arstechnica.com/2026/01/25/gsm8k_mini_aya_3_core",
              "published": "2026-01-25",
              "snippet": "GSM8K Mini provides standardized evaluation of Aya 3 Core..."
            },
            {
              "docId": "2026-01-25_mit_technology__2318",
              "url": "https://technologyreview.com/2026/01/25/gsm8k_mini_aya_3_core",
              "published": "2026-01-25",
              "snippet": "GSM8K Mini provides standardized evaluation of Aya 3 Core..."
            },
            {
              "docId": "2026-01-25_arxiv_2999",
              "url": "https://arxiv.org/abs/2026/01/25/gsm8k_mini_aya_3_core",
              "published": "2026-01-25",
              "snippet": "GSM8K Mini has become the standard for evaluating Aya 3 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_lite_midjourney_v7_ultra_depends_on",
          "source": "model:whisper_v4_lite",
          "target": "model:midjourney_v7_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.47
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_jamba_2_next_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:jamba_2_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:cursor_v2_autogpt_ultra_integrates_with",
          "source": "tool:cursor_v2",
          "target": "tool:autogpt_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_codex_2_lite_integrates_with",
          "source": "repo:gpt4all_pro",
          "target": "model:codex_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-22_ars_technica_9554",
              "url": "https://arstechnica.com/2026/01/22/gpt4all_pro_codex_2_lite",
              "published": "2026-01-22",
              "snippet": "The latest release of gpt4all Pro adds native Codex 2 Lite integration..."
            },
            {
              "docId": "2026-01-24_ars_technica_9562",
              "url": "https://arstechnica.com/2026/01/24/gpt4all_pro_codex_2_lite",
              "published": "2026-01-24",
              "snippet": "gpt4all Pro now supports Codex 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_lora_edge_uses_tech",
          "source": "tool:ollama_edge",
          "target": "tech:lora_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-11-06_the_verge_9723",
              "url": "https://theverge.com/2025/11/06/ollama_edge_lora_edge",
              "published": "2025-11-06",
              "snippet": "Ollama Edge leverages LoRA Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_whisper_v4_lite_depends_on",
          "source": "model:jamba_2_lite",
          "target": "model:whisper_v4_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:hellaswag_mixtral_8x22b_ultra_measures",
          "source": "benchmark:hellaswag",
          "target": "model:mixtral_8x22b_ultra",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:copilot_core_mlflow_integrates_with",
          "source": "tool:copilot_core",
          "target": "tool:mlflow",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_winogrande_ultra_evaluated_on",
          "source": "model:jamba_2_mini",
          "target": "benchmark:winogrande_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-11_langchain_blog_2614",
              "url": "https://blog.langchain.dev/2026/01/11/jamba_2_mini_winogrande_ultra",
              "published": "2026-01-11",
              "snippet": "Jamba 2 Mini achieves 82% on WinoGrande Ultra, setting a new record..."
            },
            {
              "docId": "2026-01-18_reuters_7109",
              "url": "https://reuters.com/technology/2026/01/18/jamba_2_mini_winogrande_ultra",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Jamba 2 Mini reaching 70% on WinoGrande Ultra..."
            },
            {
              "docId": "2026-01-21_ars_technica_2750",
              "url": "https://arstechnica.com/2026/01/21/jamba_2_mini_winogrande_ultra",
              "published": "2026-01-21",
              "snippet": "On the WinoGrande Ultra benchmark, Jamba 2 Mini scored 71%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_mini_kv_cache_optimization_pro_uses_tech",
          "source": "tool:dify_mini",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:haystack_edge_chain_of_thought_lite_uses_tech",
          "source": "tool:haystack_edge",
          "target": "tech:chain_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:c4_core_sliding_window_attention_lite_uses_tech",
          "source": "dataset:c4_core",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_codex_2_mini_integrates_with",
          "source": "repo:langchain_lite",
          "target": "model:codex_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-22_techcrunch_4014",
              "url": "https://techcrunch.com/2026/01/22/langchain_lite_codex_2_mini",
              "published": "2026-01-22",
              "snippet": "langchain Lite now supports Codex 2 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_v2_tokenizer_bpe_ultra_uses_tech",
          "source": "tool:weights_and_biases_v2",
          "target": "tech:tokenizer_bpe_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-03_nextgov_5296",
              "url": "https://nextgov.com/2026/01/03/weights_and_biases_v2_tokenize",
              "published": "2026-01-03",
              "snippet": "Under the hood, Weights & Biases v2 implements Tokenizer BPE Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-22_mit_technology__7389",
              "url": "https://technologyreview.com/2026/01/22/weights_and_biases_v2_tokenize",
              "published": "2026-01-22",
              "snippet": "Weights & Biases v2 leverages Tokenizer BPE Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_rlhf_pro_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_3007",
              "url": "https://nextgov.com/2026/01/25/llm_agents:_a_survey_pro_rlhf_",
              "published": "2026-01-25",
              "snippet": "Technical details reveal LLM Agents: A Survey Pro relies heavily on RLHF Pro..."
            },
            {
              "docId": "2026-01-25_ars_technica_4865",
              "url": "https://arstechnica.com/2026/01/25/llm_agents:_a_survey_pro_rlhf_",
              "published": "2026-01-25",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements RLHF Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-25_openai_blog_5883",
              "url": "https://openai.com/blog/2026/01/25/llm_agents:_a_survey_pro_rlhf_",
              "published": "2026-01-25",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements RLHF Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_next_dolma_core_trained_on",
          "source": "model:codex_2_next",
          "target": "dataset:dolma_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_max_kv_cache_optimization_pro_uses_tech",
          "source": "model:claude_opus_45_max",
          "target": "tech:kv_cache_optimization_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_rlhf_pro_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_mini_claude_opus_45_evaluated_on",
          "source": "paper:llm_agents:_a_survey_mini",
          "target": "model:claude_opus_45",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-25_wired_7984",
              "url": "https://wired.com/2026/01/25/llm_agents:_a_survey_mini_clau",
              "published": "2026-01-25",
              "snippet": "Evaluation results show LLM Agents: A Survey Mini reaching 73% on Claude Opus 4.5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_codex_2_edge_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "model:codex_2_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-06_the_verge_5037",
              "url": "https://theverge.com/2026/01/06/textbooks_are_all_you_need_nex",
              "published": "2026-01-06",
              "snippet": "On the Codex 2 Edge benchmark, Textbooks Are All You Need Next scored 92%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_synthetic_data_generation_lite_uses_tech",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "tech:synthetic_data_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-31_bloomberg_2596",
              "url": "https://bloomberg.com/technology/2025/12/31/retrieval_augmented_generation",
              "published": "2025-12-31",
              "snippet": "Under the hood, Retrieval-Augmented Generation for Knowledge-Intensive NLP implements Synthetic Data Generation Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-08_anthropic_blog_2966",
              "url": "https://anthropic.com/news/2026/01/08/retrieval_augmented_generation",
              "published": "2026-01-08",
              "snippet": "Technical details reveal Retrieval-Augmented Generation for Knowledge-Intensive NLP relies heavily on Synthetic Data Generation Lite..."
            },
            {
              "docId": "2026-01-21_techcrunch_1619",
              "url": "https://techcrunch.com/2026/01/21/retrieval_augmented_generation",
              "published": "2026-01-21",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP leverages Synthetic Data Generation Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_gguf_core_uses_tech",
          "source": "tool:copilot_edge",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:ollama_pro_langchain_integrates_with",
          "source": "tool:ollama_pro",
          "target": "tool:langchain",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-17_the_gradient_8804",
              "url": "https://thegradient.pub/2026/01/17/ollama_pro_langchain",
              "published": "2026-01-17",
              "snippet": "Ollama Pro announced official support for LangChain..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_v2_refinedweb_next_trained_on",
          "source": "model:gemma_3_v2",
          "target": "dataset:refinedweb_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_lite_chain_of_thought_ultra_uses_tech",
          "source": "dataset:starcoder_data_lite",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:gemma_3_pro_qlora_edge_uses_tech",
          "source": "model:gemma_3_pro",
          "target": "tech:qlora_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_palm_3_next_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:palm_3_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-11-23_venturebeat_6224",
              "url": "https://venturebeat.com/2025/11/23/flash_attention:_fast_and_memo",
              "published": "2025-11-23",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention achieves 70% on PaLM 3 Next, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_llama_4_core_integrates_with",
          "source": "repo:ollama",
          "target": "model:llama_4_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_whisper_v4_v2_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:whisper_v4_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-20_arxiv_9612",
              "url": "https://arxiv.org/abs/2026/01/20/llamacpp_mini_whisper_v4_v2",
              "published": "2026-01-20",
              "snippet": "llama.cpp Mini now supports Whisper v4 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-24_bloomberg_2198",
              "url": "https://bloomberg.com/technology/2026/01/24/llamacpp_mini_whisper_v4_v2",
              "published": "2026-01-24",
              "snippet": "llama.cpp Mini now supports Whisper v4 v2 with full feature parity..."
            },
            {
              "docId": "2026-01-25_venturebeat_8439",
              "url": "https://venturebeat.com/2026/01/25/llamacpp_mini_whisper_v4_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of llama.cpp Mini adds native Whisper v4 v2 integration..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_1829",
              "url": "https://huggingface.co/blog/2026/01/25/llamacpp_mini_whisper_v4_v2",
              "published": "2026-01-25",
              "snippet": "llama.cpp Mini now supports Whisper v4 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_edge_dify_core_integrates_with",
          "source": "tool:gradio_edge",
          "target": "tool:dify_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-04_the_verge_2201",
              "url": "https://theverge.com/2026/01/04/gradio_edge_dify_core",
              "published": "2026-01-04",
              "snippet": "Gradio Edge now supports Dify Core with full feature parity..."
            },
            {
              "docId": "2026-01-23_nvidia_blog_2959",
              "url": "https://blogs.nvidia.com/2026/01/23/gradio_edge_dify_core",
              "published": "2026-01-23",
              "snippet": "The latest release of Gradio Edge adds native Dify Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_next_fineweb_trained_on",
          "source": "model:gpt_4o_mini_2_next",
          "target": "dataset:fineweb",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.49,
          "evidence": [
            {
              "docId": "2026-01-17_ars_technica_1163",
              "url": "https://arstechnica.com/2026/01/17/gpt_4o_mini_2_next_fineweb",
              "published": "2026-01-17",
              "snippet": "GPT-4o Mini 2 Next utilized FineWeb as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_mixtral_8x22b_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:mixtral_8x22b",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-27_ars_technica_8694",
              "url": "https://arstechnica.com/2025/12/27/langchain_max_mixtral_8x22b",
              "published": "2025-12-27",
              "snippet": "The latest release of langchain Max adds native Mixtral 8x22B integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_litellm_integrates_with",
          "source": "tool:vllm_core",
          "target": "tool:litellm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:ollama_pro_midjourney_v7_max_integrates_with",
          "source": "tool:ollama_pro",
          "target": "model:midjourney_v7_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_lite_the_pile_plus_trained_on",
          "source": "model:whisper_v4_lite",
          "target": "dataset:the_pile_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_mt_bench_edge_evaluated_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "benchmark:mt_bench_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-24_wired_6125",
              "url": "https://wired.com/2025/12/24/gemini_ultra_2_pro_mt_bench_ed",
              "published": "2025-12-24",
              "snippet": "On the MT-Bench Edge benchmark, Gemini Ultra 2 Pro scored 99%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_nemotron_5_integrates_with",
          "source": "tool:langchain_lite",
          "target": "model:nemotron_5",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-14_hugging_face_bl_7075",
              "url": "https://huggingface.co/blog/2026/01/14/langchain_lite_nemotron_5",
              "published": "2026-01-14",
              "snippet": "LangChain Lite announced official support for Nemotron-5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_max_dify_core_integrates_with",
          "source": "tool:cursor_max",
          "target": "tool:dify_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_7144",
              "url": "https://venturebeat.com/2026/01/23/cursor_max_dify_core",
              "published": "2026-01-23",
              "snippet": "Cursor Max now supports Dify Core with full feature parity..."
            },
            {
              "docId": "2026-01-24_arxiv_4645",
              "url": "https://arxiv.org/abs/2026/01/24/cursor_max_dify_core",
              "published": "2026-01-24",
              "snippet": "Cursor Max now supports Dify Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:refinedweb_mini_quantization_v2_uses_tech",
          "source": "dataset:refinedweb_mini",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:arc_agi_lite_whisper_v4_edge_measures",
          "source": "benchmark:arc_agi_lite",
          "target": "model:whisper_v4_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-25_wired_7367",
              "url": "https://wired.com/2026/01/25/arc_agi_lite_whisper_v4_edge",
              "published": "2026-01-25",
              "snippet": "ARC-AGI Lite provides standardized evaluation of Whisper v4 Edge..."
            },
            {
              "docId": "2026-01-25_langchain_blog_2879",
              "url": "https://blog.langchain.dev/2026/01/25/arc_agi_lite_whisper_v4_edge",
              "published": "2026-01-25",
              "snippet": "ARC-AGI Lite provides standardized evaluation of Whisper v4 Edge..."
            },
            {
              "docId": "2026-01-25_venturebeat_4989",
              "url": "https://venturebeat.com/2026/01/25/arc_agi_lite_whisper_v4_edge",
              "published": "2026-01-25",
              "snippet": "ARC-AGI Lite provides standardized evaluation of Whisper v4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_sora_2_ultra_depends_on",
          "source": "model:claude_sonnet_4_next",
          "target": "model:sora_2_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_refinedweb_trained_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "dataset:refinedweb",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_whisper_v4_edge_depends_on",
          "source": "model:claude_sonnet_4_next",
          "target": "model:whisper_v4_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:falcon_3_max_gpt_4o_mini_2_next_depends_on",
          "source": "model:falcon_3_max",
          "target": "model:gpt_4o_mini_2_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_plus_red_teaming_pro_uses_tech",
          "source": "dataset:the_stack_v2_plus",
          "target": "tech:red_teaming_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_nemotron_5_mini_integrates_with",
          "source": "repo:ollama_ultra",
          "target": "model:nemotron_5_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_sora_2_ultra_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:sora_2_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_pro_flash_attention_lite_uses_tech",
          "source": "dataset:starcoder_data_pro",
          "target": "tech:flash_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:transformers_next_qwen_3_next_integrates_with",
          "source": "repo:transformers_next",
          "target": "model:qwen_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-22_nextgov_9843",
              "url": "https://nextgov.com/2025/12/22/transformers_next_qwen_3_next",
              "published": "2025-12-22",
              "snippet": "transformers Next now supports Qwen-3 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_vllm_integrates_with",
          "source": "tool:semantic_kernel_v2",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:dolma_next_lora_lite_uses_tech",
          "source": "dataset:dolma_next",
          "target": "tech:lora_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_lite_c4_ultra_trained_on",
          "source": "model:deepseek_v3_lite",
          "target": "dataset:c4_ultra",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:grok_3_plus_mbpp_lite_evaluated_on",
          "source": "model:grok_3_plus",
          "target": "benchmark:mbpp_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-23_bloomberg_4183",
              "url": "https://bloomberg.com/technology/2026/01/23/grok_3_plus_mbpp_lite",
              "published": "2026-01-23",
              "snippet": "On the MBPP Lite benchmark, Grok-3 Plus scored 99%..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_6904",
              "url": "https://blog.google/technology/ai/2026/01/25/grok_3_plus_mbpp_lite",
              "published": "2026-01-25",
              "snippet": "Grok-3 Plus achieves 91% on MBPP Lite, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_gguf_pro_uses_tech",
          "source": "dataset:c4",
          "target": "tech:gguf_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:scaling_data_constrained_language_models_synthetic_data_generation_mini_uses_tech",
          "source": "paper:scaling_data_constrained_language_models",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-04_anthropic_blog_8804",
              "url": "https://anthropic.com/news/2026/01/04/scaling_data_constrained_langu",
              "published": "2026-01-04",
              "snippet": "Under the hood, Scaling Data-Constrained Language Models implements Synthetic Data Generation Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-16_wired_7738",
              "url": "https://wired.com/2026/01/16/scaling_data_constrained_langu",
              "published": "2026-01-16",
              "snippet": "Under the hood, Scaling Data-Constrained Language Models implements Synthetic Data Generation Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_v2_whisper_v4_pro_integrates_with",
          "source": "tool:llamaindex_v2",
          "target": "model:whisper_v4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__5979",
              "url": "https://technologyreview.com/2026/01/25/llamaindex_v2_whisper_v4_pro",
              "published": "2026-01-25",
              "snippet": "The latest release of LlamaIndex v2 adds native Whisper v4 Pro integration..."
            },
            {
              "docId": "2026-01-25_reuters_3028",
              "url": "https://reuters.com/technology/2026/01/25/llamaindex_v2_whisper_v4_pro",
              "published": "2026-01-25",
              "snippet": "The latest release of LlamaIndex v2 adds native Whisper v4 Pro integration..."
            },
            {
              "docId": "2026-01-25_venturebeat_8378",
              "url": "https://venturebeat.com/2026/01/25/llamaindex_v2_whisper_v4_pro",
              "published": "2026-01-25",
              "snippet": "LlamaIndex v2 announced official support for Whisper v4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_sliding_window_attention_lite_uses_tech",
          "source": "model:sora_2",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:crewai_max_vllm_lite_integrates_with",
          "source": "tool:crewai_max",
          "target": "tool:vllm_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-07_hugging_face_bl_4308",
              "url": "https://huggingface.co/blog/2025/12/07/crewai_max_vllm_lite",
              "published": "2025-12-07",
              "snippet": "The latest release of CrewAI Max adds native vLLM Lite integration..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_3677",
              "url": "https://blogs.nvidia.com/2026/01/19/crewai_max_vllm_lite",
              "published": "2026-01-19",
              "snippet": "CrewAI Max now supports vLLM Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__chain_of_thought_ultra_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:chain_of_thought_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_ultra_cursor_max_integrates_with",
          "source": "tool:semantic_kernel_ultra",
          "target": "tool:cursor_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_gsm8k_edge_evaluated_on",
          "source": "model:sora_2_edge",
          "target": "benchmark:gsm8k_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_synthetic_data_generation_pro_uses_tech",
          "source": "dataset:redpajama_v2",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:math_edge_grok_3_measures",
          "source": "benchmark:math_edge",
          "target": "model:grok_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-03_arxiv_9809",
              "url": "https://arxiv.org/abs/2026/01/03/math_edge_grok_3",
              "published": "2026-01-03",
              "snippet": "MATH Edge has become the standard for evaluating Grok-3..."
            },
            {
              "docId": "2026-01-05_venturebeat_8399",
              "url": "https://venturebeat.com/2026/01/05/math_edge_grok_3",
              "published": "2026-01-05",
              "snippet": "MATH Edge has become the standard for evaluating Grok-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mini_weights_and_biases_next_integrates_with",
          "source": "tool:litellm_mini",
          "target": "tool:weights_and_biases_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-04_nextgov_4869",
              "url": "https://nextgov.com/2026/01/04/litellm_mini_weights_and_biase",
              "published": "2026-01-04",
              "snippet": "The latest release of LiteLLM Mini adds native Weights & Biases Next integration..."
            },
            {
              "docId": "2026-01-18_microsoft_resea_9922",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/litellm_mini_weights_and_biase",
              "published": "2026-01-18",
              "snippet": "LiteLLM Mini now supports Weights & Biases Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_pro_speculative_decoding_lite_uses_tech",
          "source": "dataset:starcoder_data_pro",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_tree_of_thought_next_uses_tech",
          "source": "paper:direct_preference_optimization_v2",
          "target": "tech:tree_of_thought_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_litellm_plus_integrates_with",
          "source": "tool:ollama_edge",
          "target": "tool:litellm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_distillation_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:distillation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:crewai_v2_sliding_window_attention_plus_uses_tech",
          "source": "tool:crewai_v2",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:swe_bench_pro_mixtral_8x22b_plus_measures",
          "source": "benchmark:swe_bench_pro",
          "target": "model:mixtral_8x22b_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_nextgov_8148",
              "url": "https://nextgov.com/2026/01/22/swe_bench_pro_mixtral_8x22b_pl",
              "published": "2026-01-22",
              "snippet": "SWE-bench Pro provides standardized evaluation of Mixtral 8x22B Plus..."
            },
            {
              "docId": "2026-01-24_ars_technica_6796",
              "url": "https://arstechnica.com/2026/01/24/swe_bench_pro_mixtral_8x22b_pl",
              "published": "2026-01-24",
              "snippet": "SWE-bench Pro provides standardized evaluation of Mixtral 8x22B Plus..."
            },
            {
              "docId": "2026-01-25_the_gradient_5769",
              "url": "https://thegradient.pub/2026/01/25/swe_bench_pro_mixtral_8x22b_pl",
              "published": "2026-01-25",
              "snippet": "The SWE-bench Pro benchmark measures Mixtral 8x22B Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_pro_tree_of_thought_uses_tech",
          "source": "tool:autogpt_pro",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-24_nvidia_blog_9066",
              "url": "https://blogs.nvidia.com/2026/01/24/autogpt_pro_tree_of_thought",
              "published": "2026-01-24",
              "snippet": "Technical details reveal AutoGPT Pro relies heavily on Tree of Thought..."
            },
            {
              "docId": "2026-01-24_techcrunch_3306",
              "url": "https://techcrunch.com/2026/01/24/autogpt_pro_tree_of_thought",
              "published": "2026-01-24",
              "snippet": "Under the hood, AutoGPT Pro implements Tree of Thought for improved efficiency..."
            },
            {
              "docId": "2026-01-24_anthropic_blog_5815",
              "url": "https://anthropic.com/news/2026/01/24/autogpt_pro_tree_of_thought",
              "published": "2026-01-24",
              "snippet": "Under the hood, AutoGPT Pro implements Tree of Thought for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_retrieval_augmented_generation_core_uses_tech",
          "source": "tool:langchain_edge",
          "target": "tech:retrieval_augmented_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_mini_gpt_5_lite_depends_on",
          "source": "model:whisper_v4_mini",
          "target": "model:gpt_5_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:dolma_pro_rotary_position_embedding_plus_uses_tech",
          "source": "dataset:dolma_pro",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_group_query_attention_plus_uses_tech",
          "source": "repo:langchain_lite",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-19_wired_1462",
              "url": "https://wired.com/2026/01/19/langchain_lite_group_query_att",
              "published": "2026-01-19",
              "snippet": "Under the hood, langchain Lite implements Group Query Attention Plus for improved efficiency..."
            },
            {
              "docId": "2026-01-19_microsoft_resea_9145",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/19/langchain_lite_group_query_att",
              "published": "2026-01-19",
              "snippet": "langchain Lite leverages Group Query Attention Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_ultra_winogrande_ultra_evaluated_on",
          "source": "model:qwen_3_ultra",
          "target": "benchmark:winogrande_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_6351",
              "url": "https://anthropic.com/news/2026/01/25/qwen_3_ultra_winogrande_ultra",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Qwen-3 Ultra reaching 84% on WinoGrande Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_pro_starcoder_data_pro_trained_on",
          "source": "model:yi_large_pro",
          "target": "dataset:starcoder_data_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:winogrande_edge_midjourney_v7_v2_measures",
          "source": "benchmark:winogrande_edge",
          "target": "model:midjourney_v7_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_9327",
              "url": "https://reuters.com/technology/2026/01/24/winogrande_edge_midjourney_v7_",
              "published": "2026-01-24",
              "snippet": "WinoGrande Edge has become the standard for evaluating Midjourney V7 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_flash_attention_edge_uses_tech",
          "source": "paper:textbooks_are_all_you_need",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-08_venturebeat_1873",
              "url": "https://venturebeat.com/2026/01/08/textbooks_are_all_you_need_fla",
              "published": "2026-01-08",
              "snippet": "Textbooks Are All You Need leverages Flash Attention Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_grok_3_ultra_integrates_with",
          "source": "repo:localai",
          "target": "model:grok_3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-03_arxiv_4534",
              "url": "https://arxiv.org/abs/2026/01/03/localai_grok_3_ultra",
              "published": "2026-01-03",
              "snippet": "LocalAI now supports Grok-3 Ultra with full feature parity..."
            },
            {
              "docId": "2026-01-19_nextgov_1310",
              "url": "https://nextgov.com/2026/01/19/localai_grok_3_ultra",
              "published": "2026-01-19",
              "snippet": "LocalAI now supports Grok-3 Ultra with full feature parity..."
            },
            {
              "docId": "2026-01-22_google_ai_blog_5550",
              "url": "https://blog.google/technology/ai/2026/01/22/localai_grok_3_ultra",
              "published": "2026-01-22",
              "snippet": "LocalAI announced official support for Grok-3 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_command_r_plus_pro_evaluated_on",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "model:command_r_plus_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-27_openai_blog_9632",
              "url": "https://openai.com/blog/2025/12/27/mixture_of_experts_meets_instr",
              "published": "2025-12-27",
              "snippet": "On the Command R+ Pro benchmark, Mixture of Experts Meets Instruction Tuning scored 92%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_pro_command_r_plus_measures",
          "source": "benchmark:truthfulqa_pro",
          "target": "model:command_r_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2025-12-29_openai_blog_6274",
              "url": "https://openai.com/blog/2025/12/29/truthfulqa_pro_command_r_plus",
              "published": "2025-12-29",
              "snippet": "TruthfulQA Pro provides standardized evaluation of Command R+..."
            },
            {
              "docId": "2026-01-02_nextgov_9182",
              "url": "https://nextgov.com/2026/01/02/truthfulqa_pro_command_r_plus",
              "published": "2026-01-02",
              "snippet": "TruthfulQA Pro provides standardized evaluation of Command R+..."
            },
            {
              "docId": "2026-01-22_the_gradient_9166",
              "url": "https://thegradient.pub/2026/01/22/truthfulqa_pro_command_r_plus",
              "published": "2026-01-22",
              "snippet": "The TruthfulQA Pro benchmark measures Command R+ across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_aya_3_next_integrates_with",
          "source": "repo:vllm",
          "target": "model:aya_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_5760",
              "url": "https://venturebeat.com/2026/01/23/vllm_aya_3_next",
              "published": "2026-01-23",
              "snippet": "vllm announced official support for Aya 3 Next..."
            },
            {
              "docId": "2026-01-23_techcrunch_2577",
              "url": "https://techcrunch.com/2026/01/23/vllm_aya_3_next",
              "published": "2026-01-23",
              "snippet": "vllm now supports Aya 3 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_ultra_gpt_5_next_depends_on",
          "source": "model:stable_diffusion_4_ultra",
          "target": "model:gpt_5_next",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:jamba_2_edge_deepseek_v3_mini_depends_on",
          "source": "model:jamba_2_edge",
          "target": "model:deepseek_v3_mini",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:flowise_v2_flowise_next_integrates_with",
          "source": "tool:flowise_v2",
          "target": "tool:flowise_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_distillation_edge_uses_tech",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "tech:distillation_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-03_nextgov_1200",
              "url": "https://nextgov.com/2026/01/03/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-03",
              "snippet": "Under the hood, Tree of Thoughts: Deliberate Problem Solving with LLMs implements Distillation Edge for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_tensorrt_llm_max_integrates_with",
          "source": "tool:vllm_pro",
          "target": "tool:tensorrt_llm_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:localai_plus_codex_2_lite_integrates_with",
          "source": "repo:localai_plus",
          "target": "model:codex_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-25_anthropic_blog_8662",
              "url": "https://anthropic.com/news/2025/12/25/localai_plus_codex_2_lite",
              "published": "2025-12-25",
              "snippet": "LocalAI Plus announced official support for Codex 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_multimodal_fusion_max_uses_tech",
          "source": "tool:tensorrt_llm",
          "target": "tech:multimodal_fusion_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:streamlit_flash_attention_mini_uses_tech",
          "source": "tool:streamlit",
          "target": "tech:flash_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:gsm8k_whisper_v4_pro_measures",
          "source": "benchmark:gsm8k",
          "target": "model:whisper_v4_pro",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_bigbench_hard_edge_evaluated_on",
          "source": "model:aya_3_max",
          "target": "benchmark:bigbench_hard_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-09_langchain_blog_6946",
              "url": "https://blog.langchain.dev/2025/12/09/aya_3_max_bigbench_hard_edge",
              "published": "2025-12-09",
              "snippet": "On the BigBench Hard Edge benchmark, Aya 3 Max scored 93%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_sora_2_max_measures",
          "source": "benchmark:arc_agi",
          "target": "model:sora_2_max",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_pro_speculative_decoding_lite_uses_tech",
          "source": "tool:weights_and_biases_pro",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-18_nvidia_blog_6256",
              "url": "https://blogs.nvidia.com/2026/01/18/weights_and_biases_pro_specula",
              "published": "2026-01-18",
              "snippet": "Under the hood, Weights & Biases Pro implements Speculative Decoding Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-18_meta_ai_blog_9479",
              "url": "https://ai.meta.com/blog/2026/01/18/weights_and_biases_pro_specula",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Weights & Biases Pro relies heavily on Speculative Decoding Lite..."
            },
            {
              "docId": "2026-01-21_wired_4563",
              "url": "https://wired.com/2026/01/21/weights_and_biases_pro_specula",
              "published": "2026-01-21",
              "snippet": "Under the hood, Weights & Biases Pro implements Speculative Decoding Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_flash_attention_max_uses_tech",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_9985",
              "url": "https://arxiv.org/abs/2026/01/24/mixture_of_experts_meets_instr",
              "published": "2026-01-24",
              "snippet": "Mixture of Experts Meets Instruction Tuning leverages Flash Attention Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_stable_diffusion_4_plus_integrates_with",
          "source": "tool:mlflow",
          "target": "model:stable_diffusion_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-15_wired_2263",
              "url": "https://wired.com/2026/01/15/mlflow_stable_diffusion_4_plus",
              "published": "2026-01-15",
              "snippet": "The latest release of MLflow adds native Stable Diffusion 4 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_quantization_max_uses_tech",
          "source": "tool:gradio",
          "target": "tech:quantization_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:localai_plus_synthetic_data_generation_uses_tech",
          "source": "repo:localai_plus",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-16_meta_ai_blog_1989",
              "url": "https://ai.meta.com/blog/2026/01/16/localai_plus_synthetic_data_ge",
              "published": "2026-01-16",
              "snippet": "Under the hood, LocalAI Plus implements Synthetic Data Generation for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_chain_of_thought_uses_tech",
          "source": "model:sora_2_edge",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-23_techcrunch_4913",
              "url": "https://techcrunch.com/2025/12/23/sora_2_edge_chain_of_thought",
              "published": "2025-12-23",
              "snippet": "Technical details reveal Sora 2 Edge relies heavily on Chain-of-Thought..."
            },
            {
              "docId": "2026-01-11_wired_7747",
              "url": "https://wired.com/2026/01/11/sora_2_edge_chain_of_thought",
              "published": "2026-01-11",
              "snippet": "Technical details reveal Sora 2 Edge relies heavily on Chain-of-Thought..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_lite_dall_e_4_measures",
          "source": "benchmark:arc_agi_lite",
          "target": "model:dall_e_4",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:jamba_2_core_mixtral_8x22b_plus_depends_on",
          "source": "model:jamba_2_core",
          "target": "model:mixtral_8x22b_plus",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:common_crawl_edge_group_query_attention_ultra_uses_tech",
          "source": "dataset:common_crawl_edge",
          "target": "tech:group_query_attention_ultra",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:codex_2_next_gsm8k_core_evaluated_on",
          "source": "model:codex_2_next",
          "target": "benchmark:gsm8k_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__2184",
              "url": "https://technologyreview.com/2026/01/24/codex_2_next_gsm8k_core",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Codex 2 Next reaching 99% on GSM8K Core..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_4851",
              "url": "https://blogs.nvidia.com/2026/01/25/codex_2_next_gsm8k_core",
              "published": "2026-01-25",
              "snippet": "On the GSM8K Core benchmark, Codex 2 Next scored 75%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_max_dall_e_4_mini_measures",
          "source": "benchmark:humaneval_max",
          "target": "model:dall_e_4_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-18_openai_blog_6382",
              "url": "https://openai.com/blog/2026/01/18/humaneval_max_dall_e_4_mini",
              "published": "2026-01-18",
              "snippet": "HumanEval Max provides standardized evaluation of DALL-E 4 Mini..."
            },
            {
              "docId": "2026-01-22_anthropic_blog_2327",
              "url": "https://anthropic.com/news/2026/01/22/humaneval_max_dall_e_4_mini",
              "published": "2026-01-22",
              "snippet": "The HumanEval Max benchmark measures DALL-E 4 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_9631",
              "url": "https://blogs.nvidia.com/2026/01/24/humaneval_max_dall_e_4_mini",
              "published": "2026-01-24",
              "snippet": "HumanEval Max has become the standard for evaluating DALL-E 4 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_transformer_architecture_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-25_langchain_blog_2403",
              "url": "https://blog.langchain.dev/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Under the hood, Attention Is All You Need v2 Mini implements Transformer Architecture for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_red_teaming_pro_uses_tech",
          "source": "tool:flowise_plus",
          "target": "tech:red_teaming_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_transformer_architecture_lite_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-15_nextgov_5604",
              "url": "https://nextgov.com/2026/01/15/llm_agents:_a_survey_core_tran",
              "published": "2026-01-15",
              "snippet": "LLM Agents: A Survey Core leverages Transformer Architecture Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_mini_redpajama_v2_core_trained_on",
          "source": "model:deepseek_v3_mini",
          "target": "dataset:redpajama_v2_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:phi_4_hellaswag_plus_evaluated_on",
          "source": "model:phi_4",
          "target": "benchmark:hellaswag_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_5782",
              "url": "https://nextgov.com/2026/01/25/phi_4_hellaswag_plus",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag Plus benchmark, Phi-4 scored 91%..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_5218",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/phi_4_hellaswag_plus",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Phi-4 reaching 83% on HellaSwag Plus..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_5071",
              "url": "https://huggingface.co/blog/2026/01/25/phi_4_hellaswag_plus",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag Plus benchmark, Phi-4 scored 90%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_plus_mc4_edge_trained_on",
          "source": "model:nemotron_5_plus",
          "target": "dataset:mc4_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-06_nvidia_blog_6895",
              "url": "https://blogs.nvidia.com/2026/01/06/nemotron_5_plus_mc4_edge",
              "published": "2026-01-06",
              "snippet": "Nemotron-5 Plus utilized mC4 Edge as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_8725",
              "url": "https://anthropic.com/news/2026/01/25/nemotron_5_plus_mc4_edge",
              "published": "2026-01-25",
              "snippet": "The training corpus for Nemotron-5 Plus includes mC4 Edge..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_4146",
              "url": "https://blog.google/technology/ai/2026/01/25/nemotron_5_plus_mc4_edge",
              "published": "2026-01-25",
              "snippet": "Nemotron-5 Plus utilized mC4 Edge as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_lora_max_uses_tech",
          "source": "model:stable_diffusion_4",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_edge_red_teaming_lite_uses_tech",
          "source": "tool:weights_and_biases_edge",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-14_arxiv_6013",
              "url": "https://arxiv.org/abs/2026/01/14/weights_and_biases_edge_red_te",
              "published": "2026-01-14",
              "snippet": "Technical details reveal Weights & Biases Edge relies heavily on Red Teaming Lite..."
            },
            {
              "docId": "2026-01-18_ars_technica_2922",
              "url": "https://arstechnica.com/2026/01/18/weights_and_biases_edge_red_te",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Weights & Biases Edge relies heavily on Red Teaming Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_whisper_v4_edge_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:whisper_v4_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_max_codex_2_lite_integrates_with",
          "source": "tool:tensorrt_llm_max",
          "target": "model:codex_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-06_meta_ai_blog_5196",
              "url": "https://ai.meta.com/blog/2026/01/06/tensorrt_llm_max_codex_2_lite",
              "published": "2026-01-06",
              "snippet": "TensorRT-LLM Max now supports Codex 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_v2_the_stack_v2_core_trained_on",
          "source": "model:command_r_plus_v2",
          "target": "dataset:the_stack_v2_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_tree_of_thought_mini_uses_tech",
          "source": "model:whisper_v4_core",
          "target": "tech:tree_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-11_hugging_face_bl_1182",
              "url": "https://huggingface.co/blog/2026/01/11/whisper_v4_core_tree_of_though",
              "published": "2026-01-11",
              "snippet": "Whisper v4 Core leverages Tree of Thought Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-13_the_gradient_8371",
              "url": "https://thegradient.pub/2026/01/13/whisper_v4_core_tree_of_though",
              "published": "2026-01-13",
              "snippet": "Technical details reveal Whisper v4 Core relies heavily on Tree of Thought Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_mini_gsm8k_edge_evaluated_on",
          "source": "model:sora_2_mini",
          "target": "benchmark:gsm8k_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.97
        }
      },
      {
        "data": {
          "id": "e:copilot_chain_of_thought_mini_uses_tech",
          "source": "tool:copilot",
          "target": "tech:chain_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:localai_litellm_plus_integrates_with",
          "source": "tool:localai",
          "target": "tool:litellm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-23_weights_and_bia_8963",
              "url": "https://wandb.ai/articles/2026/01/23/localai_litellm_plus",
              "published": "2026-01-23",
              "snippet": "LocalAI now supports LiteLLM Plus with full feature parity..."
            },
            {
              "docId": "2026-01-25_mit_technology__5489",
              "url": "https://technologyreview.com/2026/01/25/localai_litellm_plus",
              "published": "2026-01-25",
              "snippet": "LocalAI now supports LiteLLM Plus with full feature parity..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_4603",
              "url": "https://anthropic.com/news/2026/01/25/localai_litellm_plus",
              "published": "2026-01-25",
              "snippet": "The latest release of LocalAI adds native LiteLLM Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_qwen_3_next_measures",
          "source": "benchmark:math",
          "target": "model:qwen_3_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:crewai_max_midjourney_v7_mini_integrates_with",
          "source": "tool:crewai_max",
          "target": "model:midjourney_v7_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-18_weights_and_bia_2937",
              "url": "https://wandb.ai/articles/2026/01/18/crewai_max_midjourney_v7_mini",
              "published": "2026-01-18",
              "snippet": "CrewAI Max now supports Midjourney V7 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_tool_use_core_uses_tech",
          "source": "model:mixtral_8x22b_plus",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:llamaindex_core_phi_4_pro_integrates_with",
          "source": "tool:llamaindex_core",
          "target": "model:phi_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-10_openai_blog_5282",
              "url": "https://openai.com/blog/2026/01/10/llamaindex_core_phi_4_pro",
              "published": "2026-01-10",
              "snippet": "The latest release of LlamaIndex Core adds native Phi-4 Pro integration..."
            },
            {
              "docId": "2026-01-14_bloomberg_6260",
              "url": "https://bloomberg.com/technology/2026/01/14/llamaindex_core_phi_4_pro",
              "published": "2026-01-14",
              "snippet": "LlamaIndex Core now supports Phi-4 Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__jamba_2_next_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:jamba_2_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-29_weights_and_bia_6546",
              "url": "https://wandb.ai/articles/2025/12/29/scaling_laws_for_neural_langua",
              "published": "2025-12-29",
              "snippet": "Scaling Laws for Neural Language Models (2025) achieves 88% on Jamba 2 Next, setting a new record..."
            },
            {
              "docId": "2026-01-19_ars_technica_3651",
              "url": "https://arstechnica.com/2026/01/19/scaling_laws_for_neural_langua",
              "published": "2026-01-19",
              "snippet": "On the Jamba 2 Next benchmark, Scaling Laws for Neural Language Models (2025) scored 97%..."
            },
            {
              "docId": "2026-01-19_the_verge_3412",
              "url": "https://theverge.com/2026/01/19/scaling_laws_for_neural_langua",
              "published": "2026-01-19",
              "snippet": "Evaluation results show Scaling Laws for Neural Language Models (2025) reaching 99% on Jamba 2 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_codex_2_v2_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:codex_2_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-23_mit_technology__8496",
              "url": "https://technologyreview.com/2025/12/23/gpt4all_codex_2_v2",
              "published": "2025-12-23",
              "snippet": "gpt4all announced official support for Codex 2 v2..."
            },
            {
              "docId": "2026-01-12_meta_ai_blog_2604",
              "url": "https://ai.meta.com/blog/2026/01/12/gpt4all_codex_2_v2",
              "published": "2026-01-12",
              "snippet": "gpt4all announced official support for Codex 2 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_next_red_teaming_ultra_uses_tech",
          "source": "tool:weights_and_biases_next",
          "target": "tech:red_teaming_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-17_microsoft_resea_6466",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/17/weights_and_biases_next_red_te",
              "published": "2026-01-17",
              "snippet": "Technical details reveal Weights & Biases Next relies heavily on Red Teaming Ultra..."
            },
            {
              "docId": "2026-01-20_the_gradient_6235",
              "url": "https://thegradient.pub/2026/01/20/weights_and_biases_next_red_te",
              "published": "2026-01-20",
              "snippet": "Technical details reveal Weights & Biases Next relies heavily on Red Teaming Ultra..."
            },
            {
              "docId": "2026-01-22_nextgov_6197",
              "url": "https://nextgov.com/2026/01/22/weights_and_biases_next_red_te",
              "published": "2026-01-22",
              "snippet": "Technical details reveal Weights & Biases Next relies heavily on Red Teaming Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_v2_wikipedia_dump_2025_max_trained_on",
          "source": "model:nemotron_5_v2",
          "target": "dataset:wikipedia_dump_2025_max",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:transformers_gemini_ultra_2_plus_integrates_with",
          "source": "repo:transformers",
          "target": "model:gemini_ultra_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_humaneval_plus_evaluated_on",
          "source": "model:jamba_2_mini",
          "target": "benchmark:humaneval_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:aya_3_pro_mt_bench_pro_evaluated_on",
          "source": "model:aya_3_pro",
          "target": "benchmark:mt_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-24_the_verge_1887",
              "url": "https://theverge.com/2026/01/24/aya_3_pro_mt_bench_pro",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Aya 3 Pro reaching 70% on MT-Bench Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_aya_3_pro_measures",
          "source": "benchmark:gpqa",
          "target": "model:aya_3_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-22_microsoft_resea_7209",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/22/gpqa_aya_3_pro",
              "published": "2026-01-22",
              "snippet": "GPQA has become the standard for evaluating Aya 3 Pro..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_4497",
              "url": "https://huggingface.co/blog/2026/01/24/gpqa_aya_3_pro",
              "published": "2026-01-24",
              "snippet": "The GPQA benchmark measures Aya 3 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_lite_slimpajama_lite_trained_on",
          "source": "model:gemma_3_lite",
          "target": "dataset:slimpajama_lite",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-25_anthropic_blog_5418",
              "url": "https://anthropic.com/news/2025/12/25/gemma_3_lite_slimpajama_lite",
              "published": "2025-12-25",
              "snippet": "Gemma 3 Lite was trained on SlimPajama Lite comprising billions of tokens..."
            },
            {
              "docId": "2026-01-13_mit_technology__6405",
              "url": "https://technologyreview.com/2026/01/13/gemma_3_lite_slimpajama_lite",
              "published": "2026-01-13",
              "snippet": "Gemma 3 Lite utilized SlimPajama Lite as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_pro_codex_2_mini_evaluated_on",
          "source": "paper:direct_preference_optimization_pro",
          "target": "model:codex_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-22_wired_6966",
              "url": "https://wired.com/2026/01/22/direct_preference_optimization",
              "published": "2026-01-22",
              "snippet": "On the Codex 2 Mini benchmark, Direct Preference Optimization Pro scored 85%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_v2_distillation_edge_uses_tech",
          "source": "model:stable_diffusion_4_v2",
          "target": "tech:distillation_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:palm_3_v2_claude_opus_45_plus_depends_on",
          "source": "model:palm_3_v2",
          "target": "model:claude_opus_45_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_synthetic_data_generation_pro_uses_tech",
          "source": "repo:transformers_pro",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_1304",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/transformers_pro_synthetic_dat",
              "published": "2026-01-18",
              "snippet": "transformers Pro leverages Synthetic Data Generation Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-20_langchain_blog_1549",
              "url": "https://blog.langchain.dev/2026/01/20/transformers_pro_synthetic_dat",
              "published": "2026-01-20",
              "snippet": "transformers Pro leverages Synthetic Data Generation Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_anthropic_blog_5129",
              "url": "https://anthropic.com/news/2026/01/23/transformers_pro_synthetic_dat",
              "published": "2026-01-23",
              "snippet": "Under the hood, transformers Pro implements Synthetic Data Generation Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-23_arxiv_4499",
              "url": "https://arxiv.org/abs/2026/01/23/transformers_pro_synthetic_dat",
              "published": "2026-01-23",
              "snippet": "Under the hood, transformers Pro implements Synthetic Data Generation Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_edge_the_pile_core_trained_on",
          "source": "model:mixtral_8x22b_edge",
          "target": "dataset:the_pile_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_flash_attention_core_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "tech:flash_attention_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:grok_3_core_arc_agi_core_evaluated_on",
          "source": "model:grok_3_core",
          "target": "benchmark:arc_agi_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.97
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_wikipedia_dump_2025_core_trained_on",
          "source": "model:aya_3_max",
          "target": "dataset:wikipedia_dump_2025_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.46,
          "evidence": [
            {
              "docId": "2026-01-17_anthropic_blog_6107",
              "url": "https://anthropic.com/news/2026/01/17/aya_3_max_wikipedia_dump_2025_",
              "published": "2026-01-17",
              "snippet": "The training corpus for Aya 3 Max includes Wikipedia Dump 2025 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_codex_2_pro_integrates_with",
          "source": "repo:vllm",
          "target": "model:codex_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-21_reuters_8229",
              "url": "https://reuters.com/technology/2026/01/21/vllm_codex_2_pro",
              "published": "2026-01-21",
              "snippet": "vllm announced official support for Codex 2 Pro..."
            },
            {
              "docId": "2026-01-25_wired_2423",
              "url": "https://wired.com/2026/01/25/vllm_codex_2_pro",
              "published": "2026-01-25",
              "snippet": "vllm announced official support for Codex 2 Pro..."
            },
            {
              "docId": "2026-01-25_reuters_2605",
              "url": "https://reuters.com/technology/2026/01/25/vllm_codex_2_pro",
              "published": "2026-01-25",
              "snippet": "vllm announced official support for Codex 2 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_max_palm_3_measures",
          "source": "benchmark:gpqa_max",
          "target": "model:palm_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-23_the_gradient_2024",
              "url": "https://thegradient.pub/2026/01/23/gpqa_max_palm_3",
              "published": "2026-01-23",
              "snippet": "GPQA Max has become the standard for evaluating PaLM 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_group_query_attention_plus_uses_tech",
          "source": "paper:textbooks_are_all_you_need",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-18_techcrunch_9445",
              "url": "https://techcrunch.com/2025/12/18/textbooks_are_all_you_need_gro",
              "published": "2025-12-18",
              "snippet": "Under the hood, Textbooks Are All You Need implements Group Query Attention Plus for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_core_gpt_4o_mini_2_lite_integrates_with",
          "source": "tool:dify_core",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-04_arxiv_8138",
              "url": "https://arxiv.org/abs/2026/01/04/dify_core_gpt_4o_mini_2_lite",
              "published": "2026-01-04",
              "snippet": "Dify Core announced official support for GPT-4o Mini 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_litellm_pro_integrates_with",
          "source": "tool:langchain_edge",
          "target": "tool:litellm_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:qwen_3_core_swe_bench_pro_evaluated_on",
          "source": "model:qwen_3_core",
          "target": "benchmark:swe_bench_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-11-20_microsoft_resea_2861",
              "url": "https://microsoft.com/en-us/research/blog/2025/11/20/qwen_3_core_swe_bench_pro",
              "published": "2025-11-20",
              "snippet": "On the SWE-bench Pro benchmark, Qwen-3 Core scored 86%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_lite_cody_ultra_integrates_with",
          "source": "tool:vllm_lite",
          "target": "tool:cody_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_rlhf_mini_uses_tech",
          "source": "model:llama_4_plus",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-18_hugging_face_bl_2866",
              "url": "https://huggingface.co/blog/2026/01/18/llama_4_plus_rlhf_mini",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Llama 4 Plus relies heavily on RLHF Mini..."
            },
            {
              "docId": "2026-01-22_mit_technology__3998",
              "url": "https://technologyreview.com/2026/01/22/llama_4_plus_rlhf_mini",
              "published": "2026-01-22",
              "snippet": "Technical details reveal Llama 4 Plus relies heavily on RLHF Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_quantization_lite_uses_tech",
          "source": "model:llama_4_pro",
          "target": "tech:quantization_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-07_bloomberg_5312",
              "url": "https://bloomberg.com/technology/2026/01/07/llama_4_pro_quantization_lite",
              "published": "2026-01-07",
              "snippet": "Llama 4 Pro leverages Quantization Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_gpt_5_plus_depends_on",
          "source": "model:gemma_3_mini",
          "target": "model:gpt_5_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_claude_opus_45_v2_depends_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "model:claude_opus_45_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_nemotron_5_v2_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:nemotron_5_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_semantic_kernel_v2_integrates_with",
          "source": "tool:gradio_plus",
          "target": "tool:semantic_kernel_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_aya_3_max_integrates_with",
          "source": "repo:llamacpp_mini",
          "target": "model:aya_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:vllm_max_deepseek_v3_core_integrates_with",
          "source": "repo:vllm_max",
          "target": "model:deepseek_v3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-18_reuters_7380",
              "url": "https://reuters.com/technology/2026/01/18/vllm_max_deepseek_v3_core",
              "published": "2026-01-18",
              "snippet": "The latest release of vllm Max adds native DeepSeek-V3 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_plus_command_r_plus_lite_integrates_with",
          "source": "tool:weights_and_biases_plus",
          "target": "model:command_r_plus_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_6540",
              "url": "https://blog.google/technology/ai/2026/01/25/weights_and_biases_plus_comman",
              "published": "2026-01-25",
              "snippet": "The latest release of Weights & Biases Plus adds native Command R+ Lite integration..."
            },
            {
              "docId": "2026-01-25_arxiv_4218",
              "url": "https://arxiv.org/abs/2026/01/25/weights_and_biases_plus_comman",
              "published": "2026-01-25",
              "snippet": "The latest release of Weights & Biases Plus adds native Command R+ Lite integration..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_8145",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/weights_and_biases_plus_comman",
              "published": "2026-01-25",
              "snippet": "The latest release of Weights & Biases Plus adds native Command R+ Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_roots_next_trained_on",
          "source": "model:qwen_3_next",
          "target": "dataset:roots_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-10_reuters_7869",
              "url": "https://reuters.com/technology/2026/01/10/qwen_3_next_roots_next",
              "published": "2026-01-10",
              "snippet": "Qwen-3 Next was trained on ROOTS Next comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_whisper_v4_mini_integrates_with",
          "source": "repo:transformers_next",
          "target": "model:whisper_v4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-21_hugging_face_bl_2021",
              "url": "https://huggingface.co/blog/2026/01/21/transformers_next_whisper_v4_m",
              "published": "2026-01-21",
              "snippet": "transformers Next announced official support for Whisper v4 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_command_r_plus_integrates_with",
          "source": "repo:gpt4all_v2",
          "target": "model:command_r_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-23_anthropic_blog_6655",
              "url": "https://anthropic.com/news/2025/12/23/gpt4all_v2_command_r_plus",
              "published": "2025-12-23",
              "snippet": "gpt4all v2 now supports Command R+ with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_v2_llama_4_next_integrates_with",
          "source": "repo:localai_v2",
          "target": "model:llama_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-06_weights_and_bia_6024",
              "url": "https://wandb.ai/articles/2026/01/06/localai_v2_llama_4_next",
              "published": "2026-01-06",
              "snippet": "The latest release of LocalAI v2 adds native Llama 4 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_next_flowise_integrates_with",
          "source": "tool:haystack_next",
          "target": "tool:flowise",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-19_reuters_5988",
              "url": "https://reuters.com/technology/2026/01/19/haystack_next_flowise",
              "published": "2026-01-19",
              "snippet": "The latest release of Haystack Next adds native Flowise integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_plus_fineweb_pro_trained_on",
          "source": "model:nemotron_5_plus",
          "target": "dataset:fineweb_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_next_mc4_trained_on",
          "source": "model:mixtral_8x22b_next",
          "target": "dataset:mc4",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:common_crawl_core_group_query_attention_plus_uses_tech",
          "source": "dataset:common_crawl_core",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_flash_attention_max_uses_tech",
          "source": "model:claude_opus_45_lite",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-17_mit_technology__7750",
              "url": "https://technologyreview.com/2026/01/17/claude_opus_45_lite_flash_atte",
              "published": "2026-01-17",
              "snippet": "Claude Opus 4.5 Lite leverages Flash Attention Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_ultra_midjourney_v7_ultra_integrates_with",
          "source": "tool:copilot_ultra",
          "target": "model:midjourney_v7_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-11_microsoft_resea_2222",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/11/copilot_ultra_midjourney_v7_ul",
              "published": "2026-01-11",
              "snippet": "The latest release of Copilot Ultra adds native Midjourney V7 Ultra integration..."
            },
            {
              "docId": "2026-01-23_anthropic_blog_5877",
              "url": "https://anthropic.com/news/2026/01/23/copilot_ultra_midjourney_v7_ul",
              "published": "2026-01-23",
              "snippet": "The latest release of Copilot Ultra adds native Midjourney V7 Ultra integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_next_rotary_position_embedding_max_uses_tech",
          "source": "model:sora_2_next",
          "target": "tech:rotary_position_embedding_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:transformers_gemini_ultra_2_core_integrates_with",
          "source": "repo:transformers",
          "target": "model:gemini_ultra_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-04_weights_and_bia_6416",
              "url": "https://wandb.ai/articles/2026/01/04/transformers_gemini_ultra_2_co",
              "published": "2026-01-04",
              "snippet": "transformers announced official support for Gemini Ultra 2 Core..."
            },
            {
              "docId": "2026-01-22_meta_ai_blog_5372",
              "url": "https://ai.meta.com/blog/2026/01/22/transformers_gemini_ultra_2_co",
              "published": "2026-01-22",
              "snippet": "transformers now supports Gemini Ultra 2 Core with full feature parity..."
            },
            {
              "docId": "2026-01-24_reuters_1811",
              "url": "https://reuters.com/technology/2026/01/24/transformers_gemini_ultra_2_co",
              "published": "2026-01-24",
              "snippet": "transformers now supports Gemini Ultra 2 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_lite_command_r_plus_plus_measures",
          "source": "benchmark:gpqa_lite",
          "target": "model:command_r_plus_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-25_langchain_blog_5694",
              "url": "https://blog.langchain.dev/2026/01/25/gpqa_lite_command_r_plus_plus",
              "published": "2026-01-25",
              "snippet": "GPQA Lite has become the standard for evaluating Command R+ Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_tokenizer_bpe_next_uses_tech",
          "source": "repo:vllm",
          "target": "tech:tokenizer_bpe_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_rlhf_plus_uses_tech",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "tech:rlhf_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-03_venturebeat_7262",
              "url": "https://venturebeat.com/2026/01/03/textbooks_are_all_you_need_nex",
              "published": "2026-01-03",
              "snippet": "Technical details reveal Textbooks Are All You Need Next relies heavily on RLHF Plus..."
            },
            {
              "docId": "2026-01-21_nextgov_3553",
              "url": "https://nextgov.com/2026/01/21/textbooks_are_all_you_need_nex",
              "published": "2026-01-21",
              "snippet": "Under the hood, Textbooks Are All You Need Next implements RLHF Plus for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_plus_tool_use_core_uses_tech",
          "source": "dataset:the_pile_plus",
          "target": "tech:tool_use_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_dolma_pro_trained_on",
          "source": "model:midjourney_v7_core",
          "target": "dataset:dolma_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:gpqa_whisper_v4_mini_measures",
          "source": "benchmark:gpqa",
          "target": "model:whisper_v4_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-07_ars_technica_7194",
              "url": "https://arstechnica.com/2026/01/07/gpqa_whisper_v4_mini",
              "published": "2026-01-07",
              "snippet": "The GPQA benchmark measures Whisper v4 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-22_reuters_1923",
              "url": "https://reuters.com/technology/2026/01/22/gpqa_whisper_v4_mini",
              "published": "2026-01-22",
              "snippet": "The GPQA benchmark measures Whisper v4 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-23_venturebeat_1458",
              "url": "https://venturebeat.com/2026/01/23/gpqa_whisper_v4_mini",
              "published": "2026-01-23",
              "snippet": "GPQA has become the standard for evaluating Whisper v4 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_refinedweb_ultra_trained_on",
          "source": "model:gpt_5",
          "target": "dataset:refinedweb_ultra",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-15_langchain_blog_9844",
              "url": "https://blog.langchain.dev/2026/01/15/gpt_5_refinedweb_ultra",
              "published": "2026-01-15",
              "snippet": "GPT-5 utilized RefinedWeb Ultra as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_v2_codex_2_core_integrates_with",
          "source": "repo:localai_v2",
          "target": "model:codex_2_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-21_openai_blog_6206",
              "url": "https://openai.com/blog/2026/01/21/localai_v2_codex_2_core",
              "published": "2026-01-21",
              "snippet": "LocalAI v2 announced official support for Codex 2 Core..."
            },
            {
              "docId": "2026-01-22_the_verge_1276",
              "url": "https://theverge.com/2026/01/22/localai_v2_codex_2_core",
              "published": "2026-01-22",
              "snippet": "LocalAI v2 now supports Codex 2 Core with full feature parity..."
            },
            {
              "docId": "2026-01-23_bloomberg_9369",
              "url": "https://bloomberg.com/technology/2026/01/23/localai_v2_codex_2_core",
              "published": "2026-01-23",
              "snippet": "The latest release of LocalAI v2 adds native Codex 2 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_plus_rlhf_max_uses_tech",
          "source": "model:aya_3_plus",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:autogpt_speculative_decoding_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-25_wired_9073",
              "url": "https://wired.com/2026/01/25/autogpt_speculative_decoding",
              "published": "2026-01-25",
              "snippet": "AutoGPT leverages Speculative Decoding to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_the_gradient_2411",
              "url": "https://thegradient.pub/2026/01/25/autogpt_speculative_decoding",
              "published": "2026-01-25",
              "snippet": "Technical details reveal AutoGPT relies heavily on Speculative Decoding..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_claude_opus_45_max_integrates_with",
          "source": "repo:transformers_next",
          "target": "model:claude_opus_45_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-02_google_ai_blog_2156",
              "url": "https://blog.google/technology/ai/2026/01/02/transformers_next_claude_opus_",
              "published": "2026-01-02",
              "snippet": "transformers Next now supports Claude Opus 4.5 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_mini_tokenizer_bpe_next_uses_tech",
          "source": "repo:localai_mini",
          "target": "tech:tokenizer_bpe_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2025-12-04_the_verge_9944",
              "url": "https://theverge.com/2025/12/04/localai_mini_tokenizer_bpe_nex",
              "published": "2025-12-04",
              "snippet": "Under the hood, LocalAI Mini implements Tokenizer BPE Next for improved efficiency..."
            },
            {
              "docId": "2026-01-09_reuters_1628",
              "url": "https://reuters.com/technology/2026/01/09/localai_mini_tokenizer_bpe_nex",
              "published": "2026-01-09",
              "snippet": "Technical details reveal LocalAI Mini relies heavily on Tokenizer BPE Next..."
            },
            {
              "docId": "2026-01-12_meta_ai_blog_6976",
              "url": "https://ai.meta.com/blog/2026/01/12/localai_mini_tokenizer_bpe_nex",
              "published": "2026-01-12",
              "snippet": "Under the hood, LocalAI Mini implements Tokenizer BPE Next for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__mixture_of_experts_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-21_hugging_face_bl_6139",
              "url": "https://huggingface.co/blog/2025/12/21/scaling_laws_for_neural_langua",
              "published": "2025-12-21",
              "snippet": "Scaling Laws for Neural Language Models (2025) leverages Mixture of Experts to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-02_wired_4537",
              "url": "https://wired.com/2026/01/02/scaling_laws_for_neural_langua",
              "published": "2026-01-02",
              "snippet": "Scaling Laws for Neural Language Models (2025) leverages Mixture of Experts to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_command_r_plus_edge_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:command_r_plus_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:swe_bench_plus_llama_4_ultra_measures",
          "source": "benchmark:swe_bench_plus",
          "target": "model:llama_4_ultra",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_mmlu_ultra_evaluated_on",
          "source": "model:yi_large_next",
          "target": "benchmark:mmlu_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-23_the_verge_5071",
              "url": "https://theverge.com/2026/01/23/yi_large_next_mmlu_ultra",
              "published": "2026-01-23",
              "snippet": "On the MMLU Ultra benchmark, Yi-Large Next scored 75%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_gpt_5_core_integrates_with",
          "source": "repo:langchain_edge",
          "target": "model:gpt_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-01_the_verge_7949",
              "url": "https://theverge.com/2026/01/01/langchain_edge_gpt_5_core",
              "published": "2026-01-01",
              "snippet": "The latest release of langchain Edge adds native GPT-5 Core integration..."
            },
            {
              "docId": "2026-01-11_reuters_9228",
              "url": "https://reuters.com/technology/2026/01/11/langchain_edge_gpt_5_core",
              "published": "2026-01-11",
              "snippet": "langchain Edge now supports GPT-5 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_palm_3_max_integrates_with",
          "source": "repo:vllm",
          "target": "model:palm_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_swe_bench_plus_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:swe_bench_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_2812",
              "url": "https://anthropic.com/news/2026/01/25/claude_opus_45_swe_bench_plus",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 achieves 78% on SWE-bench Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_midjourney_v7_core_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "model:midjourney_v7_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-13_anthropic_blog_7597",
              "url": "https://anthropic.com/news/2026/01/13/attention_is_all_you_need_v2_m",
              "published": "2026-01-13",
              "snippet": "On the Midjourney V7 Core benchmark, Attention Is All You Need v2 scored 97%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_edge_rlhf_max_uses_tech",
          "source": "tool:dify_edge",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:localai_lite_synthetic_data_generation_pro_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:synthetic_data_generation_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-15_hugging_face_bl_2104",
              "url": "https://huggingface.co/blog/2026/01/15/localai_lite_synthetic_data_ge",
              "published": "2026-01-15",
              "snippet": "LocalAI Lite leverages Synthetic Data Generation Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_transformer_architecture_mini_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:transformer_architecture_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-26_the_gradient_2388",
              "url": "https://thegradient.pub/2025/12/26/llm_agents:_a_survey_core_tran",
              "published": "2025-12-26",
              "snippet": "Technical details reveal LLM Agents: A Survey Core relies heavily on Transformer Architecture Mini..."
            },
            {
              "docId": "2026-01-08_google_ai_blog_4857",
              "url": "https://blog.google/technology/ai/2026/01/08/llm_agents:_a_survey_core_tran",
              "published": "2026-01-08",
              "snippet": "Technical details reveal LLM Agents: A Survey Core relies heavily on Transformer Architecture Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_max_distillation_pro_uses_tech",
          "source": "model:nemotron_5_max",
          "target": "tech:distillation_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_edge_qwen_3_next_measures",
          "source": "benchmark:lmsys_chatbot_arena_edge",
          "target": "model:qwen_3_next",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-31_nextgov_7378",
              "url": "https://nextgov.com/2025/12/31/lmsys_chatbot_arena_edge_qwen_",
              "published": "2025-12-31",
              "snippet": "LMSYS Chatbot Arena Edge has become the standard for evaluating Qwen-3 Next..."
            },
            {
              "docId": "2026-01-19_wired_2099",
              "url": "https://wired.com/2026/01/19/lmsys_chatbot_arena_edge_qwen_",
              "published": "2026-01-19",
              "snippet": "The LMSYS Chatbot Arena Edge benchmark measures Qwen-3 Next across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_max_rotary_position_embedding_pro_uses_tech",
          "source": "model:gpt_4o_mini_2_max",
          "target": "tech:rotary_position_embedding_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_phi_4_edge_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:phi_4_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-11-29_mit_technology__9485",
              "url": "https://technologyreview.com/2025/11/29/textbooks_are_all_you_need_min",
              "published": "2025-11-29",
              "snippet": "On the Phi-4 Edge benchmark, Textbooks Are All You Need Mini scored 92%..."
            },
            {
              "docId": "2026-01-04_venturebeat_9829",
              "url": "https://venturebeat.com/2026/01/04/textbooks_are_all_you_need_min",
              "published": "2026-01-04",
              "snippet": "On the Phi-4 Edge benchmark, Textbooks Are All You Need Mini scored 81%..."
            },
            {
              "docId": "2026-01-24_arxiv_7488",
              "url": "https://arxiv.org/abs/2026/01/24/textbooks_are_all_you_need_min",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Textbooks Are All You Need Mini reaching 95% on Phi-4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_slimpajama_core_trained_on",
          "source": "model:palm_3",
          "target": "dataset:slimpajama_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_mixtral_8x22b_edge_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:mixtral_8x22b_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_pro_sliding_window_attention_next_uses_tech",
          "source": "tool:weights_and_biases_pro",
          "target": "tech:sliding_window_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_speculative_decoding_pro_uses_tech",
          "source": "model:nemotron_5",
          "target": "tech:speculative_decoding_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2025-12-07_nextgov_9440",
              "url": "https://nextgov.com/2025/12/07/nemotron_5_speculative_decodin",
              "published": "2025-12-07",
              "snippet": "Nemotron-5 leverages Speculative Decoding Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-10_meta_ai_blog_6523",
              "url": "https://ai.meta.com/blog/2025/12/10/nemotron_5_speculative_decodin",
              "published": "2025-12-10",
              "snippet": "Technical details reveal Nemotron-5 relies heavily on Speculative Decoding Pro..."
            },
            {
              "docId": "2026-01-13_meta_ai_blog_8179",
              "url": "https://ai.meta.com/blog/2026/01/13/nemotron_5_speculative_decodin",
              "published": "2026-01-13",
              "snippet": "Under the hood, Nemotron-5 implements Speculative Decoding Pro for improved efficiency..."
            },
            {
              "docId": "2026-01-17_techcrunch_9556",
              "url": "https://techcrunch.com/2026/01/17/nemotron_5_speculative_decodin",
              "published": "2026-01-17",
              "snippet": "Technical details reveal Nemotron-5 relies heavily on Speculative Decoding Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_gemini_ultra_2_edge_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:gemini_ultra_2_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-03_weights_and_bia_6583",
              "url": "https://wandb.ai/articles/2026/01/03/textbooks_are_all_you_need_min",
              "published": "2026-01-03",
              "snippet": "Textbooks Are All You Need Mini achieves 98% on Gemini Ultra 2 Edge, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_deepseek_v3_max_depends_on",
          "source": "model:codex_2_core",
          "target": "model:deepseek_v3_max",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_max_codex_2_core_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_max",
          "target": "model:codex_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-17_google_ai_blog_3108",
              "url": "https://blog.google/technology/ai/2026/01/17/textbooks_are_all_you_need_max",
              "published": "2026-01-17",
              "snippet": "Evaluation results show Textbooks Are All You Need Max reaching 87% on Codex 2 Core..."
            },
            {
              "docId": "2026-01-22_techcrunch_9138",
              "url": "https://techcrunch.com/2026/01/22/textbooks_are_all_you_need_max",
              "published": "2026-01-22",
              "snippet": "Textbooks Are All You Need Max achieves 82% on Codex 2 Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_starcoder_data_core_trained_on",
          "source": "model:llama_4_pro",
          "target": "dataset:starcoder_data_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_grok_3_core_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:grok_3_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:qwen_3_v2_aya_3_depends_on",
          "source": "model:qwen_3_v2",
          "target": "model:aya_3",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:the_pile_plus_qlora_pro_uses_tech",
          "source": "dataset:the_pile_plus",
          "target": "tech:qlora_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_plus_gemma_3_edge_depends_on",
          "source": "model:claude_sonnet_4_plus",
          "target": "model:gemma_3_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_command_r_plus_pro_integrates_with",
          "source": "repo:autogpt_plus",
          "target": "model:command_r_plus_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-09_techcrunch_3673",
              "url": "https://techcrunch.com/2026/01/09/autogpt_plus_command_r_plus_pr",
              "published": "2026-01-09",
              "snippet": "AutoGPT Plus announced official support for Command R+ Pro..."
            },
            {
              "docId": "2026-01-25_mit_technology__1557",
              "url": "https://technologyreview.com/2026/01/25/autogpt_plus_command_r_plus_pr",
              "published": "2026-01-25",
              "snippet": "The latest release of AutoGPT Plus adds native Command R+ Pro integration..."
            },
            {
              "docId": "2026-01-25_ars_technica_2471",
              "url": "https://arstechnica.com/2026/01/25/autogpt_plus_command_r_plus_pr",
              "published": "2026-01-25",
              "snippet": "The latest release of AutoGPT Plus adds native Command R+ Pro integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:swe_bench_max_falcon_3_lite_measures",
          "source": "benchmark:swe_bench_max",
          "target": "model:falcon_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-30_langchain_blog_1076",
              "url": "https://blog.langchain.dev/2025/12/30/swe_bench_max_falcon_3_lite",
              "published": "2025-12-30",
              "snippet": "The SWE-bench Max benchmark measures Falcon 3 Lite across multiple tasks..."
            },
            {
              "docId": "2026-01-08_weights_and_bia_5014",
              "url": "https://wandb.ai/articles/2026/01/08/swe_bench_max_falcon_3_lite",
              "published": "2026-01-08",
              "snippet": "The SWE-bench Max benchmark measures Falcon 3 Lite across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_plus_gemini_ultra_2_plus_measures",
          "source": "benchmark:mt_bench_plus",
          "target": "model:gemini_ultra_2_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:grok_3_ultra_swe_bench_lite_evaluated_on",
          "source": "model:grok_3_ultra",
          "target": "benchmark:swe_bench_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-13_weights_and_bia_1654",
              "url": "https://wandb.ai/articles/2026/01/13/grok_3_ultra_swe_bench_lite",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Grok-3 Ultra reaching 73% on SWE-bench Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dify_lite_falcon_3_ultra_integrates_with",
          "source": "tool:dify_lite",
          "target": "model:falcon_3_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-05_wired_4114",
              "url": "https://wired.com/2026/01/05/dify_lite_falcon_3_ultra",
              "published": "2026-01-05",
              "snippet": "The latest release of Dify Lite adds native Falcon 3 Ultra integration..."
            },
            {
              "docId": "2026-01-16_reuters_5745",
              "url": "https://reuters.com/technology/2026/01/16/dify_lite_falcon_3_ultra",
              "published": "2026-01-16",
              "snippet": "Dify Lite now supports Falcon 3 Ultra with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_aya_3_pro_integrates_with",
          "source": "tool:crewai",
          "target": "model:aya_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-22_the_gradient_4145",
              "url": "https://thegradient.pub/2026/01/22/crewai_aya_3_pro",
              "published": "2026-01-22",
              "snippet": "CrewAI announced official support for Aya 3 Pro..."
            },
            {
              "docId": "2026-01-23_reuters_9355",
              "url": "https://reuters.com/technology/2026/01/23/crewai_aya_3_pro",
              "published": "2026-01-23",
              "snippet": "CrewAI now supports Aya 3 Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:c4_retrieval_augmented_generation_uses_tech",
          "source": "dataset:c4",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:vllm_max_flash_attention_uses_tech",
          "source": "repo:vllm_max",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-30_bloomberg_9736",
              "url": "https://bloomberg.com/technology/2025/12/30/vllm_max_flash_attention",
              "published": "2025-12-30",
              "snippet": "vllm Max leverages Flash Attention to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-03_reuters_2236",
              "url": "https://reuters.com/technology/2026/01/03/vllm_max_flash_attention",
              "published": "2026-01-03",
              "snippet": "Under the hood, vllm Max implements Flash Attention for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_langchain_edge_integrates_with",
          "source": "tool:vllm_pro",
          "target": "tool:langchain_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-19_anthropic_blog_5669",
              "url": "https://anthropic.com/news/2026/01/19/vllm_pro_langchain_edge",
              "published": "2026-01-19",
              "snippet": "The latest release of vLLM Pro adds native LangChain Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_pro_gemini_ultra_2_measures",
          "source": "benchmark:truthfulqa_pro",
          "target": "model:gemini_ultra_2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-19_openai_blog_4715",
              "url": "https://openai.com/blog/2026/01/19/truthfulqa_pro_gemini_ultra_2",
              "published": "2026-01-19",
              "snippet": "TruthfulQA Pro provides standardized evaluation of Gemini Ultra 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_nemotron_5_max_integrates_with",
          "source": "repo:gpt4all_v2",
          "target": "model:nemotron_5_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-14_anthropic_blog_5831",
              "url": "https://anthropic.com/news/2026/01/14/gpt4all_v2_nemotron_5_max",
              "published": "2026-01-14",
              "snippet": "gpt4all v2 now supports Nemotron-5 Max with full feature parity..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_5849",
              "url": "https://blogs.nvidia.com/2026/01/19/gpt4all_v2_nemotron_5_max",
              "published": "2026-01-19",
              "snippet": "gpt4all v2 announced official support for Nemotron-5 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_jamba_2_ultra_integrates_with",
          "source": "repo:open_interpreter",
          "target": "model:jamba_2_ultra",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_sparse_attention_uses_tech",
          "source": "repo:ollama_ultra",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-24_bloomberg_5184",
              "url": "https://bloomberg.com/technology/2025/12/24/ollama_ultra_sparse_attention",
              "published": "2025-12-24",
              "snippet": "Technical details reveal ollama Ultra relies heavily on Sparse Attention..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_lite_transformer_architecture_edge_uses_tech",
          "source": "tool:haystack_lite",
          "target": "tech:transformer_architecture_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:qwen_3_lite_alpacaeval_2_lite_evaluated_on",
          "source": "model:qwen_3_lite",
          "target": "benchmark:alpacaeval_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2026-01-13_the_verge_7935",
              "url": "https://theverge.com/2026/01/13/qwen_3_lite_alpacaeval_2_lite",
              "published": "2026-01-13",
              "snippet": "Evaluation results show Qwen-3 Lite reaching 77% on AlpacaEval 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_edge_codex_2_plus_measures",
          "source": "benchmark:arc_agi_edge",
          "target": "model:codex_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-24_google_ai_blog_2224",
              "url": "https://blog.google/technology/ai/2026/01/24/arc_agi_edge_codex_2_plus",
              "published": "2026-01-24",
              "snippet": "The ARC-AGI Edge benchmark measures Codex 2 Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_next_nemotron_5_core_depends_on",
          "source": "model:grok_3_next",
          "target": "model:nemotron_5_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_pro_yi_large_mini_depends_on",
          "source": "model:deepseek_v3_pro",
          "target": "model:yi_large_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_jamba_2_lite_integrates_with",
          "source": "tool:cursor_mini",
          "target": "model:jamba_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_mini_rotary_position_embedding_max_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_mini",
          "target": "tech:rotary_position_embedding_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-25_wired_1751",
              "url": "https://wired.com/2026/01/25/attention_is_all_you_need_v2_m",
              "published": "2026-01-25",
              "snippet": "Attention Is All You Need v2 Mini leverages Rotary Position Embedding Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_lora_max_uses_tech",
          "source": "tool:langchain_lite",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2025-12-22_microsoft_resea_7809",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/22/langchain_lite_lora_max",
              "published": "2025-12-22",
              "snippet": "LangChain Lite leverages LoRA Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-30_langchain_blog_7051",
              "url": "https://blog.langchain.dev/2025/12/30/langchain_lite_lora_max",
              "published": "2025-12-30",
              "snippet": "Under the hood, LangChain Lite implements LoRA Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_core_command_r_plus_pro_measures",
          "source": "benchmark:math_core",
          "target": "model:command_r_plus_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-25_openai_blog_8269",
              "url": "https://openai.com/blog/2026/01/25/math_core_command_r_plus_pro",
              "published": "2026-01-25",
              "snippet": "MATH Core provides standardized evaluation of Command R+ Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_next_retrieval_augmented_generation_core_uses_tech",
          "source": "model:claude_opus_45_next",
          "target": "tech:retrieval_augmented_generation_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:haystack_plus_multimodal_fusion_pro_uses_tech",
          "source": "tool:haystack_plus",
          "target": "tech:multimodal_fusion_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-21_microsoft_resea_2670",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/haystack_plus_multimodal_fusio",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Haystack Plus relies heavily on Multimodal Fusion Pro..."
            },
            {
              "docId": "2026-01-21_nextgov_3073",
              "url": "https://nextgov.com/2026/01/21/haystack_plus_multimodal_fusio",
              "published": "2026-01-21",
              "snippet": "Under the hood, Haystack Plus implements Multimodal Fusion Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_mini_group_query_attention_ultra_uses_tech",
          "source": "tool:semantic_kernel_mini",
          "target": "tech:group_query_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_midjourney_v7_v2_evaluated_on",
          "source": "paper:direct_preference_optimization_next",
          "target": "model:midjourney_v7_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-20_microsoft_resea_5029",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/direct_preference_optimization",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Direct Preference Optimization Next reaching 99% on Midjourney V7 v2..."
            },
            {
              "docId": "2026-01-20_microsoft_resea_3270",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/direct_preference_optimization",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Direct Preference Optimization Next reaching 76% on Midjourney V7 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_claude_opus_45_max_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:claude_opus_45_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_jamba_2_mini_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:jamba_2_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-12_nextgov_4188",
              "url": "https://nextgov.com/2025/12/12/retrieval_augmented_generation",
              "published": "2025-12-12",
              "snippet": "On the Jamba 2 Mini benchmark, Retrieval-Augmented Generation for Knowledge-Intensive NLP scored 87%..."
            },
            {
              "docId": "2025-12-27_techcrunch_1611",
              "url": "https://techcrunch.com/2025/12/27/retrieval_augmented_generation",
              "published": "2025-12-27",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP achieves 77% on Jamba 2 Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_tool_use_plus_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:tool_use_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:gpt4all_claude_opus_45_core_integrates_with",
          "source": "repo:gpt4all",
          "target": "model:claude_opus_45_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_4265",
              "url": "https://blog.google/technology/ai/2026/01/22/gpt4all_claude_opus_45_core",
              "published": "2026-01-22",
              "snippet": "gpt4all now supports Claude Opus 4.5 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_nemotron_5_next_integrates_with",
          "source": "repo:localai_plus",
          "target": "model:nemotron_5_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-05_arxiv_2201",
              "url": "https://arxiv.org/abs/2026/01/05/localai_plus_nemotron_5_next",
              "published": "2026-01-05",
              "snippet": "LocalAI Plus now supports Nemotron-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-10_microsoft_resea_3236",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/10/localai_plus_nemotron_5_next",
              "published": "2026-01-10",
              "snippet": "LocalAI Plus now supports Nemotron-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-25_wired_6000",
              "url": "https://wired.com/2026/01/25/localai_plus_nemotron_5_next",
              "published": "2026-01-25",
              "snippet": "LocalAI Plus now supports Nemotron-5 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_next_refinedweb_next_trained_on",
          "source": "model:gpt_4o_mini_2_next",
          "target": "dataset:refinedweb_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-01_techcrunch_8964",
              "url": "https://techcrunch.com/2026/01/01/gpt_4o_mini_2_next_refinedweb_",
              "published": "2026-01-01",
              "snippet": "The training corpus for GPT-4o Mini 2 Next includes RefinedWeb Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_edge_deepseek_v3_pro_measures",
          "source": "benchmark:humaneval_edge",
          "target": "model:deepseek_v3_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-31_nvidia_blog_2276",
              "url": "https://blogs.nvidia.com/2025/12/31/humaneval_edge_deepseek_v3_pro",
              "published": "2025-12-31",
              "snippet": "The HumanEval Edge benchmark measures DeepSeek-V3 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_refinedweb_lite_trained_on",
          "source": "model:mixtral_8x22b_plus",
          "target": "dataset:refinedweb_lite",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_qwen_3_evaluated_on",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "model:qwen_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_wired_5586",
              "url": "https://wired.com/2026/01/25/llm_agents:_a_survey_pro_qwen_",
              "published": "2026-01-25",
              "snippet": "Evaluation results show LLM Agents: A Survey Pro reaching 74% on Qwen-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_yi_large_max_integrates_with",
          "source": "tool:mlflow_mini",
          "target": "model:yi_large_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_dall_e_4_plus_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:dall_e_4_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-21_venturebeat_3704",
              "url": "https://venturebeat.com/2025/12/21/retrieval_augmented_generation",
              "published": "2025-12-21",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP achieves 95% on DALL-E 4 Plus, setting a new record..."
            },
            {
              "docId": "2025-12-26_the_verge_4876",
              "url": "https://theverge.com/2025/12/26/retrieval_augmented_generation",
              "published": "2025-12-26",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP achieves 76% on DALL-E 4 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_edge_speculative_decoding_edge_uses_tech",
          "source": "tool:crewai_edge",
          "target": "tech:speculative_decoding_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:dify_group_query_attention_core_uses_tech",
          "source": "tool:dify",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-20_reuters_6893",
              "url": "https://reuters.com/technology/2026/01/20/dify_group_query_attention_cor",
              "published": "2026-01-20",
              "snippet": "Technical details reveal Dify relies heavily on Group Query Attention Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_ultra_slimpajama_core_trained_on",
          "source": "model:gpt_5_ultra",
          "target": "dataset:slimpajama_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:mlflow_plus_gguf_core_uses_tech",
          "source": "tool:mlflow_plus",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_max_qwen_3_plus_measures",
          "source": "benchmark:truthfulqa_max",
          "target": "model:qwen_3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-15_reuters_9889",
              "url": "https://reuters.com/technology/2025/12/15/truthfulqa_max_qwen_3_plus",
              "published": "2025-12-15",
              "snippet": "The TruthfulQA Max benchmark measures Qwen-3 Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_tree_of_thought_plus_uses_tech",
          "source": "model:claude_opus_45_lite",
          "target": "tech:tree_of_thought_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_midjourney_v7_lite_integrates_with",
          "source": "repo:transformers_edge",
          "target": "model:midjourney_v7_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-11-26_ars_technica_7077",
              "url": "https://arstechnica.com/2025/11/26/transformers_edge_midjourney_v",
              "published": "2025-11-26",
              "snippet": "The latest release of transformers Edge adds native Midjourney V7 Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_v2_rlhf_core_uses_tech",
          "source": "repo:gpt4all_v2",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-04_reuters_6198",
              "url": "https://reuters.com/technology/2026/01/04/gpt4all_v2_rlhf_core",
              "published": "2026-01-04",
              "snippet": "Under the hood, gpt4all v2 implements RLHF Core for improved efficiency..."
            },
            {
              "docId": "2026-01-13_techcrunch_4997",
              "url": "https://techcrunch.com/2026/01/13/gpt4all_v2_rlhf_core",
              "published": "2026-01-13",
              "snippet": "Under the hood, gpt4all v2 implements RLHF Core for improved efficiency..."
            },
            {
              "docId": "2026-01-17_mit_technology__1127",
              "url": "https://technologyreview.com/2026/01/17/gpt4all_v2_rlhf_core",
              "published": "2026-01-17",
              "snippet": "gpt4all v2 leverages RLHF Core to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_ultra_gemini_ultra_2_depends_on",
          "source": "model:jamba_2_ultra",
          "target": "model:gemini_ultra_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_gpt_5_pro_depends_on",
          "source": "model:gpt_4o_mini_2",
          "target": "model:gpt_5_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:gradio_edge_stable_diffusion_4_integrates_with",
          "source": "tool:gradio_edge",
          "target": "model:stable_diffusion_4",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:flowise_max_nemotron_5_plus_integrates_with",
          "source": "tool:flowise_max",
          "target": "model:nemotron_5_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-15_meta_ai_blog_4394",
              "url": "https://ai.meta.com/blog/2026/01/15/flowise_max_nemotron_5_plus",
              "published": "2026-01-15",
              "snippet": "Flowise Max now supports Nemotron-5 Plus with full feature parity..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_4385",
              "url": "https://huggingface.co/blog/2026/01/24/flowise_max_nemotron_5_plus",
              "published": "2026-01-24",
              "snippet": "Flowise Max now supports Nemotron-5 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_tokenizer_bpe_lite_uses_tech",
          "source": "repo:langchain_max",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_qlora_v2_uses_tech",
          "source": "repo:ollama_mini",
          "target": "tech:qlora_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-15_hugging_face_bl_1380",
              "url": "https://huggingface.co/blog/2026/01/15/ollama_mini_qlora_v2",
              "published": "2026-01-15",
              "snippet": "Under the hood, ollama Mini implements QLoRA v2 for improved efficiency..."
            },
            {
              "docId": "2026-01-15_anthropic_blog_2924",
              "url": "https://anthropic.com/news/2026/01/15/ollama_mini_qlora_v2",
              "published": "2026-01-15",
              "snippet": "Under the hood, ollama Mini implements QLoRA v2 for improved efficiency..."
            },
            {
              "docId": "2026-01-15_the_verge_9745",
              "url": "https://theverge.com/2026/01/15/ollama_mini_qlora_v2",
              "published": "2026-01-15",
              "snippet": "ollama Mini leverages QLoRA v2 to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_dolma_lite_trained_on",
          "source": "model:claude_opus_45_core",
          "target": "dataset:dolma_lite",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-20_venturebeat_5616",
              "url": "https://venturebeat.com/2025/12/20/claude_opus_45_core_dolma_lite",
              "published": "2025-12-20",
              "snippet": "Claude Opus 4.5 Core utilized Dolma Lite as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_midjourney_v7_v2_integrates_with",
          "source": "tool:tensorrt_llm_v2",
          "target": "model:midjourney_v7_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-19_mit_technology__9554",
              "url": "https://technologyreview.com/2026/01/19/tensorrt_llm_v2_midjourney_v7_",
              "published": "2026-01-19",
              "snippet": "TensorRT-LLM v2 announced official support for Midjourney V7 v2..."
            },
            {
              "docId": "2026-01-23_nextgov_4074",
              "url": "https://nextgov.com/2026/01/23/tensorrt_llm_v2_midjourney_v7_",
              "published": "2026-01-23",
              "snippet": "TensorRT-LLM v2 announced official support for Midjourney V7 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_qwen_3_lite_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:qwen_3_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-03_nvidia_blog_8987",
              "url": "https://blogs.nvidia.com/2026/01/03/flash_attention:_fast_and_memo",
              "published": "2026-01-03",
              "snippet": "Evaluation results show Flash Attention: Fast and Memory-Efficient Attention reaching 75% on Qwen-3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_sliding_window_attention_max_uses_tech",
          "source": "tool:langchain_plus",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-24_ars_technica_7202",
              "url": "https://arstechnica.com/2026/01/24/langchain_plus_sliding_window_",
              "published": "2026-01-24",
              "snippet": "Technical details reveal LangChain Plus relies heavily on Sliding Window Attention Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_pro_dall_e_4_mini_integrates_with",
          "source": "tool:localai_pro",
          "target": "model:dall_e_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_4374",
              "url": "https://nextgov.com/2026/01/24/localai_pro_dall_e_4_mini",
              "published": "2026-01-24",
              "snippet": "LocalAI Pro now supports DALL-E 4 Mini with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_gguf_edge_uses_tech",
          "source": "model:claude_sonnet_4",
          "target": "tech:gguf_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_edge_roots_v2_trained_on",
          "source": "model:whisper_v4_edge",
          "target": "dataset:roots_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-10_langchain_blog_5283",
              "url": "https://blog.langchain.dev/2026/01/10/whisper_v4_edge_roots_v2",
              "published": "2026-01-10",
              "snippet": "Whisper v4 Edge utilized ROOTS v2 as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_core_gpqa_max_evaluated_on",
          "source": "model:codex_2_core",
          "target": "benchmark:gpqa_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-19_google_ai_blog_7674",
              "url": "https://blog.google/technology/ai/2026/01/19/codex_2_core_gpqa_max",
              "published": "2026-01-19",
              "snippet": "Evaluation results show Codex 2 Core reaching 99% on GPQA Max..."
            },
            {
              "docId": "2026-01-20_hugging_face_bl_3849",
              "url": "https://huggingface.co/blog/2026/01/20/codex_2_core_gpqa_max",
              "published": "2026-01-20",
              "snippet": "Codex 2 Core achieves 93% on GPQA Max, setting a new record..."
            },
            {
              "docId": "2026-01-20_ars_technica_1704",
              "url": "https://arstechnica.com/2026/01/20/codex_2_core_gpqa_max",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Codex 2 Core reaching 92% on GPQA Max..."
            },
            {
              "docId": "2026-01-25_wired_2109",
              "url": "https://wired.com/2026/01/25/codex_2_core_gpqa_max",
              "published": "2026-01-25",
              "snippet": "Codex 2 Core achieves 99% on GPQA Max, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_mini_retrieval_augmented_generation_lite_uses_tech",
          "source": "dataset:slimpajama_mini",
          "target": "tech:retrieval_augmented_generation_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:copilot_plus_transformer_architecture_lite_uses_tech",
          "source": "tool:copilot_plus",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-21_google_ai_blog_6756",
              "url": "https://blog.google/technology/ai/2026/01/21/copilot_plus_transformer_archi",
              "published": "2026-01-21",
              "snippet": "Copilot Plus leverages Transformer Architecture Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_the_gradient_1757",
              "url": "https://thegradient.pub/2026/01/25/copilot_plus_transformer_archi",
              "published": "2026-01-25",
              "snippet": "Under the hood, Copilot Plus implements Transformer Architecture Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_jamba_2_edge_depends_on",
          "source": "model:grok_3",
          "target": "model:jamba_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_group_query_attention_lite_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_mini_gpqa_plus_evaluated_on",
          "source": "model:whisper_v4_mini",
          "target": "benchmark:gpqa_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-09_ars_technica_4501",
              "url": "https://arstechnica.com/2026/01/09/whisper_v4_mini_gpqa_plus",
              "published": "2026-01-09",
              "snippet": "On the GPQA Plus benchmark, Whisper v4 Mini scored 93%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_group_query_attention_ultra_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:group_query_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:litellm_pro_tokenizer_bpe_ultra_uses_tech",
          "source": "tool:litellm_pro",
          "target": "tech:tokenizer_bpe_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2025-12-21_weights_and_bia_6241",
              "url": "https://wandb.ai/articles/2025/12/21/litellm_pro_tokenizer_bpe_ultr",
              "published": "2025-12-21",
              "snippet": "Under the hood, LiteLLM Pro implements Tokenizer BPE Ultra for improved efficiency..."
            },
            {
              "docId": "2025-12-28_the_gradient_4867",
              "url": "https://thegradient.pub/2025/12/28/litellm_pro_tokenizer_bpe_ultr",
              "published": "2025-12-28",
              "snippet": "LiteLLM Pro leverages Tokenizer BPE Ultra to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-01_anthropic_blog_3631",
              "url": "https://anthropic.com/news/2026/01/01/litellm_pro_tokenizer_bpe_ultr",
              "published": "2026-01-01",
              "snippet": "Technical details reveal LiteLLM Pro relies heavily on Tokenizer BPE Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_gpt_5_next_integrates_with",
          "source": "tool:semantic_kernel_v2",
          "target": "model:gpt_5_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-05_wired_9625",
              "url": "https://wired.com/2026/01/05/semantic_kernel_v2_gpt_5_next",
              "published": "2026-01-05",
              "snippet": "Semantic Kernel v2 now supports GPT-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-14_langchain_blog_7906",
              "url": "https://blog.langchain.dev/2026/01/14/semantic_kernel_v2_gpt_5_next",
              "published": "2026-01-14",
              "snippet": "Semantic Kernel v2 now supports GPT-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-20_the_gradient_7556",
              "url": "https://thegradient.pub/2026/01/20/semantic_kernel_v2_gpt_5_next",
              "published": "2026-01-20",
              "snippet": "Semantic Kernel v2 now supports GPT-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-22_ars_technica_4234",
              "url": "https://arstechnica.com/2026/01/22/semantic_kernel_v2_gpt_5_next",
              "published": "2026-01-22",
              "snippet": "Semantic Kernel v2 announced official support for GPT-5 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_lite_midjourney_v7_mini_measures",
          "source": "benchmark:gpqa_lite",
          "target": "model:midjourney_v7_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-21_nvidia_blog_7490",
              "url": "https://blogs.nvidia.com/2026/01/21/gpqa_lite_midjourney_v7_mini",
              "published": "2026-01-21",
              "snippet": "GPQA Lite provides standardized evaluation of Midjourney V7 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_mini_codex_2_lite_depends_on",
          "source": "model:jamba_2_mini",
          "target": "model:codex_2_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:gsm8k_edge_gemma_3_edge_measures",
          "source": "benchmark:gsm8k_edge",
          "target": "model:gemma_3_edge",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-24_the_gradient_8160",
              "url": "https://thegradient.pub/2026/01/24/gsm8k_edge_gemma_3_edge",
              "published": "2026-01-24",
              "snippet": "The GSM8K Edge benchmark measures Gemma 3 Edge across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_next_mixtral_8x22b_pro_integrates_with",
          "source": "tool:crewai_next",
          "target": "model:mixtral_8x22b_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:yi_large_pro_laion_5b_core_trained_on",
          "source": "model:yi_large_pro",
          "target": "dataset:laion_5b_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:winogrande_pro_qwen_3_plus_measures",
          "source": "benchmark:winogrande_pro",
          "target": "model:qwen_3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-09_mit_technology__6779",
              "url": "https://technologyreview.com/2025/12/09/winogrande_pro_qwen_3_plus",
              "published": "2025-12-09",
              "snippet": "WinoGrande Pro has become the standard for evaluating Qwen-3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_speculative_decoding_uses_tech",
          "source": "model:gemma_3_plus",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-23_arxiv_5219",
              "url": "https://arxiv.org/abs/2026/01/23/gemma_3_plus_speculative_decod",
              "published": "2026-01-23",
              "snippet": "Under the hood, Gemma 3 Plus implements Speculative Decoding for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_lmsys_chatbot_arena_plus_evaluated_on",
          "source": "model:llama_4_pro",
          "target": "benchmark:lmsys_chatbot_arena_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-14_nextgov_2374",
              "url": "https://nextgov.com/2026/01/14/llama_4_pro_lmsys_chatbot_aren",
              "published": "2026-01-14",
              "snippet": "Evaluation results show Llama 4 Pro reaching 99% on LMSYS Chatbot Arena Plus..."
            },
            {
              "docId": "2026-01-15_weights_and_bia_3877",
              "url": "https://wandb.ai/articles/2026/01/15/llama_4_pro_lmsys_chatbot_aren",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Llama 4 Pro reaching 93% on LMSYS Chatbot Arena Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_claude_opus_45_core_evaluated_on",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "model:claude_opus_45_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:haystack_lite_rlhf_ultra_uses_tech",
          "source": "tool:haystack_lite",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_3333",
              "url": "https://theverge.com/2026/01/25/haystack_lite_rlhf_ultra",
              "published": "2026-01-25",
              "snippet": "Haystack Lite leverages RLHF Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_mini_synthetic_data_generation_mini_uses_tech",
          "source": "model:nemotron_5_mini",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-22_nextgov_7827",
              "url": "https://nextgov.com/2026/01/22/nemotron_5_mini_synthetic_data",
              "published": "2026-01-22",
              "snippet": "Nemotron-5 Mini leverages Synthetic Data Generation Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-22_ars_technica_8740",
              "url": "https://arstechnica.com/2026/01/22/nemotron_5_mini_synthetic_data",
              "published": "2026-01-22",
              "snippet": "Under the hood, Nemotron-5 Mini implements Synthetic Data Generation Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-24_reuters_2970",
              "url": "https://reuters.com/technology/2026/01/24/nemotron_5_mini_synthetic_data",
              "published": "2026-01-24",
              "snippet": "Under the hood, Nemotron-5 Mini implements Synthetic Data Generation Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_v2_stable_diffusion_4_edge_depends_on",
          "source": "model:aya_3_v2",
          "target": "model:stable_diffusion_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_sora_2_plus_integrates_with",
          "source": "repo:transformers_edge",
          "target": "model:sora_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:hellaswag_max_claude_sonnet_4_max_measures",
          "source": "benchmark:hellaswag_max",
          "target": "model:claude_sonnet_4_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-22_openai_blog_1651",
              "url": "https://openai.com/blog/2026/01/22/hellaswag_max_claude_sonnet_4_",
              "published": "2026-01-22",
              "snippet": "HellaSwag Max has become the standard for evaluating Claude Sonnet 4 Max..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_6031",
              "url": "https://blogs.nvidia.com/2026/01/22/hellaswag_max_claude_sonnet_4_",
              "published": "2026-01-22",
              "snippet": "HellaSwag Max has become the standard for evaluating Claude Sonnet 4 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_speculative_decoding_ultra_uses_tech",
          "source": "repo:langchain_max",
          "target": "tech:speculative_decoding_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-13_mit_technology__5382",
              "url": "https://technologyreview.com/2026/01/13/langchain_max_speculative_deco",
              "published": "2026-01-13",
              "snippet": "Technical details reveal langchain Max relies heavily on Speculative Decoding Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixture_of_experts_meets_instruction_tun_gpt_4o_mini_2_lite_evaluated_on",
          "source": "paper:mixture_of_experts_meets_instruction_tun",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_multimodal_fusion_edge_uses_tech",
          "source": "repo:langchain_plus",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_3275",
              "url": "https://blog.google/technology/ai/2026/01/22/langchain_plus_multimodal_fusi",
              "published": "2026-01-22",
              "snippet": "langchain Plus leverages Multimodal Fusion Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_constitutional_ai_next_uses_tech",
          "source": "tool:tensorrt_llm_v2",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:localai_lite_gguf_mini_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:llama_4_edge_gsm8k_mini_evaluated_on",
          "source": "model:llama_4_edge",
          "target": "benchmark:gsm8k_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-25_langchain_blog_3233",
              "url": "https://blog.langchain.dev/2026/01/25/llama_4_edge_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Llama 4 Edge reaching 86% on GSM8K Mini..."
            },
            {
              "docId": "2026-01-25_mit_technology__8216",
              "url": "https://technologyreview.com/2026/01/25/llama_4_edge_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Llama 4 Edge reaching 74% on GSM8K Mini..."
            },
            {
              "docId": "2026-01-25_mit_technology__7337",
              "url": "https://technologyreview.com/2026/01/25/llama_4_edge_gsm8k_mini",
              "published": "2026-01-25",
              "snippet": "Llama 4 Edge achieves 75% on GSM8K Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_next_whisper_v4_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_next",
          "target": "model:whisper_v4",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-22_nvidia_blog_5598",
              "url": "https://blogs.nvidia.com/2026/01/22/attention_is_all_you_need_v2_n",
              "published": "2026-01-22",
              "snippet": "Attention Is All You Need v2 Next achieves 78% on Whisper v4, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:yi_large_next_retrieval_augmented_generation_next_uses_tech",
          "source": "model:yi_large_next",
          "target": "tech:retrieval_augmented_generation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:localai_tree_of_thought_v2_uses_tech",
          "source": "repo:localai",
          "target": "tech:tree_of_thought_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_qwen_3_next_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:qwen_3_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_core_the_stack_v2_trained_on",
          "source": "model:gemini_ultra_2_core",
          "target": "dataset:the_stack_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:math_next_command_r_plus_plus_measures",
          "source": "benchmark:math_next",
          "target": "model:command_r_plus_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-21_nextgov_6410",
              "url": "https://nextgov.com/2026/01/21/math_next_command_r_plus_plus",
              "published": "2026-01-21",
              "snippet": "MATH Next provides standardized evaluation of Command R+ Plus..."
            },
            {
              "docId": "2026-01-22_weights_and_bia_5159",
              "url": "https://wandb.ai/articles/2026/01/22/math_next_command_r_plus_plus",
              "published": "2026-01-22",
              "snippet": "MATH Next has become the standard for evaluating Command R+ Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:humaneval_mini_whisper_v4_pro_measures",
          "source": "benchmark:humaneval_mini",
          "target": "model:whisper_v4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-13_openai_blog_7340",
              "url": "https://openai.com/blog/2026/01/13/humaneval_mini_whisper_v4_pro",
              "published": "2026-01-13",
              "snippet": "HumanEval Mini provides standardized evaluation of Whisper v4 Pro..."
            },
            {
              "docId": "2026-01-19_hugging_face_bl_9919",
              "url": "https://huggingface.co/blog/2026/01/19/humaneval_mini_whisper_v4_pro",
              "published": "2026-01-19",
              "snippet": "HumanEval Mini provides standardized evaluation of Whisper v4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_max_synthetic_data_generation_mini_uses_tech",
          "source": "model:qwen_3_max",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_synthetic_data_generation_mini_uses_tech",
          "source": "model:claude_sonnet_4_next",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:grok_3_core_mbpp_core_evaluated_on",
          "source": "model:grok_3_core",
          "target": "benchmark:mbpp_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-06_nvidia_blog_3850",
              "url": "https://blogs.nvidia.com/2026/01/06/grok_3_core_mbpp_core",
              "published": "2026-01-06",
              "snippet": "Evaluation results show Grok-3 Core reaching 75% on MBPP Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_nemotron_5_plus_depends_on",
          "source": "model:midjourney_v7_core",
          "target": "model:nemotron_5_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_the_pile_plus_trained_on",
          "source": "model:sora_2_plus",
          "target": "dataset:the_pile_plus",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:arc_agi_nemotron_5_lite_measures",
          "source": "benchmark:arc_agi",
          "target": "model:nemotron_5_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-11_nvidia_blog_7954",
              "url": "https://blogs.nvidia.com/2026/01/11/arc_agi_nemotron_5_lite",
              "published": "2026-01-11",
              "snippet": "The ARC-AGI benchmark measures Nemotron-5 Lite across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_v2_claude_opus_45_next_integrates_with",
          "source": "tool:gradio_v2",
          "target": "model:claude_opus_45_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-05_microsoft_resea_5934",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/05/gradio_v2_claude_opus_45_next",
              "published": "2026-01-05",
              "snippet": "Gradio v2 announced official support for Claude Opus 4.5 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_flash_attention_next_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:flash_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_palm_3_next_integrates_with",
          "source": "tool:langchain_plus",
          "target": "model:palm_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-09_hugging_face_bl_5149",
              "url": "https://huggingface.co/blog/2026/01/09/langchain_plus_palm_3_next",
              "published": "2026-01-09",
              "snippet": "LangChain Plus announced official support for PaLM 3 Next..."
            },
            {
              "docId": "2026-01-20_wired_2043",
              "url": "https://wired.com/2026/01/20/langchain_plus_palm_3_next",
              "published": "2026-01-20",
              "snippet": "LangChain Plus announced official support for PaLM 3 Next..."
            },
            {
              "docId": "2026-01-23_hugging_face_bl_4334",
              "url": "https://huggingface.co/blog/2026/01/23/langchain_plus_palm_3_next",
              "published": "2026-01-23",
              "snippet": "LangChain Plus now supports PaLM 3 Next with full feature parity..."
            },
            {
              "docId": "2026-01-24_microsoft_resea_4460",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/langchain_plus_palm_3_next",
              "published": "2026-01-24",
              "snippet": "The latest release of LangChain Plus adds native PaLM 3 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_edge_humaneval_edge_evaluated_on",
          "source": "model:nemotron_5_edge",
          "target": "benchmark:humaneval_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-23_bloomberg_8740",
              "url": "https://bloomberg.com/technology/2026/01/23/nemotron_5_edge_humaneval_edge",
              "published": "2026-01-23",
              "snippet": "On the HumanEval Edge benchmark, Nemotron-5 Edge scored 91%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_plus_palm_3_max_integrates_with",
          "source": "tool:llamaindex_plus",
          "target": "model:palm_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-02_wired_9769",
              "url": "https://wired.com/2026/01/02/llamaindex_plus_palm_3_max",
              "published": "2026-01-02",
              "snippet": "LlamaIndex Plus now supports PaLM 3 Max with full feature parity..."
            },
            {
              "docId": "2026-01-16_nextgov_5897",
              "url": "https://nextgov.com/2026/01/16/llamaindex_plus_palm_3_max",
              "published": "2026-01-16",
              "snippet": "LlamaIndex Plus now supports PaLM 3 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_rlhf_max_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-23_microsoft_resea_1478",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/23/mlflow_max_rlhf_max",
              "published": "2026-01-23",
              "snippet": "MLflow Max leverages RLHF Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_plus_distillation_next_uses_tech",
          "source": "dataset:the_stack_v2_plus",
          "target": "tech:distillation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:arc_agi_core_llama_4_next_measures",
          "source": "benchmark:arc_agi_core",
          "target": "model:llama_4_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mini_tree_of_thought_edge_uses_tech",
          "source": "repo:llamacpp_mini",
          "target": "tech:tree_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_2109",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/llamacpp_mini_tree_of_thought_",
              "published": "2026-01-25",
              "snippet": "Technical details reveal llama.cpp Mini relies heavily on Tree of Thought Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_codex_2_v2_depends_on",
          "source": "model:stable_diffusion_4_next",
          "target": "model:codex_2_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_flowise_pro_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "tool:flowise_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:retrieval_augmented_generation_for_knowl_falcon_3_evaluated_on",
          "source": "paper:retrieval_augmented_generation_for_knowl",
          "target": "model:falcon_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-12_the_gradient_4607",
              "url": "https://thegradient.pub/2026/01/12/retrieval_augmented_generation",
              "published": "2026-01-12",
              "snippet": "On the Falcon 3 benchmark, Retrieval-Augmented Generation for Knowledge-Intensive NLP scored 86%..."
            },
            {
              "docId": "2026-01-17_hugging_face_bl_4674",
              "url": "https://huggingface.co/blog/2026/01/17/retrieval_augmented_generation",
              "published": "2026-01-17",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP achieves 96% on Falcon 3, setting a new record..."
            },
            {
              "docId": "2026-01-19_langchain_blog_6734",
              "url": "https://blog.langchain.dev/2026/01/19/retrieval_augmented_generation",
              "published": "2026-01-19",
              "snippet": "Retrieval-Augmented Generation for Knowledge-Intensive NLP achieves 91% on Falcon 3, setting a new record..."
            },
            {
              "docId": "2026-01-24_mit_technology__7519",
              "url": "https://technologyreview.com/2026/01/24/retrieval_augmented_generation",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Retrieval-Augmented Generation for Knowledge-Intensive NLP reaching 96% on Falcon 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:fineweb_lite_flash_attention_max_uses_tech",
          "source": "dataset:fineweb_lite",
          "target": "tech:flash_attention_max",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_grok_3_core_evaluated_on",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "model:grok_3_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-04_arxiv_3368",
              "url": "https://arxiv.org/abs/2026/01/04/llm_agents:_a_survey_core_grok",
              "published": "2026-01-04",
              "snippet": "LLM Agents: A Survey Core achieves 81% on Grok-3 Core, setting a new record..."
            },
            {
              "docId": "2026-01-25_bloomberg_3578",
              "url": "https://bloomberg.com/technology/2026/01/25/llm_agents:_a_survey_core_grok",
              "published": "2026-01-25",
              "snippet": "On the Grok-3 Core benchmark, LLM Agents: A Survey Core scored 86%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_claude_opus_45_v2_integrates_with",
          "source": "repo:llamacpp_v2",
          "target": "model:claude_opus_45_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-16_nvidia_blog_1327",
              "url": "https://blogs.nvidia.com/2026/01/16/llamacpp_v2_claude_opus_45_v2",
              "published": "2026-01-16",
              "snippet": "The latest release of llama.cpp v2 adds native Claude Opus 4.5 v2 integration..."
            },
            {
              "docId": "2026-01-16_reuters_6802",
              "url": "https://reuters.com/technology/2026/01/16/llamacpp_v2_claude_opus_45_v2",
              "published": "2026-01-16",
              "snippet": "The latest release of llama.cpp v2 adds native Claude Opus 4.5 v2 integration..."
            },
            {
              "docId": "2026-01-18_wired_6633",
              "url": "https://wired.com/2026/01/18/llamacpp_v2_claude_opus_45_v2",
              "published": "2026-01-18",
              "snippet": "llama.cpp v2 announced official support for Claude Opus 4.5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_constitutional_ai_max_uses_tech",
          "source": "tool:gradio",
          "target": "tech:constitutional_ai_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-09_techcrunch_7967",
              "url": "https://techcrunch.com/2026/01/09/gradio_constitutional_ai_max",
              "published": "2026-01-09",
              "snippet": "Under the hood, Gradio implements Constitutional AI Max for improved efficiency..."
            },
            {
              "docId": "2026-01-14_reuters_9818",
              "url": "https://reuters.com/technology/2026/01/14/gradio_constitutional_ai_max",
              "published": "2026-01-14",
              "snippet": "Under the hood, Gradio implements Constitutional AI Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_the_pile_max_trained_on",
          "source": "model:phi_4_lite",
          "target": "dataset:the_pile_max",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_whisper_v4_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "model:whisper_v4",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:aya_3_max_laion_5b_v2_trained_on",
          "source": "model:aya_3_max",
          "target": "dataset:laion_5b_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.49,
          "evidence": [
            {
              "docId": "2025-12-31_the_verge_5928",
              "url": "https://theverge.com/2025/12/31/aya_3_max_laion_5b_v2",
              "published": "2025-12-31",
              "snippet": "Aya 3 Max utilized LAION-5B v2 as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_edge_mmlu_plus_evaluated_on",
          "source": "model:gemini_ultra_2_edge",
          "target": "benchmark:mmlu_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-24_nextgov_1260",
              "url": "https://nextgov.com/2026/01/24/gemini_ultra_2_edge_mmlu_plus",
              "published": "2026-01-24",
              "snippet": "On the MMLU Plus benchmark, Gemini Ultra 2 Edge scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_gpt_4o_mini_2_pro_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:gpt_4o_mini_2_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-24_reuters_1706",
              "url": "https://reuters.com/technology/2026/01/24/chain_of_thought_prompting_eli",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Chain-of-Thought Prompting Elicits Reasoning reaching 77% on GPT-4o Mini 2 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_edge_deepseek_v3_mini_measures",
          "source": "benchmark:mt_bench_edge",
          "target": "model:deepseek_v3_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-01_wired_7591",
              "url": "https://wired.com/2026/01/01/mt_bench_edge_deepseek_v3_mini",
              "published": "2026-01-01",
              "snippet": "MT-Bench Edge has become the standard for evaluating DeepSeek-V3 Mini..."
            },
            {
              "docId": "2026-01-04_ars_technica_8623",
              "url": "https://arstechnica.com/2026/01/04/mt_bench_edge_deepseek_v3_mini",
              "published": "2026-01-04",
              "snippet": "MT-Bench Edge provides standardized evaluation of DeepSeek-V3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_lmsys_chatbot_arena_max_evaluated_on",
          "source": "model:llama_4_pro",
          "target": "benchmark:lmsys_chatbot_arena_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-12-31_microsoft_resea_5138",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/31/llama_4_pro_lmsys_chatbot_aren",
              "published": "2025-12-31",
              "snippet": "On the LMSYS Chatbot Arena Max benchmark, Llama 4 Pro scored 73%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_v2_mmlu_evaluated_on",
          "source": "model:claude_sonnet_4_v2",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-18_openai_blog_8768",
              "url": "https://openai.com/blog/2026/01/18/claude_sonnet_4_v2_mmlu",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Claude Sonnet 4 v2 reaching 91% on MMLU..."
            },
            {
              "docId": "2026-01-24_wired_2860",
              "url": "https://wired.com/2026/01/24/claude_sonnet_4_v2_mmlu",
              "published": "2026-01-24",
              "snippet": "Claude Sonnet 4 v2 achieves 82% on MMLU, setting a new record..."
            },
            {
              "docId": "2026-01-24_venturebeat_8108",
              "url": "https://venturebeat.com/2026/01/24/claude_sonnet_4_v2_mmlu",
              "published": "2026-01-24",
              "snippet": "Evaluation results show Claude Sonnet 4 v2 reaching 93% on MMLU..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_max_group_query_attention_lite_uses_tech",
          "source": "dataset:redpajama_v2_max",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_gpt_4o_mini_2_lite_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__4276",
              "url": "https://technologyreview.com/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning achieves 81% on GPT-4o Mini 2 Lite, setting a new record..."
            },
            {
              "docId": "2026-01-25_bloomberg_7035",
              "url": "https://bloomberg.com/technology/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "On the GPT-4o Mini 2 Lite benchmark, Chain-of-Thought Prompting Elicits Reasoning scored 90%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_pro_qwen_3_ultra_depends_on",
          "source": "model:gemini_ultra_2_pro",
          "target": "model:qwen_3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.41
        }
      },
      {
        "data": {
          "id": "e:flowise_edge_gemini_ultra_2_integrates_with",
          "source": "tool:flowise_edge",
          "target": "model:gemini_ultra_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_6451",
              "url": "https://techcrunch.com/2026/01/23/flowise_edge_gemini_ultra_2",
              "published": "2026-01-23",
              "snippet": "The latest release of Flowise Edge adds native Gemini Ultra 2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_aya_3_pro_integrates_with",
          "source": "repo:llamacpp_v2",
          "target": "model:aya_3_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:gpt4all_sparse_attention_mini_uses_tech",
          "source": "repo:gpt4all",
          "target": "tech:sparse_attention_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_next_aya_3_mini_depends_on",
          "source": "model:claude_sonnet_4_next",
          "target": "model:aya_3_mini",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_lite_sparse_attention_next_uses_tech",
          "source": "model:whisper_v4_lite",
          "target": "tech:sparse_attention_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:mlflow_plus_rotary_position_embedding_lite_uses_tech",
          "source": "tool:mlflow_plus",
          "target": "tech:rotary_position_embedding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:aya_3_pro_openwebtext2_core_trained_on",
          "source": "model:aya_3_pro",
          "target": "dataset:openwebtext2_core",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-22_google_ai_blog_1946",
              "url": "https://blog.google/technology/ai/2026/01/22/aya_3_pro_openwebtext2_core",
              "published": "2026-01-22",
              "snippet": "Aya 3 Pro utilized OpenWebText2 Core as part of its pre-training data mix..."
            },
            {
              "docId": "2026-01-22_hugging_face_bl_5030",
              "url": "https://huggingface.co/blog/2026/01/22/aya_3_pro_openwebtext2_core",
              "published": "2026-01-22",
              "snippet": "The training corpus for Aya 3 Pro includes OpenWebText2 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_plus_tensorrt_llm_plus_integrates_with",
          "source": "tool:cursor_plus",
          "target": "tool:tensorrt_llm_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-08_venturebeat_7837",
              "url": "https://venturebeat.com/2026/01/08/cursor_plus_tensorrt_llm_plus",
              "published": "2026-01-08",
              "snippet": "The latest release of Cursor Plus adds native TensorRT-LLM Plus integration..."
            },
            {
              "docId": "2026-01-22_hugging_face_bl_8792",
              "url": "https://huggingface.co/blog/2026/01/22/cursor_plus_tensorrt_llm_plus",
              "published": "2026-01-22",
              "snippet": "Cursor Plus now supports TensorRT-LLM Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_core_rlhf_next_uses_tech",
          "source": "model:grok_3_core",
          "target": "tech:rlhf_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_transformer_architecture_ultra_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:transformer_architecture_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_5689",
              "url": "https://anthropic.com/news/2026/01/25/textbooks_are_all_you_need_min",
              "published": "2026-01-25",
              "snippet": "Under the hood, Textbooks Are All You Need Mini implements Transformer Architecture Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-25_venturebeat_7827",
              "url": "https://venturebeat.com/2026/01/25/textbooks_are_all_you_need_min",
              "published": "2026-01-25",
              "snippet": "Textbooks Are All You Need Mini leverages Transformer Architecture Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_pro_flowise_max_integrates_with",
          "source": "tool:flowise_pro",
          "target": "tool:flowise_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:copilot_edge_llama_4_edge_integrates_with",
          "source": "tool:copilot_edge",
          "target": "model:llama_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:llamacpp_sora_2_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:sora_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_codex_2_edge_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:codex_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:fineweb_next_synthetic_data_generation_core_uses_tech",
          "source": "dataset:fineweb_next",
          "target": "tech:synthetic_data_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:mbpp_claude_opus_45_pro_measures",
          "source": "benchmark:mbpp",
          "target": "model:claude_opus_45_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-20_nvidia_blog_1943",
              "url": "https://blogs.nvidia.com/2026/01/20/mbpp_claude_opus_45_pro",
              "published": "2026-01-20",
              "snippet": "The MBPP benchmark measures Claude Opus 4.5 Pro across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_ultra_gemma_3_max_measures",
          "source": "benchmark:winogrande_ultra",
          "target": "model:gemma_3_max",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2025-12-24_weights_and_bia_1365",
              "url": "https://wandb.ai/articles/2025/12/24/winogrande_ultra_gemma_3_max",
              "published": "2025-12-24",
              "snippet": "WinoGrande Ultra provides standardized evaluation of Gemma 3 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_v2_synthetic_data_generation_uses_tech",
          "source": "tool:streamlit_v2",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_codex_2_plus_integrates_with",
          "source": "repo:autogpt_v2",
          "target": "model:codex_2_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-24_ars_technica_5733",
              "url": "https://arstechnica.com/2026/01/24/autogpt_v2_codex_2_plus",
              "published": "2026-01-24",
              "snippet": "AutoGPT v2 now supports Codex 2 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_jamba_2_mini_integrates_with",
          "source": "repo:vllm_pro",
          "target": "model:jamba_2_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-20_bloomberg_5254",
              "url": "https://bloomberg.com/technology/2026/01/20/vllm_pro_jamba_2_mini",
              "published": "2026-01-20",
              "snippet": "The latest release of vllm Pro adds native Jamba 2 Mini integration..."
            },
            {
              "docId": "2026-01-24_the_gradient_4751",
              "url": "https://thegradient.pub/2026/01/24/vllm_pro_jamba_2_mini",
              "published": "2026-01-24",
              "snippet": "The latest release of vllm Pro adds native Jamba 2 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_next_crewai_edge_integrates_with",
          "source": "tool:streamlit_next",
          "target": "tool:crewai_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:transformers_plus_gguf_core_uses_tech",
          "source": "repo:transformers_plus",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:ollama_lite_stable_diffusion_4_max_integrates_with",
          "source": "tool:ollama_lite",
          "target": "model:stable_diffusion_4_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-27_openai_blog_8370",
              "url": "https://openai.com/blog/2025/12/27/ollama_lite_stable_diffusion_4",
              "published": "2025-12-27",
              "snippet": "Ollama Lite announced official support for Stable Diffusion 4 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_ultra_gguf_pro_uses_tech",
          "source": "repo:llamacpp_ultra",
          "target": "tech:gguf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-04_weights_and_bia_4033",
              "url": "https://wandb.ai/articles/2026/01/04/llamacpp_ultra_gguf_pro",
              "published": "2026-01-04",
              "snippet": "Under the hood, llama.cpp Ultra implements GGUF Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_gguf_lite_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:gguf_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_synthetic_data_generation_lite_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:synthetic_data_generation_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-06_techcrunch_3729",
              "url": "https://techcrunch.com/2026/01/06/attention_is_all_you_need_v2_s",
              "published": "2026-01-06",
              "snippet": "Technical details reveal Attention Is All You Need v2 relies heavily on Synthetic Data Generation Lite..."
            },
            {
              "docId": "2026-01-17_microsoft_resea_9840",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/17/attention_is_all_you_need_v2_s",
              "published": "2026-01-17",
              "snippet": "Attention Is All You Need v2 leverages Synthetic Data Generation Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_edge_llama_4_next_integrates_with",
          "source": "repo:langchain_edge",
          "target": "model:llama_4_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-22_nvidia_blog_5697",
              "url": "https://blogs.nvidia.com/2026/01/22/langchain_edge_llama_4_next",
              "published": "2026-01-22",
              "snippet": "langchain Edge now supports Llama 4 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_next_multimodal_fusion_edge_uses_tech",
          "source": "tool:vllm_next",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:falcon_3_pro_laion_5b_trained_on",
          "source": "model:falcon_3_pro",
          "target": "dataset:laion_5b",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_wikipedia_dump_2025_core_trained_on",
          "source": "model:stable_diffusion_4_plus",
          "target": "dataset:wikipedia_dump_2025_core",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:llama_4_pro_the_stack_v2_plus_trained_on",
          "source": "model:llama_4_pro",
          "target": "dataset:the_stack_v2_plus",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_rlhf_core_uses_tech",
          "source": "repo:langchain_plus",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_next_stable_diffusion_4_edge_depends_on",
          "source": "model:deepseek_v3_next",
          "target": "model:stable_diffusion_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:llamacpp_v2_llama_4_pro_integrates_with",
          "source": "repo:llamacpp_v2",
          "target": "model:llama_4_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-11-14_venturebeat_5893",
              "url": "https://venturebeat.com/2025/11/14/llamacpp_v2_llama_4_pro",
              "published": "2025-11-14",
              "snippet": "llama.cpp v2 announced official support for Llama 4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_ultra_transformer_architecture_lite_uses_tech",
          "source": "tool:semantic_kernel_ultra",
          "target": "tech:transformer_architecture_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-21_langchain_blog_6374",
              "url": "https://blog.langchain.dev/2026/01/21/semantic_kernel_ultra_transfor",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Semantic Kernel Ultra relies heavily on Transformer Architecture Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_v2_tokenizer_bpe_lite_uses_tech",
          "source": "tool:tensorrt_llm_v2",
          "target": "tech:tokenizer_bpe_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-17_nvidia_blog_8416",
              "url": "https://blogs.nvidia.com/2025/12/17/tensorrt_llm_v2_tokenizer_bpe_",
              "published": "2025-12-17",
              "snippet": "Under the hood, TensorRT-LLM v2 implements Tokenizer BPE Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_group_query_attention_ultra_uses_tech",
          "source": "repo:text_generation_webui",
          "target": "tech:group_query_attention_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_plus_llama_4_v2_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_plus",
          "target": "model:llama_4_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:dolma_next_mixture_of_experts_lite_uses_tech",
          "source": "dataset:dolma_next",
          "target": "tech:mixture_of_experts_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:wikipedia_dump_2025_pro_multimodal_fusion_v2_uses_tech",
          "source": "dataset:wikipedia_dump_2025_pro",
          "target": "tech:multimodal_fusion_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.4
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_next_rotary_position_embedding_plus_uses_tech",
          "source": "paper:textbooks_are_all_you_need_next",
          "target": "tech:rotary_position_embedding_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-18_wired_9248",
              "url": "https://wired.com/2026/01/18/textbooks_are_all_you_need_nex",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Textbooks Are All You Need Next relies heavily on Rotary Position Embedding Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_max_grok_3_max_integrates_with",
          "source": "repo:vllm_max",
          "target": "model:grok_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-22_reuters_9800",
              "url": "https://reuters.com/technology/2026/01/22/vllm_max_grok_3_max",
              "published": "2026-01-22",
              "snippet": "vllm Max now supports Grok-3 Max with full feature parity..."
            },
            {
              "docId": "2026-01-24_reuters_8571",
              "url": "https://reuters.com/technology/2026/01/24/vllm_max_grok_3_max",
              "published": "2026-01-24",
              "snippet": "vllm Max now supports Grok-3 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_command_r_plus_lite_integrates_with",
          "source": "repo:ollama_core",
          "target": "model:command_r_plus_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-14_weights_and_bia_7528",
              "url": "https://wandb.ai/articles/2026/01/14/ollama_core_command_r_plus_lit",
              "published": "2026-01-14",
              "snippet": "The latest release of ollama Core adds native Command R+ Lite integration..."
            },
            {
              "docId": "2026-01-18_ars_technica_7450",
              "url": "https://arstechnica.com/2026/01/18/ollama_core_command_r_plus_lit",
              "published": "2026-01-18",
              "snippet": "ollama Core announced official support for Command R+ Lite..."
            },
            {
              "docId": "2026-01-18_hugging_face_bl_1191",
              "url": "https://huggingface.co/blog/2026/01/18/ollama_core_command_r_plus_lit",
              "published": "2026-01-18",
              "snippet": "ollama Core now supports Command R+ Lite with full feature parity..."
            },
            {
              "docId": "2026-01-20_techcrunch_1500",
              "url": "https://techcrunch.com/2026/01/20/ollama_core_command_r_plus_lit",
              "published": "2026-01-20",
              "snippet": "The latest release of ollama Core adds native Command R+ Lite integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_data_constrained_language_models_llama_4_ultra_evaluated_on",
          "source": "paper:scaling_data_constrained_language_models",
          "target": "model:llama_4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:palm_3_core_group_query_attention_lite_uses_tech",
          "source": "model:palm_3_core",
          "target": "tech:group_query_attention_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_7430",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/palm_3_core_group_query_attent",
              "published": "2026-01-18",
              "snippet": "Under the hood, PaLM 3 Core implements Group Query Attention Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-24_openai_blog_9075",
              "url": "https://openai.com/blog/2026/01/24/palm_3_core_group_query_attent",
              "published": "2026-01-24",
              "snippet": "Technical details reveal PaLM 3 Core relies heavily on Group Query Attention Lite..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_4410",
              "url": "https://huggingface.co/blog/2026/01/24/palm_3_core_group_query_attent",
              "published": "2026-01-24",
              "snippet": "Under the hood, PaLM 3 Core implements Group Query Attention Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:alpacaeval_2_lite_qwen_3_lite_measures",
          "source": "benchmark:alpacaeval_2_lite",
          "target": "model:qwen_3_lite",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_qwen_3_plus_integrates_with",
          "source": "repo:langchain_plus",
          "target": "model:qwen_3_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-15_nextgov_1974",
              "url": "https://nextgov.com/2026/01/15/langchain_plus_qwen_3_plus",
              "published": "2026-01-15",
              "snippet": "langchain Plus announced official support for Qwen-3 Plus..."
            },
            {
              "docId": "2026-01-19_reuters_8889",
              "url": "https://reuters.com/technology/2026/01/19/langchain_plus_qwen_3_plus",
              "published": "2026-01-19",
              "snippet": "The latest release of langchain Plus adds native Qwen-3 Plus integration..."
            },
            {
              "docId": "2026-01-23_openai_blog_8349",
              "url": "https://openai.com/blog/2026/01/23/langchain_plus_qwen_3_plus",
              "published": "2026-01-23",
              "snippet": "langchain Plus announced official support for Qwen-3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_lite_constitutional_ai_next_uses_tech",
          "source": "tool:streamlit_lite",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_chain_of_thought_uses_tech",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_qwen_3_mini_integrates_with",
          "source": "repo:text_generation_webui_max",
          "target": "model:qwen_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-24_google_ai_blog_7465",
              "url": "https://blog.google/technology/ai/2026/01/24/text_generation_webui_max_qwen",
              "published": "2026-01-24",
              "snippet": "text-generation-webui Max announced official support for Qwen-3 Mini..."
            },
            {
              "docId": "2026-01-25_bloomberg_1348",
              "url": "https://bloomberg.com/technology/2026/01/25/text_generation_webui_max_qwen",
              "published": "2026-01-25",
              "snippet": "text-generation-webui Max announced official support for Qwen-3 Mini..."
            },
            {
              "docId": "2026-01-25_langchain_blog_4561",
              "url": "https://blog.langchain.dev/2026/01/25/text_generation_webui_max_qwen",
              "published": "2026-01-25",
              "snippet": "text-generation-webui Max announced official support for Qwen-3 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_flash_attention_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:arc_agi_falcon_3_plus_measures",
          "source": "benchmark:arc_agi",
          "target": "model:falcon_3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-20_microsoft_resea_7501",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/arc_agi_falcon_3_plus",
              "published": "2026-01-20",
              "snippet": "ARC-AGI has become the standard for evaluating Falcon 3 Plus..."
            },
            {
              "docId": "2026-01-21_the_verge_9790",
              "url": "https://theverge.com/2026/01/21/arc_agi_falcon_3_plus",
              "published": "2026-01-21",
              "snippet": "ARC-AGI has become the standard for evaluating Falcon 3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_openwebtext2_edge_trained_on",
          "source": "model:gemma_3_mini",
          "target": "dataset:openwebtext2_edge",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.5
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_palm_3_max_evaluated_on",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "model:palm_3_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-13_bloomberg_5830",
              "url": "https://bloomberg.com/technology/2025/12/13/textbooks_are_all_you_need_min",
              "published": "2025-12-13",
              "snippet": "Evaluation results show Textbooks Are All You Need Mini reaching 74% on PaLM 3 Max..."
            },
            {
              "docId": "2025-12-24_hugging_face_bl_9330",
              "url": "https://huggingface.co/blog/2025/12/24/textbooks_are_all_you_need_min",
              "published": "2025-12-24",
              "snippet": "Evaluation results show Textbooks Are All You Need Mini reaching 71% on PaLM 3 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_v2_rlhf_max_uses_tech",
          "source": "model:midjourney_v7_v2",
          "target": "tech:rlhf_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-13_weights_and_bia_5067",
              "url": "https://wandb.ai/articles/2026/01/13/midjourney_v7_v2_rlhf_max",
              "published": "2026-01-13",
              "snippet": "Midjourney V7 v2 leverages RLHF Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-20_mit_technology__7900",
              "url": "https://technologyreview.com/2026/01/20/midjourney_v7_v2_rlhf_max",
              "published": "2026-01-20",
              "snippet": "Midjourney V7 v2 leverages RLHF Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_7919",
              "url": "https://anthropic.com/news/2026/01/25/midjourney_v7_v2_rlhf_max",
              "published": "2026-01-25",
              "snippet": "Under the hood, Midjourney V7 v2 implements RLHF Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_next_falcon_3_core_depends_on",
          "source": "model:llama_4_next",
          "target": "model:falcon_3_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_dolma_lite_trained_on",
          "source": "model:command_r_plus",
          "target": "dataset:dolma_lite",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:gpqa_ultra_codex_2_plus_measures",
          "source": "benchmark:gpqa_ultra",
          "target": "model:codex_2_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-13_techcrunch_5139",
              "url": "https://techcrunch.com/2026/01/13/gpqa_ultra_codex_2_plus",
              "published": "2026-01-13",
              "snippet": "The GPQA Ultra benchmark measures Codex 2 Plus across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_lite_qlora_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:qlora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-21_wired_3657",
              "url": "https://wired.com/2026/01/21/localai_lite_qlora",
              "published": "2026-01-21",
              "snippet": "Under the hood, LocalAI Lite implements QLoRA for improved efficiency..."
            },
            {
              "docId": "2026-01-24_nextgov_7842",
              "url": "https://nextgov.com/2026/01/24/localai_lite_qlora",
              "published": "2026-01-24",
              "snippet": "LocalAI Lite leverages QLoRA to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_venturebeat_2584",
              "url": "https://venturebeat.com/2026/01/25/localai_lite_qlora",
              "published": "2026-01-25",
              "snippet": "Under the hood, LocalAI Lite implements QLoRA for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_plus_jamba_2_v2_measures",
          "source": "benchmark:mmlu_plus",
          "target": "model:jamba_2_v2",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-23_the_gradient_8759",
              "url": "https://thegradient.pub/2026/01/23/mmlu_plus_jamba_2_v2",
              "published": "2026-01-23",
              "snippet": "MMLU Plus has become the standard for evaluating Jamba 2 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mbpp_core_jamba_2_ultra_measures",
          "source": "benchmark:mbpp_core",
          "target": "model:jamba_2_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-03_nextgov_5337",
              "url": "https://nextgov.com/2026/01/03/mbpp_core_jamba_2_ultra",
              "published": "2026-01-03",
              "snippet": "MBPP Core provides standardized evaluation of Jamba 2 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__lora_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-15_microsoft_resea_7980",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/15/constitutional_ai:_harmlessnes",
              "published": "2026-01-15",
              "snippet": "Under the hood, Constitutional AI: Harmlessness from AI Feedback implements LoRA for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_max_retrieval_augmented_generation_core_uses_tech",
          "source": "tool:crewai_max",
          "target": "tech:retrieval_augmented_generation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-22_wired_7180",
              "url": "https://wired.com/2026/01/22/crewai_max_retrieval_augmented",
              "published": "2026-01-22",
              "snippet": "Technical details reveal CrewAI Max relies heavily on Retrieval-Augmented Generation Core..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_4685",
              "url": "https://ai.meta.com/blog/2026/01/25/crewai_max_retrieval_augmented",
              "published": "2026-01-25",
              "snippet": "Technical details reveal CrewAI Max relies heavily on Retrieval-Augmented Generation Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_llama_4_plus_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:llama_4_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-13_the_gradient_7796",
              "url": "https://thegradient.pub/2026/01/13/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-13",
              "snippet": "On the Llama 4 Plus benchmark, LoRA: Low-Rank Adaptation of Large Language Models scored 97%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_lora_v2_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:lora_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-09_venturebeat_8555",
              "url": "https://venturebeat.com/2026/01/09/direct_preference_optimization",
              "published": "2026-01-09",
              "snippet": "Technical details reveal Direct Preference Optimization Next relies heavily on LoRA v2..."
            },
            {
              "docId": "2026-01-14_meta_ai_blog_8674",
              "url": "https://ai.meta.com/blog/2026/01/14/direct_preference_optimization",
              "published": "2026-01-14",
              "snippet": "Technical details reveal Direct Preference Optimization Next relies heavily on LoRA v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_rlhf_ultra_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:rlhf_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:mlflow_lite_dall_e_4_edge_integrates_with",
          "source": "tool:mlflow_lite",
          "target": "model:dall_e_4_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:autogpt_pro_speculative_decoding_lite_uses_tech",
          "source": "tool:autogpt_pro",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_next_alpacaeval_2_core_evaluated_on",
          "source": "model:gpt_4o_mini_2_next",
          "target": "benchmark:alpacaeval_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-14_meta_ai_blog_7919",
              "url": "https://ai.meta.com/blog/2025/12/14/gpt_4o_mini_2_next_alpacaeval_",
              "published": "2025-12-14",
              "snippet": "GPT-4o Mini 2 Next achieves 79% on AlpacaEval 2 Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_claude_sonnet_4_v2_depends_on",
          "source": "model:mixtral_8x22b",
          "target": "model:claude_sonnet_4_v2",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:cursor_max_deepseek_v3_v2_integrates_with",
          "source": "tool:cursor_max",
          "target": "model:deepseek_v3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_meta_ai_blog_8257",
              "url": "https://ai.meta.com/blog/2026/01/25/cursor_max_deepseek_v3_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of Cursor Max adds native DeepSeek-V3 v2 integration..."
            },
            {
              "docId": "2026-01-25_wired_6340",
              "url": "https://wired.com/2026/01/25/cursor_max_deepseek_v3_v2",
              "published": "2026-01-25",
              "snippet": "The latest release of Cursor Max adds native DeepSeek-V3 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_jamba_2_mini_depends_on",
          "source": "model:sora_2_plus",
          "target": "model:jamba_2_mini",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:llamacpp_plus_kv_cache_optimization_ultra_uses_tech",
          "source": "repo:llamacpp_plus",
          "target": "tech:kv_cache_optimization_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-14_the_gradient_6341",
              "url": "https://thegradient.pub/2026/01/14/llamacpp_plus_kv_cache_optimiz",
              "published": "2026-01-14",
              "snippet": "Under the hood, llama.cpp Plus implements KV Cache Optimization Ultra for improved efficiency..."
            },
            {
              "docId": "2026-01-24_hugging_face_bl_6128",
              "url": "https://huggingface.co/blog/2026/01/24/llamacpp_plus_kv_cache_optimiz",
              "published": "2026-01-24",
              "snippet": "Technical details reveal llama.cpp Plus relies heavily on KV Cache Optimization Ultra..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_2149",
              "url": "https://blogs.nvidia.com/2026/01/24/llamacpp_plus_kv_cache_optimiz",
              "published": "2026-01-24",
              "snippet": "llama.cpp Plus leverages KV Cache Optimization Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_pro_mt_bench_plus_evaluated_on",
          "source": "model:mixtral_8x22b_pro",
          "target": "benchmark:mt_bench_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-02_microsoft_resea_8897",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/02/mixtral_8x22b_pro_mt_bench_plu",
              "published": "2026-01-02",
              "snippet": "Mixtral 8x22B Pro achieves 82% on MT-Bench Plus, setting a new record..."
            },
            {
              "docId": "2026-01-17_the_verge_4266",
              "url": "https://theverge.com/2026/01/17/mixtral_8x22b_pro_mt_bench_plu",
              "published": "2026-01-17",
              "snippet": "Evaluation results show Mixtral 8x22B Pro reaching 83% on MT-Bench Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_jamba_2_integrates_with",
          "source": "repo:ollama_ultra",
          "target": "model:jamba_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-25_venturebeat_5907",
              "url": "https://venturebeat.com/2026/01/25/ollama_ultra_jamba_2",
              "published": "2026-01-25",
              "snippet": "The latest release of ollama Ultra adds native Jamba 2 integration..."
            },
            {
              "docId": "2026-01-25_openai_blog_3364",
              "url": "https://openai.com/blog/2026/01/25/ollama_ultra_jamba_2",
              "published": "2026-01-25",
              "snippet": "ollama Ultra now supports Jamba 2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tree_of_thoughts:_deliberate_problem_sol_dpo_uses_tech",
          "source": "paper:tree_of_thoughts:_deliberate_problem_sol",
          "target": "tech:dpo",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-09_reuters_6537",
              "url": "https://reuters.com/technology/2026/01/09/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-09",
              "snippet": "Technical details reveal Tree of Thoughts: Deliberate Problem Solving with LLMs relies heavily on DPO..."
            },
            {
              "docId": "2026-01-21_the_verge_9856",
              "url": "https://theverge.com/2026/01/21/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Tree of Thoughts: Deliberate Problem Solving with LLMs relies heavily on DPO..."
            },
            {
              "docId": "2026-01-23_bloomberg_3712",
              "url": "https://bloomberg.com/technology/2026/01/23/tree_of_thoughts:_deliberate_p",
              "published": "2026-01-23",
              "snippet": "Tree of Thoughts: Deliberate Problem Solving with LLMs leverages DPO to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_tool_use_plus_uses_tech",
          "source": "tool:cursor",
          "target": "tech:tool_use_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_bigbench_hard_evaluated_on",
          "source": "model:stable_diffusion_4_next",
          "target": "benchmark:bigbench_hard",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_3732",
              "url": "https://huggingface.co/blog/2026/01/25/stable_diffusion_4_next_bigben",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Stable Diffusion 4 Next reaching 91% on BigBench Hard..."
            },
            {
              "docId": "2026-01-25_nextgov_4569",
              "url": "https://nextgov.com/2026/01/25/stable_diffusion_4_next_bigben",
              "published": "2026-01-25",
              "snippet": "Stable Diffusion 4 Next achieves 93% on BigBench Hard, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:falcon_3_next_red_teaming_edge_uses_tech",
          "source": "model:falcon_3_next",
          "target": "tech:red_teaming_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.55,
          "evidence": [
            {
              "docId": "2025-12-25_hugging_face_bl_8113",
              "url": "https://huggingface.co/blog/2025/12/25/falcon_3_next_red_teaming_edge",
              "published": "2025-12-25",
              "snippet": "Under the hood, Falcon 3 Next implements Red Teaming Edge for improved efficiency..."
            },
            {
              "docId": "2025-12-31_google_ai_blog_8796",
              "url": "https://blog.google/technology/ai/2025/12/31/falcon_3_next_red_teaming_edge",
              "published": "2025-12-31",
              "snippet": "Falcon 3 Next leverages Red Teaming Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_command_r_plus_mini_integrates_with",
          "source": "repo:gpt4all_pro",
          "target": "model:command_r_plus_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_meta_ai_blog_7875",
              "url": "https://ai.meta.com/blog/2026/01/25/gpt4all_pro_command_r_plus_min",
              "published": "2026-01-25",
              "snippet": "The latest release of gpt4all Pro adds native Command R+ Mini integration..."
            },
            {
              "docId": "2026-01-25_arxiv_5263",
              "url": "https://arxiv.org/abs/2026/01/25/gpt4all_pro_command_r_plus_min",
              "published": "2026-01-25",
              "snippet": "gpt4all Pro announced official support for Command R+ Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_dpo_v2_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:dpo_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.93
        }
      },
      {
        "data": {
          "id": "e:ollama_core_flash_attention_lite_uses_tech",
          "source": "repo:ollama_core",
          "target": "tech:flash_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_plus_sparse_attention_pro_uses_tech",
          "source": "dataset:redpajama_v2_plus",
          "target": "tech:sparse_attention_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_lora_ultra_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:lora_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_1215",
              "url": "https://techcrunch.com/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Direct Preference Optimization Next leverages LoRA Ultra to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_pro_grok_3_plus_measures",
          "source": "benchmark:gpqa_pro",
          "target": "model:grok_3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2025-12-12_techcrunch_2945",
              "url": "https://techcrunch.com/2025/12/12/gpqa_pro_grok_3_plus",
              "published": "2025-12-12",
              "snippet": "GPQA Pro provides standardized evaluation of Grok-3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_bigbench_hard_evaluated_on",
          "source": "model:mixtral_8x22b_plus",
          "target": "benchmark:bigbench_hard",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-25_nextgov_5248",
              "url": "https://nextgov.com/2026/01/25/mixtral_8x22b_plus_bigbench_ha",
              "published": "2026-01-25",
              "snippet": "Mixtral 8x22B Plus achieves 70% on BigBench Hard, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_plus_synthetic_data_generation_uses_tech",
          "source": "dataset:the_pile_plus",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_mini_refinedweb_pro_trained_on",
          "source": "model:mixtral_8x22b_mini",
          "target": "dataset:refinedweb_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_pro_gguf_lite_uses_tech",
          "source": "model:nemotron_5_pro",
          "target": "tech:gguf_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_v2_the_pile_next_trained_on",
          "source": "model:claude_opus_45_v2",
          "target": "dataset:the_pile_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_gemma_3_plus_depends_on",
          "source": "model:jamba_2_lite",
          "target": "model:gemma_3_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_refinedweb_trained_on",
          "source": "model:claude_opus_45",
          "target": "dataset:refinedweb",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-25_bloomberg_9300",
              "url": "https://bloomberg.com/technology/2026/01/25/claude_opus_45_refinedweb",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 was trained on RefinedWeb comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_qlora_core_uses_tech",
          "source": "repo:localai_plus",
          "target": "tech:qlora_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-21_techcrunch_7013",
              "url": "https://techcrunch.com/2026/01/21/localai_plus_qlora_core",
              "published": "2026-01-21",
              "snippet": "LocalAI Plus leverages QLoRA Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_the_gradient_3833",
              "url": "https://thegradient.pub/2026/01/23/localai_plus_qlora_core",
              "published": "2026-01-23",
              "snippet": "Technical details reveal LocalAI Plus relies heavily on QLoRA Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_tree_of_thought_mini_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:tree_of_thought_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2025-11-27_nvidia_blog_3054",
              "url": "https://blogs.nvidia.com/2025/11/27/self_play_fine_tuning_for_lang",
              "published": "2025-11-27",
              "snippet": "Under the hood, Self-Play Fine-Tuning for Language Models implements Tree of Thought Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_ultra_speculative_decoding_lite_uses_tech",
          "source": "model:jamba_2_ultra",
          "target": "tech:speculative_decoding_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:langchain_core_streamlit_max_integrates_with",
          "source": "tool:langchain_core",
          "target": "tool:streamlit_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_3070",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/langchain_core_streamlit_max",
              "published": "2026-01-18",
              "snippet": "LangChain Core now supports Streamlit Max with full feature parity..."
            },
            {
              "docId": "2026-01-21_wired_1230",
              "url": "https://wired.com/2026/01/21/langchain_core_streamlit_max",
              "published": "2026-01-21",
              "snippet": "LangChain Core now supports Streamlit Max with full feature parity..."
            },
            {
              "docId": "2026-01-21_nextgov_3358",
              "url": "https://nextgov.com/2026/01/21/langchain_core_streamlit_max",
              "published": "2026-01-21",
              "snippet": "LangChain Core announced official support for Streamlit Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:math_llama_4_plus_measures",
          "source": "benchmark:math",
          "target": "model:llama_4_plus",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_swe_bench_next_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:swe_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_max_c4_trained_on",
          "source": "model:claude_sonnet_4_max",
          "target": "dataset:c4",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-10_nvidia_blog_9284",
              "url": "https://blogs.nvidia.com/2026/01/10/claude_sonnet_4_max_c4",
              "published": "2026-01-10",
              "snippet": "Claude Sonnet 4 Max was trained on C4 comprising billions of tokens..."
            },
            {
              "docId": "2026-01-23_mit_technology__6297",
              "url": "https://technologyreview.com/2026/01/23/claude_sonnet_4_max_c4",
              "published": "2026-01-23",
              "snippet": "The training corpus for Claude Sonnet 4 Max includes C4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_edge_gpqa_lite_evaluated_on",
          "source": "model:mixtral_8x22b_edge",
          "target": "benchmark:gpqa_lite",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-18_ars_technica_2463",
              "url": "https://arstechnica.com/2026/01/18/mixtral_8x22b_edge_gpqa_lite",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Mixtral 8x22B Edge reaching 74% on GPQA Lite..."
            },
            {
              "docId": "2026-01-22_the_verge_3905",
              "url": "https://theverge.com/2026/01/22/mixtral_8x22b_edge_gpqa_lite",
              "published": "2026-01-22",
              "snippet": "On the GPQA Lite benchmark, Mixtral 8x22B Edge scored 71%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_max_distillation_pro_uses_tech",
          "source": "tool:copilot_max",
          "target": "tech:distillation_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_qwen_3_lite_depends_on",
          "source": "model:deepseek_v3_max",
          "target": "model:qwen_3_lite",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:vllm_core_phi_4_integrates_with",
          "source": "repo:vllm_core",
          "target": "model:phi_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-15_venturebeat_4053",
              "url": "https://venturebeat.com/2026/01/15/vllm_core_phi_4",
              "published": "2026-01-15",
              "snippet": "The latest release of vllm Core adds native Phi-4 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_v2_mixtral_8x22b_pro_depends_on",
          "source": "model:nemotron_5_v2",
          "target": "model:mixtral_8x22b_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_lora_next_uses_tech",
          "source": "repo:open_interpreter",
          "target": "tech:lora_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_8300",
              "url": "https://huggingface.co/blog/2026/01/22/open_interpreter_lora_next",
              "published": "2026-01-22",
              "snippet": "open-interpreter leverages LoRA Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_codex_2_core_measures",
          "source": "benchmark:arc_agi",
          "target": "model:codex_2_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-17_techcrunch_5363",
              "url": "https://techcrunch.com/2026/01/17/arc_agi_codex_2_core",
              "published": "2026-01-17",
              "snippet": "The ARC-AGI benchmark measures Codex 2 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_next_rlhf_next_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:rlhf_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.69,
          "evidence": [
            {
              "docId": "2026-01-09_hugging_face_bl_3726",
              "url": "https://huggingface.co/blog/2026/01/09/transformers_next_rlhf_next",
              "published": "2026-01-09",
              "snippet": "Technical details reveal transformers Next relies heavily on RLHF Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dolma_pro_multimodal_fusion_pro_uses_tech",
          "source": "dataset:dolma_pro",
          "target": "tech:multimodal_fusion_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:deepseek_v3_max_grok_3_next_depends_on",
          "source": "model:deepseek_v3_max",
          "target": "model:grok_3_next",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mixture_of_experts_lite_uses_tech",
          "source": "paper:textbooks_are_all_you_need",
          "target": "tech:mixture_of_experts_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-05_bloomberg_3920",
              "url": "https://bloomberg.com/technology/2026/01/05/textbooks_are_all_you_need_mix",
              "published": "2026-01-05",
              "snippet": "Textbooks Are All You Need leverages Mixture of Experts Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-19_reuters_5174",
              "url": "https://reuters.com/technology/2026/01/19/textbooks_are_all_you_need_mix",
              "published": "2026-01-19",
              "snippet": "Textbooks Are All You Need leverages Mixture of Experts Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-21_anthropic_blog_9174",
              "url": "https://anthropic.com/news/2026/01/21/textbooks_are_all_you_need_mix",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Textbooks Are All You Need relies heavily on Mixture of Experts Lite..."
            },
            {
              "docId": "2026-01-22_google_ai_blog_8247",
              "url": "https://blog.google/technology/ai/2026/01/22/textbooks_are_all_you_need_mix",
              "published": "2026-01-22",
              "snippet": "Under the hood, Textbooks Are All You Need implements Mixture of Experts Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_next_aya_3_core_integrates_with",
          "source": "tool:localai_next",
          "target": "model:aya_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:flowise_next_grok_3_next_integrates_with",
          "source": "tool:flowise_next",
          "target": "model:grok_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-11-24_microsoft_resea_6124",
              "url": "https://microsoft.com/en-us/research/blog/2025/11/24/flowise_next_grok_3_next",
              "published": "2025-11-24",
              "snippet": "Flowise Next announced official support for Grok-3 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lite_humaneval_mini_evaluated_on",
          "source": "model:jamba_2_lite",
          "target": "benchmark:humaneval_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-18_nextgov_7216",
              "url": "https://nextgov.com/2026/01/18/jamba_2_lite_humaneval_mini",
              "published": "2026-01-18",
              "snippet": "Jamba 2 Lite achieves 80% on HumanEval Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_next_red_teaming_pro_uses_tech",
          "source": "paper:direct_preference_optimization_next",
          "target": "tech:red_teaming_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-25_techcrunch_9168",
              "url": "https://techcrunch.com/2025/12/25/direct_preference_optimization",
              "published": "2025-12-25",
              "snippet": "Direct Preference Optimization Next leverages Red Teaming Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-29_openai_blog_7239",
              "url": "https://openai.com/blog/2025/12/29/direct_preference_optimization",
              "published": "2025-12-29",
              "snippet": "Direct Preference Optimization Next leverages Red Teaming Pro to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-09_hugging_face_bl_4341",
              "url": "https://huggingface.co/blog/2026/01/09/direct_preference_optimization",
              "published": "2026-01-09",
              "snippet": "Direct Preference Optimization Next leverages Red Teaming Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_mini_flowise_edge_integrates_with",
          "source": "tool:gradio_mini",
          "target": "tool:flowise_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-21_microsoft_resea_6532",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/gradio_mini_flowise_edge",
              "published": "2026-01-21",
              "snippet": "Gradio Mini announced official support for Flowise Edge..."
            },
            {
              "docId": "2026-01-22_nvidia_blog_6353",
              "url": "https://blogs.nvidia.com/2026/01/22/gradio_mini_flowise_edge",
              "published": "2026-01-22",
              "snippet": "Gradio Mini now supports Flowise Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_the_pile_max_trained_on",
          "source": "model:claude_opus_45_lite",
          "target": "dataset:the_pile_max",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-24_venturebeat_8099",
              "url": "https://venturebeat.com/2026/01/24/claude_opus_45_lite_the_pile_m",
              "published": "2026-01-24",
              "snippet": "Claude Opus 4.5 Lite was trained on The Pile Max comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_max_yi_large_next_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_max",
          "target": "model:yi_large_next",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-10_google_ai_blog_3247",
              "url": "https://blog.google/technology/ai/2026/01/10/attention_is_all_you_need_v2_m",
              "published": "2026-01-10",
              "snippet": "On the Yi-Large Next benchmark, Attention Is All You Need v2 Max scored 95%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_claude_sonnet_4_ultra_depends_on",
          "source": "model:llama_4_plus",
          "target": "model:claude_sonnet_4_ultra",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:qwen_3_mini_llama_4_edge_depends_on",
          "source": "model:qwen_3_mini",
          "target": "model:llama_4_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:common_crawl_v2_chain_of_thought_edge_uses_tech",
          "source": "dataset:common_crawl_v2",
          "target": "tech:chain_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_pro_multimodal_fusion_ultra_uses_tech",
          "source": "paper:llm_agents:_a_survey_pro",
          "target": "tech:multimodal_fusion_ultra",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-11_nextgov_1259",
              "url": "https://nextgov.com/2026/01/11/llm_agents:_a_survey_pro_multi",
              "published": "2026-01-11",
              "snippet": "Under the hood, LLM Agents: A Survey Pro implements Multimodal Fusion Ultra for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_qlora_v2_uses_tech",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "tech:qlora_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:autogpt_pro_midjourney_v7_edge_integrates_with",
          "source": "tool:autogpt_pro",
          "target": "model:midjourney_v7_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_mini_rlhf_plus_uses_tech",
          "source": "paper:textbooks_are_all_you_need_mini",
          "target": "tech:rlhf_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:gemma_3_mini_nemotron_5_plus_depends_on",
          "source": "model:gemma_3_mini",
          "target": "model:nemotron_5_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_core_gsm8k_core_evaluated_on",
          "source": "model:gemini_ultra_2_core",
          "target": "benchmark:gsm8k_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_3737",
              "url": "https://anthropic.com/news/2026/01/24/gemini_ultra_2_core_gsm8k_core",
              "published": "2026-01-24",
              "snippet": "Gemini Ultra 2 Core achieves 74% on GSM8K Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_rotary_position_embedding_pro_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "tech:rotary_position_embedding_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_plus_group_query_attention_plus_uses_tech",
          "source": "model:mixtral_8x22b_plus",
          "target": "tech:group_query_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:hellaswag_plus_deepseek_v3_mini_measures",
          "source": "benchmark:hellaswag_plus",
          "target": "model:deepseek_v3_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_core_grok_3_depends_on",
          "source": "model:gemini_ultra_2_core",
          "target": "model:grok_3",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_edge_nemotron_5_lite_measures",
          "source": "benchmark:truthfulqa_edge",
          "target": "model:nemotron_5_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-11-18_nextgov_7415",
              "url": "https://nextgov.com/2025/11/18/truthfulqa_edge_nemotron_5_lit",
              "published": "2025-11-18",
              "snippet": "TruthfulQA Edge provides standardized evaluation of Nemotron-5 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_gradio_core_integrates_with",
          "source": "tool:gradio_plus",
          "target": "tool:gradio_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:litellm_plus_gpt_5_lite_integrates_with",
          "source": "tool:litellm_plus",
          "target": "model:gpt_5_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-23_the_verge_3063",
              "url": "https://theverge.com/2026/01/23/litellm_plus_gpt_5_lite",
              "published": "2026-01-23",
              "snippet": "LiteLLM Plus now supports GPT-5 Lite with full feature parity..."
            },
            {
              "docId": "2026-01-25_techcrunch_6195",
              "url": "https://techcrunch.com/2026/01/25/litellm_plus_gpt_5_lite",
              "published": "2026-01-25",
              "snippet": "LiteLLM Plus now supports GPT-5 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_tree_of_thought_next_uses_tech",
          "source": "repo:autogpt",
          "target": "tech:tree_of_thought_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_langchain_lite_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "tool:langchain_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-03_venturebeat_6173",
              "url": "https://venturebeat.com/2026/01/03/tensorrt_llm_langchain_lite",
              "published": "2026-01-03",
              "snippet": "TensorRT-LLM now supports LangChain Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_max_transformer_architecture_edge_uses_tech",
          "source": "dataset:openwebtext2_max",
          "target": "tech:transformer_architecture_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_core_synthetic_data_generation_plus_uses_tech",
          "source": "model:mixtral_8x22b_core",
          "target": "tech:synthetic_data_generation_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_edge_alpacaeval_2_core_evaluated_on",
          "source": "model:whisper_v4_edge",
          "target": "benchmark:alpacaeval_2_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-10_google_ai_blog_4013",
              "url": "https://blog.google/technology/ai/2026/01/10/whisper_v4_edge_alpacaeval_2_c",
              "published": "2026-01-10",
              "snippet": "On the AlpacaEval 2 Core benchmark, Whisper v4 Edge scored 99%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_core_lora_v2_uses_tech",
          "source": "dataset:openwebtext2_core",
          "target": "tech:lora_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:transformers_edge_mixtral_8x22b_max_integrates_with",
          "source": "repo:transformers_edge",
          "target": "model:mixtral_8x22b_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2025-12-04_microsoft_resea_8976",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/04/transformers_edge_mixtral_8x22",
              "published": "2025-12-04",
              "snippet": "The latest release of transformers Edge adds native Mixtral 8x22B Max integration..."
            },
            {
              "docId": "2025-12-15_the_verge_5468",
              "url": "https://theverge.com/2025/12/15/transformers_edge_mixtral_8x22",
              "published": "2025-12-15",
              "snippet": "transformers Edge announced official support for Mixtral 8x22B Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_nemotron_5_edge_integrates_with",
          "source": "repo:vllm_core",
          "target": "model:nemotron_5_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-15_meta_ai_blog_4302",
              "url": "https://ai.meta.com/blog/2026/01/15/vllm_core_nemotron_5_edge",
              "published": "2026-01-15",
              "snippet": "vllm Core now supports Nemotron-5 Edge with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_palm_3_next_integrates_with",
          "source": "repo:ollama_edge",
          "target": "model:palm_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_next_rlhf_pro_uses_tech",
          "source": "tool:weights_and_biases_next",
          "target": "tech:rlhf_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-24_anthropic_blog_9270",
              "url": "https://anthropic.com/news/2026/01/24/weights_and_biases_next_rlhf_p",
              "published": "2026-01-24",
              "snippet": "Weights & Biases Next leverages RLHF Pro to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:arc_agi_max_grok_3_max_measures",
          "source": "benchmark:arc_agi_max",
          "target": "model:grok_3_max",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_yi_large_next_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:yi_large_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:phi_4_plus_swe_bench_evaluated_on",
          "source": "model:phi_4_plus",
          "target": "benchmark:swe_bench",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-02_nvidia_blog_2399",
              "url": "https://blogs.nvidia.com/2026/01/02/phi_4_plus_swe_bench",
              "published": "2026-01-02",
              "snippet": "On the SWE-bench benchmark, Phi-4 Plus scored 85%..."
            },
            {
              "docId": "2026-01-02_techcrunch_5236",
              "url": "https://techcrunch.com/2026/01/02/phi_4_plus_swe_bench",
              "published": "2026-01-02",
              "snippet": "Phi-4 Plus achieves 95% on SWE-bench, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_plus_autogpt_pro_integrates_with",
          "source": "tool:gradio_plus",
          "target": "tool:autogpt_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-23_meta_ai_blog_1248",
              "url": "https://ai.meta.com/blog/2026/01/23/gradio_plus_autogpt_pro",
              "published": "2026-01-23",
              "snippet": "The latest release of Gradio Plus adds native AutoGPT Pro integration..."
            },
            {
              "docId": "2026-01-23_arxiv_2458",
              "url": "https://arxiv.org/abs/2026/01/23/gradio_plus_autogpt_pro",
              "published": "2026-01-23",
              "snippet": "Gradio Plus announced official support for AutoGPT Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_lite_gemini_ultra_2_core_depends_on",
          "source": "model:claude_opus_45_lite",
          "target": "model:gemini_ultra_2_core",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:ollama_mixtral_8x22b_integrates_with",
          "source": "repo:ollama",
          "target": "model:mixtral_8x22b",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2025-11-30_arxiv_3464",
              "url": "https://arxiv.org/abs/2025/11/30/ollama_mixtral_8x22b",
              "published": "2025-11-30",
              "snippet": "ollama announced official support for Mixtral 8x22B..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_ultra_sora_2_plus_depends_on",
          "source": "model:sora_2_ultra",
          "target": "model:sora_2_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:palm_3_edge_qlora_plus_uses_tech",
          "source": "model:palm_3_edge",
          "target": "tech:qlora_plus",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-23_openai_blog_5133",
              "url": "https://openai.com/blog/2026/01/23/palm_3_edge_qlora_plus",
              "published": "2026-01-23",
              "snippet": "Under the hood, PaLM 3 Edge implements QLoRA Plus for improved efficiency..."
            },
            {
              "docId": "2026-01-25_venturebeat_6847",
              "url": "https://venturebeat.com/2026/01/25/palm_3_edge_qlora_plus",
              "published": "2026-01-25",
              "snippet": "PaLM 3 Edge leverages QLoRA Plus to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_gradio_edge_integrates_with",
          "source": "tool:cursor",
          "target": "tool:gradio_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-09_techcrunch_3139",
              "url": "https://techcrunch.com/2026/01/09/cursor_gradio_edge",
              "published": "2026-01-09",
              "snippet": "Cursor now supports Gradio Edge with full feature parity..."
            },
            {
              "docId": "2026-01-19_langchain_blog_9312",
              "url": "https://blog.langchain.dev/2026/01/19/cursor_gradio_edge",
              "published": "2026-01-19",
              "snippet": "The latest release of Cursor adds native Gradio Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_ultra_gemini_ultra_2_lite_measures",
          "source": "benchmark:winogrande_ultra",
          "target": "model:gemini_ultra_2_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-17_anthropic_blog_8030",
              "url": "https://anthropic.com/news/2026/01/17/winogrande_ultra_gemini_ultra_",
              "published": "2026-01-17",
              "snippet": "WinoGrande Ultra has become the standard for evaluating Gemini Ultra 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_edge_gpt_4o_mini_2_integrates_with",
          "source": "tool:autogpt_edge",
          "target": "model:gpt_4o_mini_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-25_microsoft_resea_8598",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/autogpt_edge_gpt_4o_mini_2",
              "published": "2026-01-25",
              "snippet": "AutoGPT Edge now supports GPT-4o Mini 2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_max_rlhf_core_uses_tech",
          "source": "tool:semantic_kernel_max",
          "target": "tech:rlhf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:mc4_plus_lora_max_uses_tech",
          "source": "dataset:mc4_plus",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_max_llama_4_pro_measures",
          "source": "benchmark:truthfulqa_max",
          "target": "model:llama_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-12_langchain_blog_3570",
              "url": "https://blog.langchain.dev/2026/01/12/truthfulqa_max_llama_4_pro",
              "published": "2026-01-12",
              "snippet": "TruthfulQA Max has become the standard for evaluating Llama 4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:phi_4_gsm8k_edge_evaluated_on",
          "source": "model:phi_4",
          "target": "benchmark:gsm8k_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.98
        }
      },
      {
        "data": {
          "id": "e:dify_v2_claude_opus_45_plus_integrates_with",
          "source": "tool:dify_v2",
          "target": "model:claude_opus_45_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.95,
          "evidence": [
            {
              "docId": "2026-01-15_mit_technology__1960",
              "url": "https://technologyreview.com/2026/01/15/dify_v2_claude_opus_45_plus",
              "published": "2026-01-15",
              "snippet": "The latest release of Dify v2 adds native Claude Opus 4.5 Plus integration..."
            },
            {
              "docId": "2026-01-18_weights_and_bia_7666",
              "url": "https://wandb.ai/articles/2026/01/18/dify_v2_claude_opus_45_plus",
              "published": "2026-01-18",
              "snippet": "Dify v2 now supports Claude Opus 4.5 Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:jamba_2_c4_next_trained_on",
          "source": "model:jamba_2",
          "target": "dataset:c4_next",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-25_the_verge_8834",
              "url": "https://theverge.com/2026/01/25/jamba_2_c4_next",
              "published": "2026-01-25",
              "snippet": "Jamba 2 was trained on C4 Next comprising billions of tokens..."
            },
            {
              "docId": "2026-01-25_techcrunch_8467",
              "url": "https://techcrunch.com/2026/01/25/jamba_2_c4_next",
              "published": "2026-01-25",
              "snippet": "The training corpus for Jamba 2 includes C4 Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_core_gpt_4o_mini_2_depends_on",
          "source": "model:palm_3_core",
          "target": "model:gpt_4o_mini_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_plus_sparse_attention_lite_uses_tech",
          "source": "dataset:the_stack_v2_plus",
          "target": "tech:sparse_attention_lite",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:slimpajama_pro_constitutional_ai_v2_uses_tech",
          "source": "dataset:slimpajama_pro",
          "target": "tech:constitutional_ai_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_autogpt_integrates_with",
          "source": "tool:flowise_plus",
          "target": "tool:autogpt",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.55,
          "evidence": [
            {
              "docId": "2026-01-03_venturebeat_4503",
              "url": "https://venturebeat.com/2026/01/03/flowise_plus_autogpt",
              "published": "2026-01-03",
              "snippet": "Flowise Plus now supports AutoGPT with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gradio_edge_weights_and_biases_pro_integrates_with",
          "source": "tool:gradio_edge",
          "target": "tool:weights_and_biases_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.55,
          "evidence": [
            {
              "docId": "2025-12-31_the_gradient_5882",
              "url": "https://thegradient.pub/2025/12/31/gradio_edge_weights_and_biases",
              "published": "2025-12-31",
              "snippet": "Gradio Edge now supports Weights & Biases Pro with full feature parity..."
            },
            {
              "docId": "2026-01-17_weights_and_bia_1577",
              "url": "https://wandb.ai/articles/2026/01/17/gradio_edge_weights_and_biases",
              "published": "2026-01-17",
              "snippet": "Gradio Edge now supports Weights & Biases Pro with full feature parity..."
            },
            {
              "docId": "2026-01-23_mit_technology__9496",
              "url": "https://technologyreview.com/2026/01/23/gradio_edge_weights_and_biases",
              "published": "2026-01-23",
              "snippet": "Gradio Edge announced official support for Weights & Biases Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_core_lora_edge_uses_tech",
          "source": "paper:llm_agents:_a_survey_core",
          "target": "tech:lora_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-06_nextgov_3415",
              "url": "https://nextgov.com/2026/01/06/llm_agents:_a_survey_core_lora",
              "published": "2026-01-06",
              "snippet": "Technical details reveal LLM Agents: A Survey Core relies heavily on LoRA Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_plus_gemma_3_v2_integrates_with",
          "source": "tool:flowise_plus",
          "target": "model:gemma_3_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_mbpp_v2_evaluated_on",
          "source": "model:gemma_3_plus",
          "target": "benchmark:mbpp_v2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_falcon_3_plus_evaluated_on",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "model:falcon_3_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-16_weights_and_bia_7022",
              "url": "https://wandb.ai/articles/2026/01/16/self_play_fine_tuning_for_lang",
              "published": "2026-01-16",
              "snippet": "Self-Play Fine-Tuning for Language Models achieves 75% on Falcon 3 Plus, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_core_fineweb_pro_trained_on",
          "source": "model:qwen_3_core",
          "target": "dataset:fineweb_pro",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:falcon_3_ultra_kv_cache_optimization_next_uses_tech",
          "source": "model:falcon_3_ultra",
          "target": "tech:kv_cache_optimization_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:gemma_3_plus_humaneval_plus_evaluated_on",
          "source": "model:gemma_3_plus",
          "target": "benchmark:humaneval_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-21_techcrunch_9666",
              "url": "https://techcrunch.com/2026/01/21/gemma_3_plus_humaneval_plus",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Gemma 3 Plus reaching 77% on HumanEval Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_codex_2_edge_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:codex_2_edge",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:falcon_3_v2_nemotron_5_depends_on",
          "source": "model:falcon_3_v2",
          "target": "model:nemotron_5",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:jamba_2_lora_core_uses_tech",
          "source": "model:jamba_2",
          "target": "tech:lora_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:autogpt_next_haystack_edge_integrates_with",
          "source": "tool:autogpt_next",
          "target": "tool:haystack_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-06_anthropic_blog_3540",
              "url": "https://anthropic.com/news/2026/01/06/autogpt_next_haystack_edge",
              "published": "2026-01-06",
              "snippet": "The latest release of AutoGPT Next adds native Haystack Edge integration..."
            },
            {
              "docId": "2026-01-21_openai_blog_7795",
              "url": "https://openai.com/blog/2026/01/21/autogpt_next_haystack_edge",
              "published": "2026-01-21",
              "snippet": "The latest release of AutoGPT Next adds native Haystack Edge integration..."
            },
            {
              "docId": "2026-01-22_arxiv_3977",
              "url": "https://arxiv.org/abs/2026/01/22/autogpt_next_haystack_edge",
              "published": "2026-01-22",
              "snippet": "The latest release of AutoGPT Next adds native Haystack Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_constitutional_ai_next_uses_tech",
          "source": "repo:localai_plus",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_pro_flash_attention_uses_tech",
          "source": "dataset:redpajama_v2_pro",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_group_query_attention_core_uses_tech",
          "source": "model:midjourney_v7",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_lite_mixture_of_experts_uses_tech",
          "source": "tool:semantic_kernel_lite",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:gpt4all_tree_of_thought_edge_uses_tech",
          "source": "repo:gpt4all",
          "target": "tech:tree_of_thought_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-25_hugging_face_bl_4147",
              "url": "https://huggingface.co/blog/2026/01/25/gpt4all_tree_of_thought_edge",
              "published": "2026-01-25",
              "snippet": "gpt4all leverages Tree of Thought Edge to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_9944",
              "url": "https://huggingface.co/blog/2026/01/25/gpt4all_tree_of_thought_edge",
              "published": "2026-01-25",
              "snippet": "gpt4all leverages Tree of Thought Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_max_constitutional_ai_v2_uses_tech",
          "source": "tool:mlflow_max",
          "target": "tech:constitutional_ai_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_constitutional_ai_next_uses_tech",
          "source": "repo:ollama_mini",
          "target": "tech:constitutional_ai_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-12_microsoft_resea_8090",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/12/ollama_mini_constitutional_ai_",
              "published": "2026-01-12",
              "snippet": "ollama Mini leverages Constitutional AI Next to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-13_weights_and_bia_4587",
              "url": "https://wandb.ai/articles/2026/01/13/ollama_mini_constitutional_ai_",
              "published": "2026-01-13",
              "snippet": "ollama Mini leverages Constitutional AI Next to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_reuters_4695",
              "url": "https://reuters.com/technology/2026/01/24/ollama_mini_constitutional_ai_",
              "published": "2026-01-24",
              "snippet": "ollama Mini leverages Constitutional AI Next to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_venturebeat_7965",
              "url": "https://venturebeat.com/2026/01/24/ollama_mini_constitutional_ai_",
              "published": "2026-01-24",
              "snippet": "Technical details reveal ollama Mini relies heavily on Constitutional AI Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_core_aya_3_core_integrates_with",
          "source": "tool:copilot_core",
          "target": "model:aya_3_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-07_arxiv_5403",
              "url": "https://arxiv.org/abs/2026/01/07/copilot_core_aya_3_core",
              "published": "2026-01-07",
              "snippet": "The latest release of Copilot Core adds native Aya 3 Core integration..."
            },
            {
              "docId": "2026-01-07_hugging_face_bl_2211",
              "url": "https://huggingface.co/blog/2026/01/07/copilot_core_aya_3_core",
              "published": "2026-01-07",
              "snippet": "The latest release of Copilot Core adds native Aya 3 Core integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_ultra_streamlit_v2_integrates_with",
          "source": "tool:tensorrt_llm_ultra",
          "target": "tool:streamlit_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-22_langchain_blog_5520",
              "url": "https://blog.langchain.dev/2026/01/22/tensorrt_llm_ultra_streamlit_v",
              "published": "2026-01-22",
              "snippet": "TensorRT-LLM Ultra announced official support for Streamlit v2..."
            },
            {
              "docId": "2026-01-22_weights_and_bia_6964",
              "url": "https://wandb.ai/articles/2026/01/22/tensorrt_llm_ultra_streamlit_v",
              "published": "2026-01-22",
              "snippet": "TensorRT-LLM Ultra now supports Streamlit v2 with full feature parity..."
            },
            {
              "docId": "2026-01-24_venturebeat_6740",
              "url": "https://venturebeat.com/2026/01/24/tensorrt_llm_ultra_streamlit_v",
              "published": "2026-01-24",
              "snippet": "The latest release of TensorRT-LLM Ultra adds native Streamlit v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_core_fineweb_trained_on",
          "source": "model:midjourney_v7_core",
          "target": "dataset:fineweb",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.54
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_max_math_pro_evaluated_on",
          "source": "model:gpt_4o_mini_2_max",
          "target": "benchmark:math_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-11_weights_and_bia_8779",
              "url": "https://wandb.ai/articles/2026/01/11/gpt_4o_mini_2_max_math_pro",
              "published": "2026-01-11",
              "snippet": "On the MATH Pro benchmark, GPT-4o Mini 2 Max scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_constitutional_ai_max_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:constitutional_ai_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:winogrande_ultra_sora_2_mini_measures",
          "source": "benchmark:winogrande_ultra",
          "target": "model:sora_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-25_bloomberg_5198",
              "url": "https://bloomberg.com/technology/2026/01/25/winogrande_ultra_sora_2_mini",
              "published": "2026-01-25",
              "snippet": "WinoGrande Ultra has become the standard for evaluating Sora 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_max_codex_2_next_integrates_with",
          "source": "repo:text_generation_webui_max",
          "target": "model:codex_2_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-16_hugging_face_bl_3637",
              "url": "https://huggingface.co/blog/2026/01/16/text_generation_webui_max_code",
              "published": "2026-01-16",
              "snippet": "text-generation-webui Max now supports Codex 2 Next with full feature parity..."
            },
            {
              "docId": "2026-01-21_nextgov_2358",
              "url": "https://nextgov.com/2026/01/21/text_generation_webui_max_code",
              "published": "2026-01-21",
              "snippet": "text-generation-webui Max now supports Codex 2 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_v2_llama_4_measures",
          "source": "benchmark:winogrande_v2",
          "target": "model:llama_4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-25_nvidia_blog_5727",
              "url": "https://blogs.nvidia.com/2025/12/25/winogrande_v2_llama_4",
              "published": "2025-12-25",
              "snippet": "The WinoGrande v2 benchmark measures Llama 4 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_lite_sliding_window_attention_plus_uses_tech",
          "source": "dataset:laion_5b_lite",
          "target": "tech:sliding_window_attention_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.43
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_core_slimpajama_trained_on",
          "source": "model:dall_e_4_core",
          "target": "dataset:slimpajama",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:llamaindex_v2_claude_opus_45_max_integrates_with",
          "source": "tool:llamaindex_v2",
          "target": "model:claude_opus_45_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_wired_2911",
              "url": "https://wired.com/2026/01/25/llamaindex_v2_claude_opus_45_m",
              "published": "2026-01-25",
              "snippet": "The latest release of LlamaIndex v2 adds native Claude Opus 4.5 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mlflow_v2_weights_and_biases_next_integrates_with",
          "source": "tool:mlflow_v2",
          "target": "tool:weights_and_biases_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-06_openai_blog_8181",
              "url": "https://openai.com/blog/2026/01/06/mlflow_v2_weights_and_biases_n",
              "published": "2026-01-06",
              "snippet": "MLflow v2 now supports Weights & Biases Next with full feature parity..."
            },
            {
              "docId": "2026-01-21_nextgov_5212",
              "url": "https://nextgov.com/2026/01/21/mlflow_v2_weights_and_biases_n",
              "published": "2026-01-21",
              "snippet": "MLflow v2 announced official support for Weights & Biases Next..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_whisper_v4_integrates_with",
          "source": "tool:weights_and_biases",
          "target": "model:whisper_v4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-21_meta_ai_blog_9614",
              "url": "https://ai.meta.com/blog/2025/12/21/weights_and_biases_whisper_v4",
              "published": "2025-12-21",
              "snippet": "The latest release of Weights & Biases adds native Whisper v4 integration..."
            },
            {
              "docId": "2026-01-23_arxiv_2756",
              "url": "https://arxiv.org/abs/2026/01/23/weights_and_biases_whisper_v4",
              "published": "2026-01-23",
              "snippet": "The latest release of Weights & Biases adds native Whisper v4 integration..."
            },
            {
              "docId": "2026-01-24_nvidia_blog_7293",
              "url": "https://blogs.nvidia.com/2026/01/24/weights_and_biases_whisper_v4",
              "published": "2026-01-24",
              "snippet": "Weights & Biases now supports Whisper v4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:codex_2_plus_mc4_edge_trained_on",
          "source": "model:codex_2_plus",
          "target": "dataset:mc4_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-25_reuters_7806",
              "url": "https://reuters.com/technology/2026/01/25/codex_2_plus_mc4_edge",
              "published": "2026-01-25",
              "snippet": "The training corpus for Codex 2 Plus includes mC4 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_c4_max_trained_on",
          "source": "model:whisper_v4_core",
          "target": "dataset:c4_max",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_multimodal_fusion_lite_uses_tech",
          "source": "tool:semantic_kernel",
          "target": "tech:multimodal_fusion_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_quantization_v2_uses_tech",
          "source": "tool:llamaindex_pro",
          "target": "tech:quantization_v2",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-14_reuters_8822",
              "url": "https://reuters.com/technology/2026/01/14/llamaindex_pro_quantization_v2",
              "published": "2026-01-14",
              "snippet": "Technical details reveal LlamaIndex Pro relies heavily on Quantization v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_mini_copilot_max_integrates_with",
          "source": "tool:tensorrt_llm_mini",
          "target": "tool:copilot_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:mbpp_lite_deepseek_v3_next_measures",
          "source": "benchmark:mbpp_lite",
          "target": "model:deepseek_v3_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:mixtral_8x22b_max_rotary_position_embedding_v2_uses_tech",
          "source": "model:mixtral_8x22b_max",
          "target": "tech:rotary_position_embedding_v2",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.67
        }
      },
      {
        "data": {
          "id": "e:qwen_3_ultra_claude_opus_45_lite_depends_on",
          "source": "model:qwen_3_ultra",
          "target": "model:claude_opus_45_lite",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:llama_4_plus_refinedweb_next_trained_on",
          "source": "model:llama_4_plus",
          "target": "dataset:refinedweb_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_edge_distillation_edge_uses_tech",
          "source": "tool:tensorrt_llm_edge",
          "target": "tech:distillation_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-13_langchain_blog_3559",
              "url": "https://blog.langchain.dev/2026/01/13/tensorrt_llm_edge_distillation",
              "published": "2026-01-13",
              "snippet": "Under the hood, TensorRT-LLM Edge implements Distillation Edge for improved efficiency..."
            },
            {
              "docId": "2026-01-24_wired_8123",
              "url": "https://wired.com/2026/01/24/tensorrt_llm_edge_distillation",
              "published": "2026-01-24",
              "snippet": "Technical details reveal TensorRT-LLM Edge relies heavily on Distillation Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_core_dall_e_4_integrates_with",
          "source": "repo:vllm_core",
          "target": "model:dall_e_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-20_the_gradient_2607",
              "url": "https://thegradient.pub/2026/01/20/vllm_core_dall_e_4",
              "published": "2026-01-20",
              "snippet": "vllm Core now supports DALL-E 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_mini_tool_use_uses_tech",
          "source": "dataset:the_stack_v2_mini",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_edge_claude_sonnet_4_plus_integrates_with",
          "source": "tool:weights_and_biases_edge",
          "target": "model:claude_sonnet_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-14_nvidia_blog_6738",
              "url": "https://blogs.nvidia.com/2026/01/14/weights_and_biases_edge_claude",
              "published": "2026-01-14",
              "snippet": "Weights & Biases Edge announced official support for Claude Sonnet 4 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_ultra_copilot_integrates_with",
          "source": "tool:llamaindex_ultra",
          "target": "tool:copilot",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-20_the_verge_6538",
              "url": "https://theverge.com/2026/01/20/llamaindex_ultra_copilot",
              "published": "2026-01-20",
              "snippet": "The latest release of LlamaIndex Ultra adds native Copilot integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_max_dall_e_4_max_integrates_with",
          "source": "repo:langchain_max",
          "target": "model:dall_e_4_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-04_venturebeat_5604",
              "url": "https://venturebeat.com/2026/01/04/langchain_max_dall_e_4_max",
              "published": "2026-01-04",
              "snippet": "langchain Max now supports DALL-E 4 Max with full feature parity..."
            },
            {
              "docId": "2026-01-12_nvidia_blog_7970",
              "url": "https://blogs.nvidia.com/2026/01/12/langchain_max_dall_e_4_max",
              "published": "2026-01-12",
              "snippet": "The latest release of langchain Max adds native DALL-E 4 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_distillation_next_uses_tech",
          "source": "model:stable_diffusion_4_next",
          "target": "tech:distillation_next",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_next_sliding_window_attention_v2_uses_tech",
          "source": "model:nemotron_5_next",
          "target": "tech:sliding_window_attention_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:jamba_2_edge_slimpajama_mini_trained_on",
          "source": "model:jamba_2_edge",
          "target": "dataset:slimpajama_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:cody_streamlit_next_integrates_with",
          "source": "tool:cody",
          "target": "tool:streamlit_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2026-01-23_langchain_blog_2521",
              "url": "https://blog.langchain.dev/2026/01/23/cody_streamlit_next",
              "published": "2026-01-23",
              "snippet": "The latest release of Cody adds native Streamlit Next integration..."
            },
            {
              "docId": "2026-01-25_nextgov_8526",
              "url": "https://nextgov.com/2026/01/25/cody_streamlit_next",
              "published": "2026-01-25",
              "snippet": "The latest release of Cody adds native Streamlit Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_plus_llamaindex_plus_integrates_with",
          "source": "tool:cody_plus",
          "target": "tool:llamaindex_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_rotary_position_embedding_pro_uses_tech",
          "source": "repo:autogpt_plus",
          "target": "tech:rotary_position_embedding_pro",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-24_the_verge_6443",
              "url": "https://theverge.com/2026/01/24/autogpt_plus_rotary_position_e",
              "published": "2026-01-24",
              "snippet": "Under the hood, AutoGPT Plus implements Rotary Position Embedding Pro for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_lite_laion_5b_next_trained_on",
          "source": "model:palm_3_lite",
          "target": "dataset:laion_5b_next",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_gemma_3_max_integrates_with",
          "source": "tool:semantic_kernel",
          "target": "model:gemma_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:self_play_fine_tuning_for_language_model_gguf_mini_uses_tech",
          "source": "paper:self_play_fine_tuning_for_language_model",
          "target": "tech:gguf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-11-19_arxiv_1029",
              "url": "https://arxiv.org/abs/2025/11/19/self_play_fine_tuning_for_lang",
              "published": "2025-11-19",
              "snippet": "Self-Play Fine-Tuning for Language Models leverages GGUF Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_core_phi_4_pro_measures",
          "source": "benchmark:bigbench_hard_core",
          "target": "model:phi_4_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-11_anthropic_blog_1169",
              "url": "https://anthropic.com/news/2026/01/11/bigbench_hard_core_phi_4_pro",
              "published": "2026-01-11",
              "snippet": "BigBench Hard Core has become the standard for evaluating Phi-4 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_max_stable_diffusion_4_lite_integrates_with",
          "source": "tool:litellm_max",
          "target": "model:stable_diffusion_4_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:arc_agi_lite_sora_2_mini_measures",
          "source": "benchmark:arc_agi_lite",
          "target": "model:sora_2_mini",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:transformers_next_chain_of_thought_max_uses_tech",
          "source": "repo:transformers_next",
          "target": "tech:chain_of_thought_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-25_ars_technica_9498",
              "url": "https://arstechnica.com/2026/01/25/transformers_next_chain_of_tho",
              "published": "2026-01-25",
              "snippet": "transformers Next leverages Chain-of-Thought Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_rotary_position_embedding_lite_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:rotary_position_embedding_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-22_bloomberg_7309",
              "url": "https://bloomberg.com/technology/2026/01/22/gpt4all_pro_rotary_position_em",
              "published": "2026-01-22",
              "snippet": "gpt4all Pro leverages Rotary Position Embedding Lite to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_ars_technica_4800",
              "url": "https://arstechnica.com/2026/01/23/gpt4all_pro_rotary_position_em",
              "published": "2026-01-23",
              "snippet": "gpt4all Pro leverages Rotary Position Embedding Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_lite_retrieval_augmented_generation_core_uses_tech",
          "source": "repo:localai_lite",
          "target": "tech:retrieval_augmented_generation_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_qwen_3_pro_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "model:qwen_3_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-03_google_ai_blog_2212",
              "url": "https://blog.google/technology/ai/2026/01/03/attention_is_all_you_need_v2_u",
              "published": "2026-01-03",
              "snippet": "On the Qwen-3 Pro benchmark, Attention Is All You Need v2 Ultra scored 70%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:textbooks_are_all_you_need_quantization_mini_uses_tech",
          "source": "paper:textbooks_are_all_you_need",
          "target": "tech:quantization_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_1289",
              "url": "https://blog.langchain.dev/2026/01/17/textbooks_are_all_you_need_qua",
              "published": "2026-01-17",
              "snippet": "Technical details reveal Textbooks Are All You Need relies heavily on Quantization Mini..."
            },
            {
              "docId": "2026-01-21_arxiv_7175",
              "url": "https://arxiv.org/abs/2026/01/21/textbooks_are_all_you_need_qua",
              "published": "2026-01-21",
              "snippet": "Under the hood, Textbooks Are All You Need implements Quantization Mini for improved efficiency..."
            },
            {
              "docId": "2026-01-24_the_gradient_4363",
              "url": "https://thegradient.pub/2026/01/24/textbooks_are_all_you_need_qua",
              "published": "2026-01-24",
              "snippet": "Textbooks Are All You Need leverages Quantization Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_nvidia_blog_2437",
              "url": "https://blogs.nvidia.com/2026/01/25/textbooks_are_all_you_need_qua",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Textbooks Are All You Need relies heavily on Quantization Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_plus_gguf_core_uses_tech",
          "source": "repo:langchain_plus",
          "target": "tech:gguf_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:llamacpp_plus_nemotron_5_next_integrates_with",
          "source": "repo:llamacpp_plus",
          "target": "model:nemotron_5_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-08_ars_technica_4980",
              "url": "https://arstechnica.com/2025/12/08/llamacpp_plus_nemotron_5_next",
              "published": "2025-12-08",
              "snippet": "llama.cpp Plus now supports Nemotron-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-03_nvidia_blog_6568",
              "url": "https://blogs.nvidia.com/2026/01/03/llamacpp_plus_nemotron_5_next",
              "published": "2026-01-03",
              "snippet": "llama.cpp Plus announced official support for Nemotron-5 Next..."
            },
            {
              "docId": "2026-01-04_wired_6960",
              "url": "https://wired.com/2026/01/04/llamacpp_plus_nemotron_5_next",
              "published": "2026-01-04",
              "snippet": "llama.cpp Plus now supports Nemotron-5 Next with full feature parity..."
            },
            {
              "docId": "2026-01-23_mit_technology__3758",
              "url": "https://technologyreview.com/2026/01/23/llamacpp_plus_nemotron_5_next",
              "published": "2026-01-23",
              "snippet": "llama.cpp Plus now supports Nemotron-5 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_pro_qlora_pro_uses_tech",
          "source": "model:whisper_v4_pro",
          "target": "tech:qlora_pro",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:laion_5b_next_rlhf_mini_uses_tech",
          "source": "dataset:laion_5b_next",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:streamlit_edge_sliding_window_attention_lite_uses_tech",
          "source": "tool:streamlit_edge",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_openwebtext2_edge_trained_on",
          "source": "model:gemini_ultra_2",
          "target": "dataset:openwebtext2_edge",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-21_venturebeat_7036",
              "url": "https://venturebeat.com/2026/01/21/gemini_ultra_2_openwebtext2_ed",
              "published": "2026-01-21",
              "snippet": "Gemini Ultra 2 was trained on OpenWebText2 Edge comprising billions of tokens..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_next_jamba_2_edge_integrates_with",
          "source": "tool:flowise_next",
          "target": "model:jamba_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_deepseek_v3_lite_integrates_with",
          "source": "repo:ollama_edge",
          "target": "model:deepseek_v3_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-16_hugging_face_bl_3792",
              "url": "https://huggingface.co/blog/2025/12/16/ollama_edge_deepseek_v3_lite",
              "published": "2025-12-16",
              "snippet": "The latest release of ollama Edge adds native DeepSeek-V3 Lite integration..."
            },
            {
              "docId": "2026-01-10_venturebeat_6107",
              "url": "https://venturebeat.com/2026/01/10/ollama_edge_deepseek_v3_lite",
              "published": "2026-01-10",
              "snippet": "ollama Edge now supports DeepSeek-V3 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_core_llama_4_pro_depends_on",
          "source": "model:nemotron_5_core",
          "target": "model:llama_4_pro",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:hellaswag_mini_whisper_v4_measures",
          "source": "benchmark:hellaswag_mini",
          "target": "model:whisper_v4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-14_venturebeat_3505",
              "url": "https://venturebeat.com/2026/01/14/hellaswag_mini_whisper_v4",
              "published": "2026-01-14",
              "snippet": "HellaSwag Mini has become the standard for evaluating Whisper v4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_next_gpt_5_evaluated_on",
          "source": "paper:llm_agents:_a_survey_next",
          "target": "model:gpt_5",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-21_nextgov_8928",
              "url": "https://nextgov.com/2025/12/21/llm_agents:_a_survey_next_gpt_",
              "published": "2025-12-21",
              "snippet": "Evaluation results show LLM Agents: A Survey Next reaching 88% on GPT-5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_core_whisper_v4_mini_integrates_with",
          "source": "repo:open_interpreter_core",
          "target": "model:whisper_v4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-12_nextgov_8488",
              "url": "https://nextgov.com/2026/01/12/open_interpreter_core_whisper_",
              "published": "2026-01-12",
              "snippet": "open-interpreter Core announced official support for Whisper v4 Mini..."
            },
            {
              "docId": "2026-01-20_weights_and_bia_8168",
              "url": "https://wandb.ai/articles/2026/01/20/open_interpreter_core_whisper_",
              "published": "2026-01-20",
              "snippet": "The latest release of open-interpreter Core adds native Whisper v4 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_nemotron_5_v2_integrates_with",
          "source": "tool:flowise",
          "target": "model:nemotron_5_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-14_nextgov_7104",
              "url": "https://nextgov.com/2026/01/14/flowise_nemotron_5_v2",
              "published": "2026-01-14",
              "snippet": "The latest release of Flowise adds native Nemotron-5 v2 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_core_qlora_pro_uses_tech",
          "source": "dataset:openwebtext2_core",
          "target": "tech:qlora_pro",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.66
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_whisper_v4_ultra_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:whisper_v4_ultra",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-19_ars_technica_1660",
              "url": "https://arstechnica.com/2026/01/19/direct_preference_optimization",
              "published": "2026-01-19",
              "snippet": "Direct Preference Optimization achieves 92% on Whisper v4 Ultra, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flowise_v2_dpo_core_uses_tech",
          "source": "tool:flowise_v2",
          "target": "tech:dpo_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:lmsys_chatbot_arena_lite_falcon_3_plus_measures",
          "source": "benchmark:lmsys_chatbot_arena_lite",
          "target": "model:falcon_3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-10_the_verge_3667",
              "url": "https://theverge.com/2026/01/10/lmsys_chatbot_arena_lite_falco",
              "published": "2026-01-10",
              "snippet": "LMSYS Chatbot Arena Lite provides standardized evaluation of Falcon 3 Plus..."
            },
            {
              "docId": "2026-01-12_techcrunch_2607",
              "url": "https://techcrunch.com/2026/01/12/lmsys_chatbot_arena_lite_falco",
              "published": "2026-01-12",
              "snippet": "LMSYS Chatbot Arena Lite has become the standard for evaluating Falcon 3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_ultra_starcoder_data_pro_trained_on",
          "source": "model:grok_3_ultra",
          "target": "dataset:starcoder_data_pro",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.61,
          "evidence": [
            {
              "docId": "2026-01-19_langchain_blog_1619",
              "url": "https://blog.langchain.dev/2026/01/19/grok_3_ultra_starcoder_data_pr",
              "published": "2026-01-19",
              "snippet": "The training corpus for Grok-3 Ultra includes StarCoder Data Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mini_dall_e_4_mini_integrates_with",
          "source": "repo:ollama_mini",
          "target": "model:dall_e_4_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-22_langchain_blog_1693",
              "url": "https://blog.langchain.dev/2026/01/22/ollama_mini_dall_e_4_mini",
              "published": "2026-01-22",
              "snippet": "The latest release of ollama Mini adds native DALL-E 4 Mini integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_mini_gpt_5_core_integrates_with",
          "source": "tool:semantic_kernel_mini",
          "target": "model:gpt_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:mlflow_mini_ollama_lite_integrates_with",
          "source": "tool:mlflow_mini",
          "target": "tool:ollama_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_hellaswag_pro_evaluated_on",
          "source": "model:midjourney_v7",
          "target": "benchmark:hellaswag_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-22_mit_technology__7283",
              "url": "https://technologyreview.com/2025/12/22/midjourney_v7_hellaswag_pro",
              "published": "2025-12-22",
              "snippet": "Evaluation results show Midjourney V7 reaching 75% on HellaSwag Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_v2_gpqa_plus_evaluated_on",
          "source": "model:midjourney_v7_v2",
          "target": "benchmark:gpqa_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_quantization_uses_tech",
          "source": "dataset:redpajama_v2",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:llamacpp_phi_4_plus_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:phi_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-22_arxiv_5072",
              "url": "https://arxiv.org/abs/2025/12/22/llamacpp_phi_4_plus",
              "published": "2025-12-22",
              "snippet": "The latest release of llama.cpp adds native Phi-4 Plus integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_v2_sora_2_pro_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2_v2",
          "target": "model:sora_2_pro",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-21_the_verge_9847",
              "url": "https://theverge.com/2026/01/21/attention_is_all_you_need_v2_v",
              "published": "2026-01-21",
              "snippet": "Evaluation results show Attention Is All You Need v2 v2 reaching 83% on Sora 2 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_max_mt_bench_next_evaluated_on",
          "source": "model:sora_2_max",
          "target": "benchmark:mt_bench_next",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_distillation_core_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:distillation_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_weights_and_bia_2078",
              "url": "https://wandb.ai/articles/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Under the hood, Chain-of-Thought Prompting Elicits Reasoning implements Distillation Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:copilot_core_dpo_mini_uses_tech",
          "source": "tool:copilot_core",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.85
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_core_constitutional_ai_uses_tech",
          "source": "model:nemotron_5_core",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:mmlu_core_gemma_3_pro_measures",
          "source": "benchmark:mmlu_core",
          "target": "model:gemma_3_pro",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-24_arxiv_1895",
              "url": "https://arxiv.org/abs/2026/01/24/mmlu_core_gemma_3_pro",
              "published": "2026-01-24",
              "snippet": "MMLU Core has become the standard for evaluating Gemma 3 Pro..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_red_teaming_lite_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:red_teaming_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:winogrande_v2_mixtral_8x22b_measures",
          "source": "benchmark:winogrande_v2",
          "target": "model:mixtral_8x22b",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-18_techcrunch_5422",
              "url": "https://techcrunch.com/2026/01/18/winogrande_v2_mixtral_8x22b",
              "published": "2026-01-18",
              "snippet": "WinoGrande v2 has become the standard for evaluating Mixtral 8x22B..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_edge_flash_attention_edge_uses_tech",
          "source": "tool:ollama_edge",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2026-01-21_wired_1278",
              "url": "https://wired.com/2026/01/21/ollama_edge_flash_attention_ed",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Ollama Edge relies heavily on Flash Attention Edge..."
            },
            {
              "docId": "2026-01-22_techcrunch_8195",
              "url": "https://techcrunch.com/2026/01/22/ollama_edge_flash_attention_ed",
              "published": "2026-01-22",
              "snippet": "Technical details reveal Ollama Edge relies heavily on Flash Attention Edge..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_7379",
              "url": "https://huggingface.co/blog/2026/01/25/ollama_edge_flash_attention_ed",
              "published": "2026-01-25",
              "snippet": "Ollama Edge leverages Flash Attention Edge to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_ultra_deepseek_v3_plus_measures",
          "source": "benchmark:mmlu_ultra",
          "target": "model:deepseek_v3_plus",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-19_weights_and_bia_6819",
              "url": "https://wandb.ai/articles/2026/01/19/mmlu_ultra_deepseek_v3_plus",
              "published": "2026-01-19",
              "snippet": "MMLU Ultra has become the standard for evaluating DeepSeek-V3 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_v2_mixture_of_experts_max_uses_tech",
          "source": "repo:localai_v2",
          "target": "tech:mixture_of_experts_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-27_arxiv_9167",
              "url": "https://arxiv.org/abs/2025/12/27/localai_v2_mixture_of_experts_",
              "published": "2025-12-27",
              "snippet": "Under the hood, LocalAI v2 implements Mixture of Experts Max for improved efficiency..."
            },
            {
              "docId": "2026-01-11_reuters_9053",
              "url": "https://reuters.com/technology/2026/01/11/localai_v2_mixture_of_experts_",
              "published": "2026-01-11",
              "snippet": "Under the hood, LocalAI v2 implements Mixture of Experts Max for improved efficiency..."
            },
            {
              "docId": "2026-01-12_weights_and_bia_4285",
              "url": "https://wandb.ai/articles/2026/01/12/localai_v2_mixture_of_experts_",
              "published": "2026-01-12",
              "snippet": "Under the hood, LocalAI v2 implements Mixture of Experts Max for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:lora:_low_rank_adaptation_of_large_langu_gpt_4o_mini_2_v2_evaluated_on",
          "source": "paper:lora:_low_rank_adaptation_of_large_langu",
          "target": "model:gpt_4o_mini_2_v2",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2026-01-25_the_gradient_6585",
              "url": "https://thegradient.pub/2026/01/25/lora:_low_rank_adaptation_of_l",
              "published": "2026-01-25",
              "snippet": "LoRA: Low-Rank Adaptation of Large Language Models achieves 76% on GPT-4o Mini 2 v2, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_mini_ollama_plus_integrates_with",
          "source": "tool:cody_mini",
          "target": "tool:ollama_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_5285",
              "url": "https://techcrunch.com/2026/01/23/cody_mini_ollama_plus",
              "published": "2026-01-23",
              "snippet": "Cody Mini now supports Ollama Plus with full feature parity..."
            },
            {
              "docId": "2026-01-24_openai_blog_1894",
              "url": "https://openai.com/blog/2026/01/24/cody_mini_ollama_plus",
              "published": "2026-01-24",
              "snippet": "Cody Mini now supports Ollama Plus with full feature parity..."
            },
            {
              "docId": "2026-01-24_mit_technology__6446",
              "url": "https://technologyreview.com/2026/01/24/cody_mini_ollama_plus",
              "published": "2026-01-24",
              "snippet": "Cody Mini now supports Ollama Plus with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_v2_command_r_plus_edge_measures",
          "source": "benchmark:bigbench_hard_v2",
          "target": "model:command_r_plus_edge",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:qwen_3_chain_of_thought_uses_tech",
          "source": "model:qwen_3",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2026-01-25_techcrunch_3837",
              "url": "https://techcrunch.com/2026/01/25/qwen_3_chain_of_thought",
              "published": "2026-01-25",
              "snippet": "Under the hood, Qwen-3 implements Chain-of-Thought for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_v2_claude_opus_45_plus_evaluated_on",
          "source": "paper:direct_preference_optimization_v2",
          "target": "model:claude_opus_45_plus",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-06_reuters_7709",
              "url": "https://reuters.com/technology/2026/01/06/direct_preference_optimization",
              "published": "2026-01-06",
              "snippet": "Evaluation results show Direct Preference Optimization v2 reaching 81% on Claude Opus 4.5 Plus..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_plus_retrieval_augmented_generation_plus_uses_tech",
          "source": "model:stable_diffusion_4_plus",
          "target": "tech:retrieval_augmented_generation_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:qwen_3_ultra_redpajama_v2_core_trained_on",
          "source": "model:qwen_3_ultra",
          "target": "dataset:redpajama_v2_core",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:vllm_core_synthetic_data_generation_uses_tech",
          "source": "repo:vllm_core",
          "target": "tech:synthetic_data_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.7,
          "evidence": [
            {
              "docId": "2026-01-17_mit_technology__4380",
              "url": "https://technologyreview.com/2026/01/17/vllm_core_synthetic_data_gener",
              "published": "2026-01-17",
              "snippet": "Under the hood, vllm Core implements Synthetic Data Generation for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_v2_gguf_max_uses_tech",
          "source": "repo:ollama_v2",
          "target": "tech:gguf_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-30_venturebeat_4055",
              "url": "https://venturebeat.com/2025/12/30/ollama_v2_gguf_max",
              "published": "2025-12-30",
              "snippet": "ollama v2 leverages GGUF Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_plus_mt_bench_edge_evaluated_on",
          "source": "model:nemotron_5_plus",
          "target": "benchmark:mt_bench_edge",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-15_hugging_face_bl_3848",
              "url": "https://huggingface.co/blog/2026/01/15/nemotron_5_plus_mt_bench_edge",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Nemotron-5 Plus reaching 81% on MT-Bench Edge..."
            },
            {
              "docId": "2026-01-15_ars_technica_9492",
              "url": "https://arstechnica.com/2026/01/15/nemotron_5_plus_mt_bench_edge",
              "published": "2026-01-15",
              "snippet": "On the MT-Bench Edge benchmark, Nemotron-5 Plus scored 98%..."
            },
            {
              "docId": "2026-01-20_weights_and_bia_7475",
              "url": "https://wandb.ai/articles/2026/01/20/nemotron_5_plus_mt_bench_edge",
              "published": "2026-01-20",
              "snippet": "On the MT-Bench Edge benchmark, Nemotron-5 Plus scored 87%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_dolma_v2_trained_on",
          "source": "model:stable_diffusion_4_next",
          "target": "dataset:dolma_v2",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:langchain_v2_claude_sonnet_4_plus_integrates_with",
          "source": "tool:langchain_v2",
          "target": "model:claude_sonnet_4_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:langchain_lite_chain_of_thought_lite_uses_tech",
          "source": "tool:langchain_lite",
          "target": "tech:chain_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_ultra_falcon_3_next_integrates_with",
          "source": "tool:semantic_kernel_ultra",
          "target": "model:falcon_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-25_mit_technology__6515",
              "url": "https://technologyreview.com/2025/12/25/semantic_kernel_ultra_falcon_3",
              "published": "2025-12-25",
              "snippet": "Semantic Kernel Ultra now supports Falcon 3 Next with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_core_laion_5b_v2_trained_on",
          "source": "model:llama_4_core",
          "target": "dataset:laion_5b_v2",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-17_weights_and_bia_6328",
              "url": "https://wandb.ai/articles/2026/01/17/llama_4_core_laion_5b_v2",
              "published": "2026-01-17",
              "snippet": "Llama 4 Core was trained on LAION-5B v2 comprising billions of tokens..."
            },
            {
              "docId": "2026-01-18_reuters_4993",
              "url": "https://reuters.com/technology/2026/01/18/llama_4_core_laion_5b_v2",
              "published": "2026-01-18",
              "snippet": "The training corpus for Llama 4 Core includes LAION-5B v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__constitutional_ai_v2_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:constitutional_ai_v2",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_transformer_architecture_next_uses_tech",
          "source": "paper:llm_agents:_a_survey",
          "target": "tech:transformer_architecture_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-24_microsoft_resea_8942",
              "url": "https://microsoft.com/en-us/research/blog/2025/12/24/llm_agents:_a_survey_transform",
              "published": "2025-12-24",
              "snippet": "LLM Agents: A Survey leverages Transformer Architecture Next to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-29_reuters_3533",
              "url": "https://reuters.com/technology/2025/12/29/llm_agents:_a_survey_transform",
              "published": "2025-12-29",
              "snippet": "Technical details reveal LLM Agents: A Survey relies heavily on Transformer Architecture Next..."
            },
            {
              "docId": "2026-01-04_openai_blog_1611",
              "url": "https://openai.com/blog/2026/01/04/llm_agents:_a_survey_transform",
              "published": "2026-01-04",
              "snippet": "Under the hood, LLM Agents: A Survey implements Transformer Architecture Next for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_data_constrained_language_models_gemma_3_max_evaluated_on",
          "source": "paper:scaling_data_constrained_language_models",
          "target": "model:gemma_3_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-16_venturebeat_2353",
              "url": "https://venturebeat.com/2025/12/16/scaling_data_constrained_langu",
              "published": "2025-12-16",
              "snippet": "On the Gemma 3 Max benchmark, Scaling Data-Constrained Language Models scored 95%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cursor_mini_nemotron_5_next_integrates_with",
          "source": "tool:cursor_mini",
          "target": "model:nemotron_5_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-17_langchain_blog_9197",
              "url": "https://blog.langchain.dev/2026/01/17/cursor_mini_nemotron_5_next",
              "published": "2026-01-17",
              "snippet": "The latest release of Cursor Mini adds native Nemotron-5 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_ultra_tree_of_thought_lite_uses_tech",
          "source": "repo:ollama_ultra",
          "target": "tech:tree_of_thought_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-09_nvidia_blog_4967",
              "url": "https://blogs.nvidia.com/2026/01/09/ollama_ultra_tree_of_thought_l",
              "published": "2026-01-09",
              "snippet": "Under the hood, ollama Ultra implements Tree of Thought Lite for improved efficiency..."
            },
            {
              "docId": "2026-01-16_anthropic_blog_1838",
              "url": "https://anthropic.com/news/2026/01/16/ollama_ultra_tree_of_thought_l",
              "published": "2026-01-16",
              "snippet": "Under the hood, ollama Ultra implements Tree of Thought Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_jamba_2_v2_integrates_with",
          "source": "repo:autogpt",
          "target": "model:jamba_2_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-20_arxiv_7493",
              "url": "https://arxiv.org/abs/2026/01/20/autogpt_jamba_2_v2",
              "published": "2026-01-20",
              "snippet": "AutoGPT now supports Jamba 2 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_edge_humaneval_plus_evaluated_on",
          "source": "model:midjourney_v7_edge",
          "target": "benchmark:humaneval_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:cody_edge_ollama_integrates_with",
          "source": "tool:cody_edge",
          "target": "tool:ollama",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:localai_next_falcon_3_max_integrates_with",
          "source": "tool:localai_next",
          "target": "model:falcon_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-02_hugging_face_bl_9649",
              "url": "https://huggingface.co/blog/2026/01/02/localai_next_falcon_3_max",
              "published": "2026-01-02",
              "snippet": "The latest release of LocalAI Next adds native Falcon 3 Max integration..."
            },
            {
              "docId": "2026-01-10_microsoft_resea_7102",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/10/localai_next_falcon_3_max",
              "published": "2026-01-10",
              "snippet": "The latest release of LocalAI Next adds native Falcon 3 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:winogrande_edge_codex_2_mini_measures",
          "source": "benchmark:winogrande_edge",
          "target": "model:codex_2_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-24_mit_technology__3969",
              "url": "https://technologyreview.com/2026/01/24/winogrande_edge_codex_2_mini",
              "published": "2026-01-24",
              "snippet": "WinoGrande Edge has become the standard for evaluating Codex 2 Mini..."
            },
            {
              "docId": "2026-01-25_the_verge_1767",
              "url": "https://theverge.com/2026/01/25/winogrande_edge_codex_2_mini",
              "published": "2026-01-25",
              "snippet": "The WinoGrande Edge benchmark measures Codex 2 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-25_nextgov_5382",
              "url": "https://nextgov.com/2026/01/25/winogrande_edge_codex_2_mini",
              "published": "2026-01-25",
              "snippet": "The WinoGrande Edge benchmark measures Codex 2 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_5638",
              "url": "https://wandb.ai/articles/2026/01/25/winogrande_edge_codex_2_mini",
              "published": "2026-01-25",
              "snippet": "WinoGrande Edge provides standardized evaluation of Codex 2 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_ultra_group_query_attention_core_uses_tech",
          "source": "paper:direct_preference_optimization_ultra",
          "target": "tech:group_query_attention_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-03_hugging_face_bl_7522",
              "url": "https://huggingface.co/blog/2026/01/03/direct_preference_optimization",
              "published": "2026-01-03",
              "snippet": "Under the hood, Direct Preference Optimization Ultra implements Group Query Attention Core for improved efficiency..."
            },
            {
              "docId": "2026-01-04_venturebeat_8223",
              "url": "https://venturebeat.com/2026/01/04/direct_preference_optimization",
              "published": "2026-01-04",
              "snippet": "Technical details reveal Direct Preference Optimization Ultra relies heavily on Group Query Attention Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt4all_pro_kv_cache_optimization_mini_uses_tech",
          "source": "repo:gpt4all_pro",
          "target": "tech:kv_cache_optimization_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2025-12-02_google_ai_blog_1428",
              "url": "https://blog.google/technology/ai/2025/12/02/gpt4all_pro_kv_cache_optimizat",
              "published": "2025-12-02",
              "snippet": "gpt4all Pro leverages KV Cache Optimization Mini to achieve state-of-the-art performance..."
            },
            {
              "docId": "2025-12-25_techcrunch_1900",
              "url": "https://techcrunch.com/2025/12/25/gpt4all_pro_kv_cache_optimizat",
              "published": "2025-12-25",
              "snippet": "gpt4all Pro leverages KV Cache Optimization Mini to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_pro_yi_large_lite_integrates_with",
          "source": "tool:vllm_pro",
          "target": "model:yi_large_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-24_the_verge_6613",
              "url": "https://theverge.com/2026/01/24/vllm_pro_yi_large_lite",
              "published": "2026-01-24",
              "snippet": "The latest release of vLLM Pro adds native Yi-Large Lite integration..."
            },
            {
              "docId": "2026-01-25_mit_technology__3960",
              "url": "https://technologyreview.com/2026/01/25/vllm_pro_yi_large_lite",
              "published": "2026-01-25",
              "snippet": "vLLM Pro now supports Yi-Large Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_core_codex_2_pro_integrates_with",
          "source": "tool:localai_core",
          "target": "model:codex_2_pro",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-30_the_gradient_6334",
              "url": "https://thegradient.pub/2025/12/30/localai_core_codex_2_pro",
              "published": "2025-12-30",
              "snippet": "LocalAI Core now supports Codex 2 Pro with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:cody_flowise_edge_integrates_with",
          "source": "tool:cody",
          "target": "tool:flowise_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:autogpt_v2_quantization_lite_uses_tech",
          "source": "tool:autogpt_v2",
          "target": "tech:quantization_lite",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_qlora_plus_uses_tech",
          "source": "tool:semantic_kernel_v2",
          "target": "tech:qlora_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:llamaindex_core_gemma_3_edge_integrates_with",
          "source": "tool:llamaindex_core",
          "target": "model:gemma_3_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-23_wired_4613",
              "url": "https://wired.com/2026/01/23/llamaindex_core_gemma_3_edge",
              "published": "2026-01-23",
              "snippet": "LlamaIndex Core announced official support for Gemma 3 Edge..."
            },
            {
              "docId": "2026-01-25_bloomberg_2961",
              "url": "https://bloomberg.com/technology/2026/01/25/llamaindex_core_gemma_3_edge",
              "published": "2026-01-25",
              "snippet": "LlamaIndex Core announced official support for Gemma 3 Edge..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:bigbench_hard_v2_claude_opus_45_v2_measures",
          "source": "benchmark:bigbench_hard_v2",
          "target": "model:claude_opus_45_v2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:qwen_3_v2_common_crawl_trained_on",
          "source": "model:qwen_3_v2",
          "target": "dataset:common_crawl",
          "rel": "TRAINED_ON",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-26_techcrunch_4228",
              "url": "https://techcrunch.com/2025/12/26/qwen_3_v2_common_crawl",
              "published": "2025-12-26",
              "snippet": "Qwen-3 v2 was trained on Common Crawl comprising billions of tokens..."
            },
            {
              "docId": "2026-01-22_techcrunch_7448",
              "url": "https://techcrunch.com/2026/01/22/qwen_3_v2_common_crawl",
              "published": "2026-01-22",
              "snippet": "The training corpus for Qwen-3 v2 includes Common Crawl..."
            },
            {
              "docId": "2026-01-22_reuters_3921",
              "url": "https://reuters.com/technology/2026/01/22/qwen_3_v2_common_crawl",
              "published": "2026-01-22",
              "snippet": "Qwen-3 v2 utilized Common Crawl as part of its pre-training data mix..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mc4_edge_sliding_window_attention_max_uses_tech",
          "source": "dataset:mc4_edge",
          "target": "tech:sliding_window_attention_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:mt_bench_edge_claude_sonnet_4_mini_measures",
          "source": "benchmark:mt_bench_edge",
          "target": "model:claude_sonnet_4_mini",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-12_openai_blog_2511",
              "url": "https://openai.com/blog/2026/01/12/mt_bench_edge_claude_sonnet_4_",
              "published": "2026-01-12",
              "snippet": "The MT-Bench Edge benchmark measures Claude Sonnet 4 Mini across multiple tasks..."
            },
            {
              "docId": "2026-01-15_nextgov_2336",
              "url": "https://nextgov.com/2026/01/15/mt_bench_edge_claude_sonnet_4_",
              "published": "2026-01-15",
              "snippet": "MT-Bench Edge provides standardized evaluation of Claude Sonnet 4 Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_v2_aya_3_next_integrates_with",
          "source": "tool:semantic_kernel_v2",
          "target": "model:aya_3_next",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-25_venturebeat_1692",
              "url": "https://venturebeat.com/2025/12/25/semantic_kernel_v2_aya_3_next",
              "published": "2025-12-25",
              "snippet": "The latest release of Semantic Kernel v2 adds native Aya 3 Next integration..."
            },
            {
              "docId": "2026-01-03_hugging_face_bl_4742",
              "url": "https://huggingface.co/blog/2026/01/03/semantic_kernel_v2_aya_3_next",
              "published": "2026-01-03",
              "snippet": "The latest release of Semantic Kernel v2 adds native Aya 3 Next integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:openwebtext2_lite_quantization_uses_tech",
          "source": "dataset:openwebtext2_lite",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_lite_mixture_of_experts_lite_uses_tech",
          "source": "model:gpt_4o_mini_2_lite",
          "target": "tech:mixture_of_experts_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-19_bloomberg_4942",
              "url": "https://bloomberg.com/technology/2026/01/19/gpt_4o_mini_2_lite_mixture_of_",
              "published": "2026-01-19",
              "snippet": "GPT-4o Mini 2 Lite leverages Mixture of Experts Lite to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_plus_codex_2_next_measures",
          "source": "benchmark:mmlu_plus",
          "target": "model:codex_2_next",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.91
        }
      },
      {
        "data": {
          "id": "e:transformers_pro_command_r_plus_edge_integrates_with",
          "source": "repo:transformers_pro",
          "target": "model:command_r_plus_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-25_reuters_4893",
              "url": "https://reuters.com/technology/2026/01/25/transformers_pro_command_r_plu",
              "published": "2026-01-25",
              "snippet": "The latest release of transformers Pro adds native Command R+ Edge integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_core_qwen_3_mini_integrates_with",
          "source": "repo:ollama_core",
          "target": "model:qwen_3_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:langchain_pro_gradio_edge_integrates_with",
          "source": "tool:langchain_pro",
          "target": "tool:gradio_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:open_interpreter_pro_gpt_4o_mini_2_max_integrates_with",
          "source": "repo:open_interpreter_pro",
          "target": "model:gpt_4o_mini_2_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-11-30_techcrunch_6652",
              "url": "https://techcrunch.com/2025/11/30/open_interpreter_pro_gpt_4o_mi",
              "published": "2025-11-30",
              "snippet": "open-interpreter Pro now supports GPT-4o Mini 2 Max with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_tool_use_lite_uses_tech",
          "source": "repo:ollama",
          "target": "tech:tool_use_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-16_nvidia_blog_1940",
              "url": "https://blogs.nvidia.com/2026/01/16/ollama_tool_use_lite",
              "published": "2026-01-16",
              "snippet": "Under the hood, ollama implements Tool Use Lite for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_core_lora_max_uses_tech",
          "source": "dataset:redpajama_v2_core",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:phi_4_lite_the_stack_v2_mini_trained_on",
          "source": "model:phi_4_lite",
          "target": "dataset:the_stack_v2_mini",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:litellm_max_sliding_window_attention_lite_uses_tech",
          "source": "tool:litellm_max",
          "target": "tech:sliding_window_attention_lite",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-10_the_verge_1669",
              "url": "https://theverge.com/2026/01/10/litellm_max_sliding_window_att",
              "published": "2026-01-10",
              "snippet": "Technical details reveal LiteLLM Max relies heavily on Sliding Window Attention Lite..."
            },
            {
              "docId": "2026-01-20_the_gradient_6105",
              "url": "https://thegradient.pub/2026/01/20/litellm_max_sliding_window_att",
              "published": "2026-01-20",
              "snippet": "Technical details reveal LiteLLM Max relies heavily on Sliding Window Attention Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_deepseek_v3_max_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:deepseek_v3_max",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-24_wired_6522",
              "url": "https://wired.com/2026/01/24/llm_agents:_a_survey_deepseek_",
              "published": "2026-01-24",
              "snippet": "Evaluation results show LLM Agents: A Survey reaching 79% on DeepSeek-V3 Max..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_next_gpt_5_ultra_measures",
          "source": "benchmark:hellaswag_next",
          "target": "model:gpt_5_ultra",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-29_the_gradient_1305",
              "url": "https://thegradient.pub/2025/12/29/hellaswag_next_gpt_5_ultra",
              "published": "2025-12-29",
              "snippet": "HellaSwag Next provides standardized evaluation of GPT-5 Ultra..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gsm8k_ultra_palm_3_lite_measures",
          "source": "benchmark:gsm8k_ultra",
          "target": "model:palm_3_lite",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-22_hugging_face_bl_9693",
              "url": "https://huggingface.co/blog/2026/01/22/gsm8k_ultra_palm_3_lite",
              "published": "2026-01-22",
              "snippet": "The GSM8K Ultra benchmark measures PaLM 3 Lite across multiple tasks..."
            },
            {
              "docId": "2026-01-23_google_ai_blog_2013",
              "url": "https://blog.google/technology/ai/2026/01/23/gsm8k_ultra_palm_3_lite",
              "published": "2026-01-23",
              "snippet": "GSM8K Ultra provides standardized evaluation of PaLM 3 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_gpt_4o_mini_2_lite_integrates_with",
          "source": "tool:vllm",
          "target": "model:gpt_4o_mini_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_6278",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/vllm_gpt_4o_mini_2_lite",
              "published": "2026-01-18",
              "snippet": "vLLM announced official support for GPT-4o Mini 2 Lite..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_max_qlora_edge_uses_tech",
          "source": "model:claude_opus_45_max",
          "target": "tech:qlora_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:llm_agents:_a_survey_yi_large_mini_evaluated_on",
          "source": "paper:llm_agents:_a_survey",
          "target": "model:yi_large_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-11_venturebeat_6287",
              "url": "https://venturebeat.com/2026/01/11/llm_agents:_a_survey_yi_large_",
              "published": "2026-01-11",
              "snippet": "LLM Agents: A Survey achieves 98% on Yi-Large Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:localai_plus_grok_3_max_integrates_with",
          "source": "repo:localai_plus",
          "target": "model:grok_3_max",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-22_meta_ai_blog_1855",
              "url": "https://ai.meta.com/blog/2026/01/22/localai_plus_grok_3_max",
              "published": "2026-01-22",
              "snippet": "The latest release of LocalAI Plus adds native Grok-3 Max integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_whisper_v4_core_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "model:whisper_v4_core",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_ultra_rotary_position_embedding_ultra_uses_tech",
          "source": "paper:attention_is_all_you_need_v2_ultra",
          "target": "tech:rotary_position_embedding_ultra",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:streamlit_v2_streamlit_mini_integrates_with",
          "source": "tool:streamlit_v2",
          "target": "tool:streamlit_mini",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2026-01-23_techcrunch_1957",
              "url": "https://techcrunch.com/2026/01/23/streamlit_v2_streamlit_mini",
              "published": "2026-01-23",
              "snippet": "Streamlit v2 announced official support for Streamlit Mini..."
            },
            {
              "docId": "2026-01-23_openai_blog_9333",
              "url": "https://openai.com/blog/2026/01/23/streamlit_v2_streamlit_mini",
              "published": "2026-01-23",
              "snippet": "Streamlit v2 announced official support for Streamlit Mini..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_pro_kv_cache_optimization_core_uses_tech",
          "source": "tool:llamaindex_pro",
          "target": "tech:kv_cache_optimization_core",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:streamlit_next_dpo_mini_uses_tech",
          "source": "tool:streamlit_next",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_v2_lora_core_uses_tech",
          "source": "model:gemini_ultra_2_v2",
          "target": "tech:lora_core",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:vllm_max_lora_next_uses_tech",
          "source": "repo:vllm_max",
          "target": "tech:lora_next",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-25_anthropic_blog_4639",
              "url": "https://anthropic.com/news/2026/01/25/vllm_max_lora_next",
              "published": "2026-01-25",
              "snippet": "vllm Max leverages LoRA Next to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_1271",
              "url": "https://ai.meta.com/blog/2026/01/25/vllm_max_lora_next",
              "published": "2026-01-25",
              "snippet": "vllm Max leverages LoRA Next to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_next_dpo_plus_uses_tech",
          "source": "model:qwen_3_next",
          "target": "tech:dpo_plus",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:yi_large_max_stable_diffusion_4_plus_depends_on",
          "source": "model:yi_large_max",
          "target": "model:stable_diffusion_4_plus",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_constitutional_ai_mini_uses_tech",
          "source": "repo:text_generation_webui",
          "target": "tech:constitutional_ai_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.69
        }
      },
      {
        "data": {
          "id": "e:vllm_max_flash_attention_edge_uses_tech",
          "source": "tool:vllm_max",
          "target": "tech:flash_attention_edge",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_multimodal_fusion_edge_uses_tech",
          "source": "model:claude_sonnet_4",
          "target": "tech:multimodal_fusion_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_core_lora_max_uses_tech",
          "source": "model:claude_sonnet_4_core",
          "target": "tech:lora_max",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-13_techcrunch_5989",
              "url": "https://techcrunch.com/2026/01/13/claude_sonnet_4_core_lora_max",
              "published": "2026-01-13",
              "snippet": "Claude Sonnet 4 Core leverages LoRA Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-19_nvidia_blog_4558",
              "url": "https://blogs.nvidia.com/2026/01/19/claude_sonnet_4_core_lora_max",
              "published": "2026-01-19",
              "snippet": "Claude Sonnet 4 Core leverages LoRA Max to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-22_anthropic_blog_6326",
              "url": "https://anthropic.com/news/2026/01/22/claude_sonnet_4_core_lora_max",
              "published": "2026-01-22",
              "snippet": "Claude Sonnet 4 Core leverages LoRA Max to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:autogpt_plus_quantization_edge_uses_tech",
          "source": "repo:autogpt_plus",
          "target": "tech:quantization_edge",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_next_sora_2_edge_depends_on",
          "source": "model:stable_diffusion_4_next",
          "target": "model:sora_2_edge",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_rlhf_mini_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:rlhf_mini",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2025-12-23_weights_and_bia_2804",
              "url": "https://wandb.ai/articles/2025/12/23/toolformer:_language_models_ca",
              "published": "2025-12-23",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements RLHF Mini for improved efficiency..."
            },
            {
              "docId": "2025-12-24_reuters_3085",
              "url": "https://reuters.com/technology/2025/12/24/toolformer:_language_models_ca",
              "published": "2025-12-24",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements RLHF Mini for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:palm_3_core_gemma_3_mini_depends_on",
          "source": "model:palm_3_core",
          "target": "model:gemma_3_mini",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.51
        }
      },
      {
        "data": {
          "id": "e:sora_2_plus_truthfulqa_mini_evaluated_on",
          "source": "model:sora_2_plus",
          "target": "benchmark:truthfulqa_mini",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.98,
          "evidence": [
            {
              "docId": "2026-01-18_ars_technica_8875",
              "url": "https://arstechnica.com/2026/01/18/sora_2_plus_truthfulqa_mini",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Sora 2 Plus reaching 87% on TruthfulQA Mini..."
            },
            {
              "docId": "2026-01-23_arxiv_7738",
              "url": "https://arxiv.org/abs/2026/01/23/sora_2_plus_truthfulqa_mini",
              "published": "2026-01-23",
              "snippet": "Sora 2 Plus achieves 92% on TruthfulQA Mini, setting a new record..."
            },
            {
              "docId": "2026-01-23_anthropic_blog_9652",
              "url": "https://anthropic.com/news/2026/01/23/sora_2_plus_truthfulqa_mini",
              "published": "2026-01-23",
              "snippet": "Sora 2 Plus achieves 72% on TruthfulQA Mini, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:text_generation_webui_next_dpo_mini_uses_tech",
          "source": "repo:text_generation_webui_next",
          "target": "tech:dpo_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:copilot_max_gemini_ultra_2_edge_integrates_with",
          "source": "tool:copilot_max",
          "target": "model:gemini_ultra_2_edge",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_core_mc4_trained_on",
          "source": "model:whisper_v4_core",
          "target": "dataset:mc4",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:sora_2_edge_grok_3_ultra_depends_on",
          "source": "model:sora_2_edge",
          "target": "model:grok_3_ultra",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_core_tokenizer_bpe_core_uses_tech",
          "source": "model:claude_opus_45_core",
          "target": "tech:tokenizer_bpe_core",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.56,
          "evidence": [
            {
              "docId": "2026-01-17_the_gradient_4352",
              "url": "https://thegradient.pub/2026/01/17/claude_opus_45_core_tokenizer_",
              "published": "2026-01-17",
              "snippet": "Claude Opus 4.5 Core leverages Tokenizer BPE Core to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_6755",
              "url": "https://blog.google/technology/ai/2026/01/24/claude_opus_45_core_tokenizer_",
              "published": "2026-01-24",
              "snippet": "Under the hood, Claude Opus 4.5 Core implements Tokenizer BPE Core for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dolma_core_gguf_uses_tech",
          "source": "dataset:dolma_core",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_synthetic_data_generation_mini_uses_tech",
          "source": "dataset:the_stack_v2",
          "target": "tech:synthetic_data_generation_mini",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:autogpt_ultra_weights_and_biases_v2_integrates_with",
          "source": "tool:autogpt_ultra",
          "target": "tool:weights_and_biases_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.57,
          "evidence": [
            {
              "docId": "2026-01-20_google_ai_blog_3519",
              "url": "https://blog.google/technology/ai/2026/01/20/autogpt_ultra_weights_and_bias",
              "published": "2026-01-20",
              "snippet": "The latest release of AutoGPT Ultra adds native Weights & Biases v2 integration..."
            },
            {
              "docId": "2026-01-22_ars_technica_3867",
              "url": "https://arstechnica.com/2026/01/22/autogpt_ultra_weights_and_bias",
              "published": "2026-01-22",
              "snippet": "The latest release of AutoGPT Ultra adds native Weights & Biases v2 integration..."
            }
          ]
        }
      }
    ]
  }
}
