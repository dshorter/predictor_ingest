{
  "meta": {
    "view": "dependencies",
    "nodeCount": 93,
    "edgeCount": 203,
    "exportedAt": "2026-01-25T12:00:00Z",
    "dateRange": {
      "start": "2025-10-25",
      "end": "2026-01-25"
    }
  },
  "elements": {
    "nodes": [
      {
        "data": {
          "id": "dataset:starcoder_data",
          "label": "StarCoder Data",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 15,
          "mentionCount30d": 34,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "tech:qlora",
          "label": "QLoRA",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-10-31",
          "lastSeen": "2025-11-15",
          "mentionCount7d": 10,
          "mentionCount30d": 10,
          "velocity": 0.5
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45",
          "label": "Claude Opus 4.5",
          "type": "Model",
          "aliases": [
            "Opus 4.5",
            "Claude Opus"
          ],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 9,
          "mentionCount30d": 84,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "model:midjourney_v7",
          "label": "Midjourney V7",
          "type": "Model",
          "aliases": [
            "MJ V7"
          ],
          "firstSeen": "2025-11-01",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 11,
          "mentionCount30d": 16,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "model:qwen_3",
          "label": "Qwen-3",
          "type": "Model",
          "aliases": [
            "Qwen3",
            "Qwen 3"
          ],
          "firstSeen": "2025-12-22",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 9,
          "mentionCount30d": 76,
          "velocity": 0.32
        }
      },
      {
        "data": {
          "id": "tool:localai",
          "label": "LocalAI",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-03",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 12,
          "mentionCount30d": 38,
          "velocity": 0.97
        }
      },
      {
        "data": {
          "id": "tech:tool_use",
          "label": "Tool Use",
          "type": "Tech",
          "aliases": [
            "Function Calling"
          ],
          "firstSeen": "2025-11-19",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 13,
          "mentionCount30d": 38,
          "velocity": 0.8
        }
      },
      {
        "data": {
          "id": "tech:group_query_attention",
          "label": "Group Query Attention",
          "type": "Tech",
          "aliases": [
            "GQA"
          ],
          "firstSeen": "2025-10-29",
          "lastSeen": "2025-11-24",
          "mentionCount7d": 17,
          "mentionCount30d": 27,
          "velocity": 0.61
        }
      },
      {
        "data": {
          "id": "tech:lora",
          "label": "LoRA",
          "type": "Tech",
          "aliases": [
            "Low-Rank Adaptation"
          ],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 15,
          "mentionCount30d": 48,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "model:aya_3",
          "label": "Aya 3",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-20",
          "mentionCount7d": 29,
          "mentionCount30d": 106,
          "velocity": 0.13
        }
      },
      {
        "data": {
          "id": "paper:chain_of_thought_prompting_elicits_reaso",
          "label": "Chain-of-Thought Prompting Elicits Reasoning",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-24",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 3,
          "mentionCount30d": 13,
          "velocity": 0.4
        }
      },
      {
        "data": {
          "id": "tech:flash_attention",
          "label": "Flash Attention",
          "type": "Tech",
          "aliases": [
            "FlashAttention"
          ],
          "firstSeen": "2025-12-21",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 17,
          "mentionCount30d": 70,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "tool:streamlit",
          "label": "Streamlit",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 18,
          "mentionCount30d": 30,
          "velocity": 0.72
        }
      },
      {
        "data": {
          "id": "model:gpt_5_core",
          "label": "GPT-5 Core",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-10-25",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 5,
          "mentionCount30d": 12,
          "velocity": 0.35
        }
      },
      {
        "data": {
          "id": "benchmark:mt_bench",
          "label": "MT-Bench",
          "type": "Benchmark",
          "aliases": [
            "MTBench"
          ],
          "firstSeen": "2025-10-31",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 19,
          "mentionCount30d": 35,
          "velocity": 0.09
        }
      },
      {
        "data": {
          "id": "model:palm_3",
          "label": "PaLM 3",
          "type": "Model",
          "aliases": [
            "PaLM3"
          ],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-12-13",
          "mentionCount7d": 13,
          "mentionCount30d": 45,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "model:llama_4",
          "label": "Llama 4",
          "type": "Model",
          "aliases": [
            "LLaMA 4",
            "Llama-4"
          ],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 21,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "model:yi_large",
          "label": "Yi-Large",
          "type": "Model",
          "aliases": [
            "Yi Large"
          ],
          "firstSeen": "2025-11-21",
          "lastSeen": "2026-01-09",
          "mentionCount7d": 6,
          "mentionCount30d": 25,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "model:codex_2",
          "label": "Codex 2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 16,
          "mentionCount30d": 49,
          "velocity": 0.25
        }
      },
      {
        "data": {
          "id": "tool:tensorrt_llm",
          "label": "TensorRT-LLM",
          "type": "Tool",
          "aliases": [
            "TensorRT LLM"
          ],
          "firstSeen": "2025-12-13",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 2,
          "mentionCount30d": 3,
          "velocity": 0.58
        }
      },
      {
        "data": {
          "id": "paper:attention_is_all_you_need_v2",
          "label": "Attention Is All You Need v2",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2025-12-29",
          "mentionCount7d": 5,
          "mentionCount30d": 25,
          "velocity": 0.74
        }
      },
      {
        "data": {
          "id": "tool:crewai",
          "label": "CrewAI",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-26",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 2,
          "mentionCount30d": 6,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "benchmark:hellaswag",
          "label": "HellaSwag",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-27",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 10,
          "mentionCount30d": 20,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tool:haystack",
          "label": "Haystack",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-09",
          "lastSeen": "2026-01-16",
          "mentionCount7d": 4,
          "mentionCount30d": 6,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "tech:sliding_window_attention",
          "label": "Sliding Window Attention",
          "type": "Tech",
          "aliases": [
            "SWA"
          ],
          "firstSeen": "2025-12-10",
          "lastSeen": "2025-12-24",
          "mentionCount7d": 3,
          "mentionCount30d": 9,
          "velocity": 0.24
        }
      },
      {
        "data": {
          "id": "model:claude_sonnet_4",
          "label": "Claude Sonnet 4",
          "type": "Model",
          "aliases": [
            "Sonnet 4"
          ],
          "firstSeen": "2025-10-25",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 54,
          "mentionCount30d": 105,
          "velocity": 0.9
        }
      },
      {
        "data": {
          "id": "dataset:laion_5b",
          "label": "LAION-5B",
          "type": "Dataset",
          "aliases": [
            "LAION"
          ],
          "firstSeen": "2025-12-18",
          "lastSeen": "2025-12-24",
          "mentionCount7d": 0,
          "mentionCount30d": 4,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2",
          "label": "Gemini Ultra 2",
          "type": "Model",
          "aliases": [
            "Gemini 2",
            "Gemini Ultra"
          ],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-31",
          "mentionCount7d": 42,
          "mentionCount30d": 93,
          "velocity": 0.23
        }
      },
      {
        "data": {
          "id": "tech:tree_of_thought",
          "label": "Tree of Thought",
          "type": "Tech",
          "aliases": [
            "ToT"
          ],
          "firstSeen": "2025-11-24",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 14,
          "mentionCount30d": 54,
          "velocity": 0.77
        }
      },
      {
        "data": {
          "id": "model:nemotron_5",
          "label": "Nemotron-5",
          "type": "Model",
          "aliases": [
            "Nemotron 5"
          ],
          "firstSeen": "2025-11-13",
          "lastSeen": "2025-11-27",
          "mentionCount7d": 13,
          "mentionCount30d": 95,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "tech:kv_cache_optimization",
          "label": "KV Cache Optimization",
          "type": "Tech",
          "aliases": [
            "KV Cache"
          ],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-21",
          "mentionCount7d": 4,
          "mentionCount30d": 12,
          "velocity": 0.85
        }
      },
      {
        "data": {
          "id": "tool:llamaindex",
          "label": "LlamaIndex",
          "type": "Tool",
          "aliases": [
            "Llama Index"
          ],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "benchmark:mmlu",
          "label": "MMLU",
          "type": "Benchmark",
          "aliases": [
            "Massive Multitask Language Understanding"
          ],
          "firstSeen": "2026-01-03",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "model:gpt_5",
          "label": "GPT-5",
          "type": "Model",
          "aliases": [
            "GPT 5",
            "gpt-5"
          ],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 8,
          "mentionCount30d": 12,
          "velocity": 0.98
        }
      },
      {
        "data": {
          "id": "paper:scaling_laws_for_neural_language_models_",
          "label": "Scaling Laws for Neural Language Models (2025)",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-20",
          "lastSeen": "2026-01-21",
          "mentionCount7d": 5,
          "mentionCount30d": 10,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "model:sora_2",
          "label": "Sora 2",
          "type": "Model",
          "aliases": [
            "Sora V2"
          ],
          "firstSeen": "2025-10-26",
          "lastSeen": "2025-12-02",
          "mentionCount7d": 24,
          "mentionCount30d": 99,
          "velocity": 0.81
        }
      },
      {
        "data": {
          "id": "dataset:the_pile",
          "label": "The Pile",
          "type": "Dataset",
          "aliases": [
            "Pile"
          ],
          "firstSeen": "2026-01-01",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 5,
          "mentionCount30d": 44,
          "velocity": 0.2
        }
      },
      {
        "data": {
          "id": "repo:llamacpp",
          "label": "llama.cpp",
          "type": "Repo",
          "aliases": [],
          "firstSeen": "2026-01-02",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 3,
          "mentionCount30d": 7,
          "velocity": 0.18
        }
      },
      {
        "data": {
          "id": "dataset:redpajama_v2",
          "label": "RedPajama v2",
          "type": "Dataset",
          "aliases": [
            "RedPajama"
          ],
          "firstSeen": "2026-01-15",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 3,
          "mentionCount30d": 4,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "paper:direct_preference_optimization",
          "label": "Direct Preference Optimization",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-13",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 4,
          "mentionCount30d": 46,
          "velocity": 0.63
        }
      },
      {
        "data": {
          "id": "model:deepseek_v3",
          "label": "DeepSeek-V3",
          "type": "Model",
          "aliases": [
            "DeepSeek V3"
          ],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-23",
          "mentionCount7d": 8,
          "mentionCount30d": 12,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "tech:rlhf",
          "label": "RLHF",
          "type": "Tech",
          "aliases": [
            "Reinforcement Learning from Human Feedback"
          ],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 24,
          "mentionCount30d": 41,
          "velocity": 0.46
        }
      },
      {
        "data": {
          "id": "dataset:dolma",
          "label": "Dolma",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-17",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 15,
          "mentionCount30d": 40,
          "velocity": 0.41
        }
      },
      {
        "data": {
          "id": "dataset:the_stack_v2",
          "label": "The Stack v2",
          "type": "Dataset",
          "aliases": [
            "The Stack"
          ],
          "firstSeen": "2026-01-17",
          "lastSeen": "2026-01-18",
          "mentionCount7d": 14,
          "mentionCount30d": 20,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "tool:semantic_kernel",
          "label": "Semantic Kernel",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-14",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 5,
          "mentionCount30d": 13,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "repo:vllm",
          "label": "vllm",
          "type": "Repo",
          "aliases": [
            "vllm-project/vllm"
          ],
          "firstSeen": "2025-12-06",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 12,
          "mentionCount30d": 30,
          "velocity": 0.84
        }
      },
      {
        "data": {
          "id": "model:mixtral_8x22b",
          "label": "Mixtral 8x22B",
          "type": "Model",
          "aliases": [
            "Mixtral-8x22B"
          ],
          "firstSeen": "2025-11-29",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 36,
          "mentionCount30d": 41,
          "velocity": 1.0
        }
      },
      {
        "data": {
          "id": "dataset:slimpajama",
          "label": "SlimPajama",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-11-11",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 5,
          "mentionCount30d": 16,
          "velocity": 0.47
        }
      },
      {
        "data": {
          "id": "model:gemma_3",
          "label": "Gemma 3",
          "type": "Model",
          "aliases": [
            "Gemma3"
          ],
          "firstSeen": "2025-12-01",
          "lastSeen": "2025-12-23",
          "mentionCount7d": 42,
          "mentionCount30d": 103,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "model:grok_3",
          "label": "Grok-3",
          "type": "Model",
          "aliases": [
            "Grok 3"
          ],
          "firstSeen": "2025-10-30",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 27,
          "mentionCount30d": 79,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "tech:rotary_position_embedding",
          "label": "Rotary Position Embedding",
          "type": "Tech",
          "aliases": [
            "RoPE"
          ],
          "firstSeen": "2025-12-08",
          "lastSeen": "2025-12-12",
          "mentionCount7d": 4,
          "mentionCount30d": 22,
          "velocity": 0.52
        }
      },
      {
        "data": {
          "id": "paper:toolformer:_language_models_can_teach_th",
          "label": "Toolformer: Language Models Can Teach Themselves to Use Tools",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-24",
          "mentionCount7d": 5,
          "mentionCount30d": 16,
          "velocity": 0.98
        }
      },
      {
        "data": {
          "id": "benchmark:gpqa",
          "label": "GPQA",
          "type": "Benchmark",
          "aliases": [
            "Graduate-Level Google-Proof QA"
          ],
          "firstSeen": "2026-01-15",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 14,
          "mentionCount30d": 43,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "model:jamba_2",
          "label": "Jamba 2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-12-10",
          "mentionCount7d": 30,
          "mentionCount30d": 62,
          "velocity": 0.11
        }
      },
      {
        "data": {
          "id": "benchmark:math",
          "label": "MATH",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-12-11",
          "lastSeen": "2026-01-06",
          "mentionCount7d": 7,
          "mentionCount30d": 25,
          "velocity": 0.66
        }
      },
      {
        "data": {
          "id": "tool:mlflow",
          "label": "MLflow",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-19",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 2,
          "mentionCount30d": 7,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "tech:quantization",
          "label": "Quantization",
          "type": "Tech",
          "aliases": [
            "GPTQ",
            "AWQ"
          ],
          "firstSeen": "2025-12-17",
          "lastSeen": "2026-01-11",
          "mentionCount7d": 8,
          "mentionCount30d": 42,
          "velocity": 0.37
        }
      },
      {
        "data": {
          "id": "repo:langchain",
          "label": "langchain",
          "type": "Repo",
          "aliases": [
            "langchain-ai/langchain"
          ],
          "firstSeen": "2025-11-08",
          "lastSeen": "2026-01-10",
          "mentionCount7d": 15,
          "mentionCount30d": 24,
          "velocity": 0.17
        }
      },
      {
        "data": {
          "id": "model:command_r_plus",
          "label": "Command R+",
          "type": "Model",
          "aliases": [
            "Command R Plus"
          ],
          "firstSeen": "2025-12-21",
          "lastSeen": "2025-12-28",
          "mentionCount7d": 22,
          "mentionCount30d": 78,
          "velocity": 0.53
        }
      },
      {
        "data": {
          "id": "tool:vllm",
          "label": "vLLM",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-17",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.21
        }
      },
      {
        "data": {
          "id": "tool:weights_and_biases",
          "label": "Weights & Biases",
          "type": "Tool",
          "aliases": [
            "W&B",
            "wandb"
          ],
          "firstSeen": "2025-11-06",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 11,
          "mentionCount30d": 29,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:claude_opus_45_v2",
          "label": "Claude Opus 4.5 v2",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-11-07",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 14,
          "mentionCount30d": 73,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "tool:gradio",
          "label": "Gradio",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 1,
          "mentionCount30d": 5,
          "velocity": 0.26
        }
      },
      {
        "data": {
          "id": "model:gpt_4o_mini_2",
          "label": "GPT-4o Mini 2",
          "type": "Model",
          "aliases": [
            "GPT4o Mini 2"
          ],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 13,
          "mentionCount30d": 71,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tech:dpo",
          "label": "DPO",
          "type": "Tech",
          "aliases": [
            "Direct Preference Optimization"
          ],
          "firstSeen": "2025-11-02",
          "lastSeen": "2025-12-26",
          "mentionCount7d": 16,
          "mentionCount30d": 34,
          "velocity": 0.45
        }
      },
      {
        "data": {
          "id": "tech:sparse_attention",
          "label": "Sparse Attention",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-11-06",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 13,
          "mentionCount30d": 59,
          "velocity": 0.36
        }
      },
      {
        "data": {
          "id": "model:dall_e_4",
          "label": "DALL-E 4",
          "type": "Model",
          "aliases": [
            "DALLE 4"
          ],
          "firstSeen": "2025-11-13",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 16,
          "mentionCount30d": 40,
          "velocity": 0.95
        }
      },
      {
        "data": {
          "id": "benchmark:humaneval",
          "label": "HumanEval",
          "type": "Benchmark",
          "aliases": [
            "Human Eval"
          ],
          "firstSeen": "2025-11-19",
          "lastSeen": "2025-12-10",
          "mentionCount7d": 0,
          "mentionCount30d": 3,
          "velocity": 0.3
        }
      },
      {
        "data": {
          "id": "tool:langchain",
          "label": "LangChain",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-12-13",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 7,
          "mentionCount30d": 18,
          "velocity": 0.89
        }
      },
      {
        "data": {
          "id": "tech:multimodal_fusion",
          "label": "Multimodal Fusion",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-18",
          "lastSeen": "2026-01-15",
          "mentionCount7d": 28,
          "mentionCount30d": 60,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "tool:litellm",
          "label": "LiteLLM",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2026-01-18",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 4,
          "mentionCount30d": 24,
          "velocity": 0.65
        }
      },
      {
        "data": {
          "id": "model:phi_4",
          "label": "Phi-4",
          "type": "Model",
          "aliases": [
            "Phi 4"
          ],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-13",
          "mentionCount7d": 8,
          "mentionCount30d": 40,
          "velocity": 0.51
        }
      },
      {
        "data": {
          "id": "benchmark:truthfulqa",
          "label": "TruthfulQA",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2026-01-16",
          "lastSeen": "2026-01-20",
          "mentionCount7d": 8,
          "mentionCount30d": 14,
          "velocity": 0.38
        }
      },
      {
        "data": {
          "id": "model:stable_diffusion_4",
          "label": "Stable Diffusion 4",
          "type": "Model",
          "aliases": [
            "SD4",
            "SD 4"
          ],
          "firstSeen": "2025-11-25",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 4,
          "mentionCount30d": 9,
          "velocity": 0.59
        }
      },
      {
        "data": {
          "id": "benchmark:gsm8k",
          "label": "GSM8K",
          "type": "Benchmark",
          "aliases": [],
          "firstSeen": "2025-11-03",
          "lastSeen": "2025-12-02",
          "mentionCount7d": 9,
          "mentionCount30d": 22,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "dataset:fineweb",
          "label": "FineWeb",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-17",
          "mentionCount7d": 8,
          "mentionCount30d": 21,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "tech:gguf",
          "label": "GGUF",
          "type": "Tech",
          "aliases": [
            "GGML"
          ],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-25",
          "mentionCount7d": 5,
          "mentionCount30d": 32,
          "velocity": 0.48
        }
      },
      {
        "data": {
          "id": "dataset:common_crawl",
          "label": "Common Crawl",
          "type": "Dataset",
          "aliases": [],
          "firstSeen": "2025-12-19",
          "lastSeen": "2026-01-22",
          "mentionCount7d": 3,
          "mentionCount30d": 27,
          "velocity": 0.44
        }
      },
      {
        "data": {
          "id": "model:whisper_v4",
          "label": "Whisper v4",
          "type": "Model",
          "aliases": [
            "Whisper V4"
          ],
          "firstSeen": "2025-10-28",
          "lastSeen": "2025-11-02",
          "mentionCount7d": 46,
          "mentionCount30d": 112,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "paper:flash_attention:_fast_and_memory_efficie",
          "label": "Flash Attention: Fast and Memory-Efficient Attention",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-11-20",
          "lastSeen": "2025-12-14",
          "mentionCount7d": 0,
          "mentionCount30d": 2,
          "velocity": 0.43
        }
      },
      {
        "data": {
          "id": "benchmark:arc_agi",
          "label": "ARC-AGI",
          "type": "Benchmark",
          "aliases": [
            "ARC AGI"
          ],
          "firstSeen": "2025-11-06",
          "lastSeen": "2026-01-07",
          "mentionCount7d": 14,
          "mentionCount30d": 39,
          "velocity": 0.79
        }
      },
      {
        "data": {
          "id": "model:falcon_3",
          "label": "Falcon 3",
          "type": "Model",
          "aliases": [
            "Falcon3"
          ],
          "firstSeen": "2025-12-24",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 12,
          "mentionCount30d": 16,
          "velocity": 0.08
        }
      },
      {
        "data": {
          "id": "tool:ollama",
          "label": "Ollama",
          "type": "Tool",
          "aliases": [],
          "firstSeen": "2025-11-28",
          "lastSeen": "2026-01-03",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.22
        }
      },
      {
        "data": {
          "id": "tech:speculative_decoding",
          "label": "Speculative Decoding",
          "type": "Tech",
          "aliases": [],
          "firstSeen": "2025-12-06",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 9,
          "mentionCount30d": 36,
          "velocity": 0.7
        }
      },
      {
        "data": {
          "id": "tech:transformer_architecture",
          "label": "Transformer Architecture",
          "type": "Tech",
          "aliases": [
            "Transformers"
          ],
          "firstSeen": "2025-11-12",
          "lastSeen": "2025-12-03",
          "mentionCount7d": 22,
          "mentionCount30d": 35,
          "velocity": 0.57
        }
      },
      {
        "data": {
          "id": "model:gemini_ultra_2_lite",
          "label": "Gemini Ultra 2 Lite",
          "type": "Model",
          "aliases": [],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 16,
          "mentionCount30d": 40,
          "velocity": 0.56
        }
      },
      {
        "data": {
          "id": "tech:chain_of_thought",
          "label": "Chain-of-Thought",
          "type": "Tech",
          "aliases": [
            "CoT"
          ],
          "firstSeen": "2026-01-04",
          "lastSeen": "2026-01-19",
          "mentionCount7d": 30,
          "mentionCount30d": 77,
          "velocity": 0.21
        }
      },
      {
        "data": {
          "id": "tech:constitutional_ai",
          "label": "Constitutional AI",
          "type": "Tech",
          "aliases": [
            "CAI"
          ],
          "firstSeen": "2025-12-17",
          "lastSeen": "2025-12-22",
          "mentionCount7d": 17,
          "mentionCount30d": 35,
          "velocity": 0.71
        }
      },
      {
        "data": {
          "id": "tech:mixture_of_experts",
          "label": "Mixture of Experts",
          "type": "Tech",
          "aliases": [
            "MoE"
          ],
          "firstSeen": "2025-11-25",
          "lastSeen": "2025-12-27",
          "mentionCount7d": 11,
          "mentionCount30d": 23,
          "velocity": 0.67
        }
      },
      {
        "data": {
          "id": "paper:constitutional_ai:_harmlessness_from_ai_",
          "label": "Constitutional AI: Harmlessness from AI Feedback",
          "type": "Paper",
          "aliases": [],
          "firstSeen": "2025-12-31",
          "lastSeen": "2026-01-05",
          "mentionCount7d": 4,
          "mentionCount30d": 16,
          "velocity": 0.14
        }
      },
      {
        "data": {
          "id": "tech:retrieval_augmented_generation",
          "label": "Retrieval-Augmented Generation",
          "type": "Tech",
          "aliases": [
            "RAG"
          ],
          "firstSeen": "2025-12-09",
          "lastSeen": "2026-01-01",
          "mentionCount7d": 17,
          "mentionCount30d": 30,
          "velocity": 0.68
        }
      },
      {
        "data": {
          "id": "tool:autogpt",
          "label": "AutoGPT",
          "type": "Tool",
          "aliases": [
            "Auto-GPT"
          ],
          "firstSeen": "2025-11-28",
          "lastSeen": "2025-12-05",
          "mentionCount7d": 1,
          "mentionCount30d": 6,
          "velocity": 0.39
        }
      },
      {
        "data": {
          "id": "repo:transformers",
          "label": "transformers",
          "type": "Repo",
          "aliases": [
            "huggingface/transformers"
          ],
          "firstSeen": "2025-12-02",
          "lastSeen": "2026-01-02",
          "mentionCount7d": 1,
          "mentionCount30d": 4,
          "velocity": 0.12
        }
      }
    ],
    "edges": [
      {
        "data": {
          "id": "e:grok_3_gpt_5_core_depends_on",
          "source": "model:grok_3",
          "target": "model:gpt_5_core",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.57
        }
      },
      {
        "data": {
          "id": "e:sora_2_sliding_window_attention_uses_tech",
          "source": "model:sora_2",
          "target": "tech:sliding_window_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-13_nvidia_blog_8012",
              "url": "https://blogs.nvidia.com/2025/12/13/sora_2_sliding_window_attentio",
              "published": "2025-12-13",
              "snippet": "Sora 2 leverages Sliding Window Attention to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_constitutional_ai_uses_tech",
          "source": "repo:vllm",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_falcon_3_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "model:falcon_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-24_techcrunch_6313",
              "url": "https://techcrunch.com/2026/01/24/attention_is_all_you_need_v2_f",
              "published": "2026-01-24",
              "snippet": "On the Falcon 3 benchmark, Attention Is All You Need v2 scored 95%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_sora_2_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:sora_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_hellaswag_evaluated_on",
          "source": "model:claude_sonnet_4",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-29_the_verge_6976",
              "url": "https://theverge.com/2025/12/29/claude_sonnet_4_hellaswag",
              "published": "2025-12-29",
              "snippet": "Claude Sonnet 4 achieves 89% on HellaSwag, setting a new record..."
            },
            {
              "docId": "2026-01-25_anthropic_blog_1338",
              "url": "https://anthropic.com/news/2026/01/25/claude_sonnet_4_hellaswag",
              "published": "2026-01-25",
              "snippet": "On the HellaSwag benchmark, Claude Sonnet 4 scored 89%..."
            },
            {
              "docId": "2026-01-25_wired_2741",
              "url": "https://wired.com/2026/01/25/claude_sonnet_4_hellaswag",
              "published": "2026-01-25",
              "snippet": "Claude Sonnet 4 achieves 90% on HellaSwag, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_tree_of_thought_uses_tech",
          "source": "dataset:starcoder_data",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:dolma_mixture_of_experts_uses_tech",
          "source": "dataset:dolma",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.56
        }
      },
      {
        "data": {
          "id": "e:dall_e_4_arc_agi_evaluated_on",
          "source": "model:dall_e_4",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-12-04_hugging_face_bl_5294",
              "url": "https://huggingface.co/blog/2025/12/04/dall_e_4_arc_agi",
              "published": "2025-12-04",
              "snippet": "DALL-E 4 achieves 83% on ARC-AGI, setting a new record..."
            },
            {
              "docId": "2026-01-10_weights_and_bia_7266",
              "url": "https://wandb.ai/articles/2026/01/10/dall_e_4_arc_agi",
              "published": "2026-01-10",
              "snippet": "On the ARC-AGI benchmark, DALL-E 4 scored 93%..."
            },
            {
              "docId": "2026-01-10_google_ai_blog_4719",
              "url": "https://blog.google/technology/ai/2026/01/10/dall_e_4_arc_agi",
              "published": "2026-01-10",
              "snippet": "Evaluation results show DALL-E 4 reaching 75% on ARC-AGI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_chain_of_thought_uses_tech",
          "source": "repo:vllm",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_qwen_3_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:qwen_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-18_microsoft_resea_6597",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/18/toolformer:_language_models_ca",
              "published": "2026-01-18",
              "snippet": "On the Qwen-3 benchmark, Toolformer: Language Models Can Teach Themselves to Use Tools scored 95%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_hellaswag_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2025-12-30_techcrunch_5186",
              "url": "https://techcrunch.com/2025/12/30/claude_opus_45_hellaswag",
              "published": "2025-12-30",
              "snippet": "Claude Opus 4.5 achieves 76% on HellaSwag, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_gpqa_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-18_weights_and_bia_1171",
              "url": "https://wandb.ai/articles/2026/01/18/claude_opus_45_gpqa",
              "published": "2026-01-18",
              "snippet": "Evaluation results show Claude Opus 4.5 reaching 93% on GPQA..."
            },
            {
              "docId": "2026-01-23_hugging_face_bl_3117",
              "url": "https://huggingface.co/blog/2026/01/23/claude_opus_45_gpqa",
              "published": "2026-01-23",
              "snippet": "Evaluation results show Claude Opus 4.5 reaching 97% on GPQA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_lora_uses_tech",
          "source": "repo:vllm",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-23_google_ai_blog_5497",
              "url": "https://blog.google/technology/ai/2026/01/23/vllm_lora",
              "published": "2026-01-23",
              "snippet": "Technical details reveal vllm relies heavily on LoRA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_sparse_attention_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-24_bloomberg_9799",
              "url": "https://bloomberg.com/technology/2026/01/24/chain_of_thought_prompting_eli",
              "published": "2026-01-24",
              "snippet": "Technical details reveal Chain-of-Thought Prompting Elicits Reasoning relies heavily on Sparse Attention..."
            },
            {
              "docId": "2026-01-25_the_gradient_5732",
              "url": "https://thegradient.pub/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning leverages Sparse Attention to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_mixtral_8x22b_depends_on",
          "source": "model:command_r_plus",
          "target": "model:mixtral_8x22b",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.55
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_dpo_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:dpo",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:transformers_group_query_attention_uses_tech",
          "source": "repo:transformers",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_4660",
              "url": "https://venturebeat.com/2026/01/23/transformers_group_query_atten",
              "published": "2026-01-23",
              "snippet": "Under the hood, transformers implements Group Query Attention for improved efficiency..."
            },
            {
              "docId": "2026-01-24_google_ai_blog_4439",
              "url": "https://blog.google/technology/ai/2026/01/24/transformers_group_query_atten",
              "published": "2026-01-24",
              "snippet": "transformers leverages Group Query Attention to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_tree_of_thought_uses_tech",
          "source": "model:gpt_4o_mini_2",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:llamacpp_llama_4_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:llama_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-19_nextgov_9490",
              "url": "https://nextgov.com/2026/01/19/llamacpp_llama_4",
              "published": "2026-01-19",
              "snippet": "llama.cpp now supports Llama 4 with full feature parity..."
            },
            {
              "docId": "2026-01-20_nvidia_blog_8850",
              "url": "https://blogs.nvidia.com/2026/01/20/llamacpp_llama_4",
              "published": "2026-01-20",
              "snippet": "llama.cpp announced official support for Llama 4..."
            },
            {
              "docId": "2026-01-21_microsoft_resea_8975",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/21/llamacpp_llama_4",
              "published": "2026-01-21",
              "snippet": "llama.cpp now supports Llama 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:slimpajama_group_query_attention_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.61
        }
      },
      {
        "data": {
          "id": "e:vllm_nemotron_5_integrates_with",
          "source": "repo:vllm",
          "target": "model:nemotron_5",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-24_langchain_blog_3499",
              "url": "https://blog.langchain.dev/2026/01/24/vllm_nemotron_5",
              "published": "2026-01-24",
              "snippet": "vllm announced official support for Nemotron-5..."
            },
            {
              "docId": "2026-01-24_microsoft_resea_2431",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/vllm_nemotron_5",
              "published": "2026-01-24",
              "snippet": "The latest release of vllm adds native Nemotron-5 integration..."
            },
            {
              "docId": "2026-01-24_microsoft_resea_3845",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/vllm_nemotron_5",
              "published": "2026-01-24",
              "snippet": "vllm announced official support for Nemotron-5..."
            },
            {
              "docId": "2026-01-24_mit_technology__6997",
              "url": "https://technologyreview.com/2026/01/24/vllm_nemotron_5",
              "published": "2026-01-24",
              "snippet": "vllm announced official support for Nemotron-5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:redpajama_v2_quantization_uses_tech",
          "source": "dataset:redpajama_v2",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:litellm_llama_4_integrates_with",
          "source": "tool:litellm",
          "target": "model:llama_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-21_langchain_blog_6651",
              "url": "https://blog.langchain.dev/2026/01/21/litellm_llama_4",
              "published": "2026-01-21",
              "snippet": "LiteLLM now supports Llama 4 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_lite_group_query_attention_uses_tech",
          "source": "model:gemini_ultra_2_lite",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.65
        }
      },
      {
        "data": {
          "id": "e:transformers_transformer_architecture_uses_tech",
          "source": "repo:transformers",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-24_nvidia_blog_9589",
              "url": "https://blogs.nvidia.com/2025/12/24/transformers_transformer_archi",
              "published": "2025-12-24",
              "snippet": "Under the hood, transformers implements Transformer Architecture for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_phi_4_depends_on",
          "source": "model:nemotron_5",
          "target": "model:phi_4",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.44
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_kv_cache_optimization_uses_tech",
          "source": "model:gpt_4o_mini_2",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2026-01-13_ars_technica_9344",
              "url": "https://arstechnica.com/2026/01/13/gpt_4o_mini_2_kv_cache_optimiz",
              "published": "2026-01-13",
              "snippet": "Under the hood, GPT-4o Mini 2 implements KV Cache Optimization for improved efficiency..."
            },
            {
              "docId": "2026-01-22_the_gradient_6506",
              "url": "https://thegradient.pub/2026/01/22/gpt_4o_mini_2_kv_cache_optimiz",
              "published": "2026-01-22",
              "snippet": "Technical details reveal GPT-4o Mini 2 relies heavily on KV Cache Optimization..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:ollama_mixture_of_experts_uses_tech",
          "source": "tool:ollama",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2025-12-27_anthropic_blog_4779",
              "url": "https://anthropic.com/news/2025/12/27/ollama_mixture_of_experts",
              "published": "2025-12-27",
              "snippet": "Ollama leverages Mixture of Experts to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_wired_8010",
              "url": "https://wired.com/2026/01/25/ollama_mixture_of_experts",
              "published": "2026-01-25",
              "snippet": "Ollama leverages Mixture of Experts to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_nextgov_4180",
              "url": "https://nextgov.com/2026/01/25/ollama_mixture_of_experts",
              "published": "2026-01-25",
              "snippet": "Ollama leverages Mixture of Experts to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_jamba_2_depends_on",
          "source": "model:claude_sonnet_4",
          "target": "model:jamba_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:litellm_autogpt_integrates_with",
          "source": "tool:litellm",
          "target": "tool:autogpt",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.59,
          "evidence": [
            {
              "docId": "2026-01-21_the_gradient_4039",
              "url": "https://thegradient.pub/2026/01/21/litellm_autogpt",
              "published": "2026-01-21",
              "snippet": "LiteLLM now supports AutoGPT with full feature parity..."
            },
            {
              "docId": "2026-01-24_ars_technica_4948",
              "url": "https://arstechnica.com/2026/01/24/litellm_autogpt",
              "published": "2026-01-24",
              "snippet": "LiteLLM announced official support for AutoGPT..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_aya_3_integrates_with",
          "source": "repo:vllm",
          "target": "model:aya_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-15_venturebeat_9798",
              "url": "https://venturebeat.com/2026/01/15/vllm_aya_3",
              "published": "2026-01-15",
              "snippet": "vllm announced official support for Aya 3..."
            },
            {
              "docId": "2026-01-15_the_gradient_9714",
              "url": "https://thegradient.pub/2026/01/15/vllm_aya_3",
              "published": "2026-01-15",
              "snippet": "The latest release of vllm adds native Aya 3 integration..."
            },
            {
              "docId": "2026-01-15_wired_6955",
              "url": "https://wired.com/2026/01/15/vllm_aya_3",
              "published": "2026-01-15",
              "snippet": "vllm now supports Aya 3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_gguf_uses_tech",
          "source": "repo:langchain",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2025-12-08_langchain_blog_6997",
              "url": "https://blog.langchain.dev/2025/12/08/langchain_gguf",
              "published": "2025-12-08",
              "snippet": "Technical details reveal langchain relies heavily on GGUF..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_midjourney_v7_measures",
          "source": "benchmark:hellaswag",
          "target": "model:midjourney_v7",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-04_hugging_face_bl_4334",
              "url": "https://huggingface.co/blog/2026/01/04/hellaswag_midjourney_v7",
              "published": "2026-01-04",
              "snippet": "The HellaSwag benchmark measures Midjourney V7 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__group_query_attention_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__stable_diffusion_4_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:stable_diffusion_4",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.73,
          "evidence": [
            {
              "docId": "2026-01-22_mit_technology__3367",
              "url": "https://technologyreview.com/2026/01/22/constitutional_ai:_harmlessnes",
              "published": "2026-01-22",
              "snippet": "Constitutional AI: Harmlessness from AI Feedback achieves 88% on Stable Diffusion 4, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_truthfulqa_evaluated_on",
          "source": "model:whisper_v4",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-25_langchain_blog_2156",
              "url": "https://blog.langchain.dev/2026/01/25/whisper_v4_truthfulqa",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Whisper v4 reaching 76% on TruthfulQA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_4o_mini_2_truthfulqa_evaluated_on",
          "source": "model:gpt_4o_mini_2",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_truthfulqa_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-23_reuters_2800",
              "url": "https://reuters.com/technology/2026/01/23/claude_opus_45_truthfulqa",
              "published": "2026-01-23",
              "snippet": "On the TruthfulQA benchmark, Claude Opus 4.5 scored 82%..."
            },
            {
              "docId": "2026-01-25_venturebeat_1606",
              "url": "https://venturebeat.com/2026/01/25/claude_opus_45_truthfulqa",
              "published": "2026-01-25",
              "snippet": "Claude Opus 4.5 achieves 70% on TruthfulQA, setting a new record..."
            },
            {
              "docId": "2026-01-25_the_verge_4316",
              "url": "https://theverge.com/2026/01/25/claude_opus_45_truthfulqa",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Claude Opus 4.5 reaching 97% on TruthfulQA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_localai_integrates_with",
          "source": "tool:langchain",
          "target": "tool:localai",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:gpqa_yi_large_measures",
          "source": "benchmark:gpqa",
          "target": "model:yi_large",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-16_nvidia_blog_3843",
              "url": "https://blogs.nvidia.com/2026/01/16/gpqa_yi_large",
              "published": "2026-01-16",
              "snippet": "GPQA provides standardized evaluation of Yi-Large..."
            },
            {
              "docId": "2026-01-16_wired_3795",
              "url": "https://wired.com/2026/01/16/gpqa_yi_large",
              "published": "2026-01-16",
              "snippet": "GPQA has become the standard for evaluating Yi-Large..."
            },
            {
              "docId": "2026-01-17_techcrunch_6140",
              "url": "https://techcrunch.com/2026/01/17/gpqa_yi_large",
              "published": "2026-01-17",
              "snippet": "GPQA provides standardized evaluation of Yi-Large..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_slimpajama_trained_on",
          "source": "model:grok_3",
          "target": "dataset:slimpajama",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__palm_3_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:palm_3",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:fineweb_rotary_position_embedding_uses_tech",
          "source": "dataset:fineweb",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:llamacpp_constitutional_ai_uses_tech",
          "source": "repo:llamacpp",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-17_weights_and_bia_6138",
              "url": "https://wandb.ai/articles/2026/01/17/llamacpp_constitutional_ai",
              "published": "2026-01-17",
              "snippet": "llama.cpp leverages Constitutional AI to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-20_the_gradient_1224",
              "url": "https://thegradient.pub/2026/01/20/llamacpp_constitutional_ai",
              "published": "2026-01-20",
              "snippet": "Technical details reveal llama.cpp relies heavily on Constitutional AI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:dolma_sparse_attention_uses_tech",
          "source": "dataset:dolma",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.46
        }
      },
      {
        "data": {
          "id": "e:slimpajama_transformer_architecture_uses_tech",
          "source": "dataset:slimpajama",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:sora_2_transformer_architecture_uses_tech",
          "source": "model:sora_2",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:math_llama_4_measures",
          "source": "benchmark:math",
          "target": "model:llama_4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-20_reuters_5153",
              "url": "https://reuters.com/technology/2026/01/20/math_llama_4",
              "published": "2026-01-20",
              "snippet": "The MATH benchmark measures Llama 4 across multiple tasks..."
            },
            {
              "docId": "2026-01-25_arxiv_5249",
              "url": "https://arxiv.org/abs/2026/01/25/math_llama_4",
              "published": "2026-01-25",
              "snippet": "MATH provides standardized evaluation of Llama 4..."
            },
            {
              "docId": "2026-01-25_meta_ai_blog_4358",
              "url": "https://ai.meta.com/blog/2026/01/25/math_llama_4",
              "published": "2026-01-25",
              "snippet": "The MATH benchmark measures Llama 4 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:laion_5b_group_query_attention_uses_tech",
          "source": "dataset:laion_5b",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:autogpt_gpt_5_core_integrates_with",
          "source": "tool:autogpt",
          "target": "model:gpt_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2025-12-08_hugging_face_bl_4347",
              "url": "https://huggingface.co/blog/2025/12/08/autogpt_gpt_5_core",
              "published": "2025-12-08",
              "snippet": "AutoGPT now supports GPT-5 Core with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:stable_diffusion_4_flash_attention_uses_tech",
          "source": "model:stable_diffusion_4",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:sora_2_mixtral_8x22b_depends_on",
          "source": "model:sora_2",
          "target": "model:mixtral_8x22b",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.59
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_lite_gpqa_evaluated_on",
          "source": "model:gemini_ultra_2_lite",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_gguf_uses_tech",
          "source": "tool:tensorrt_llm",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.64
        }
      },
      {
        "data": {
          "id": "e:vllm_gpt_5_core_integrates_with",
          "source": "repo:vllm",
          "target": "model:gpt_5_core",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.79,
          "evidence": [
            {
              "docId": "2025-12-09_techcrunch_6114",
              "url": "https://techcrunch.com/2025/12/09/vllm_gpt_5_core",
              "published": "2025-12-09",
              "snippet": "vllm announced official support for GPT-5 Core..."
            },
            {
              "docId": "2025-12-29_nextgov_8931",
              "url": "https://nextgov.com/2025/12/29/vllm_gpt_5_core",
              "published": "2025-12-29",
              "snippet": "vllm now supports GPT-5 Core with full feature parity..."
            },
            {
              "docId": "2025-12-30_reuters_1812",
              "url": "https://reuters.com/technology/2025/12/30/vllm_gpt_5_core",
              "published": "2025-12-30",
              "snippet": "vllm announced official support for GPT-5 Core..."
            },
            {
              "docId": "2026-01-15_ars_technica_2369",
              "url": "https://arstechnica.com/2026/01/15/vllm_gpt_5_core",
              "published": "2026-01-15",
              "snippet": "vllm announced official support for GPT-5 Core..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_gpt_5_depends_on",
          "source": "model:whisper_v4",
          "target": "model:gpt_5",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:llamacpp_lora_uses_tech",
          "source": "repo:llamacpp",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-20_google_ai_blog_1342",
              "url": "https://blog.google/technology/ai/2026/01/20/llamacpp_lora",
              "published": "2026-01-20",
              "snippet": "Technical details reveal llama.cpp relies heavily on LoRA..."
            },
            {
              "docId": "2026-01-23_openai_blog_2119",
              "url": "https://openai.com/blog/2026/01/23/llamacpp_lora",
              "published": "2026-01-23",
              "snippet": "llama.cpp leverages LoRA to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__midjourney_v7_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:midjourney_v7",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-21_anthropic_blog_9122",
              "url": "https://anthropic.com/news/2025/12/21/scaling_laws_for_neural_langua",
              "published": "2025-12-21",
              "snippet": "Scaling Laws for Neural Language Models (2025) achieves 78% on Midjourney V7, setting a new record..."
            },
            {
              "docId": "2025-12-22_nvidia_blog_1312",
              "url": "https://blogs.nvidia.com/2025/12/22/scaling_laws_for_neural_langua",
              "published": "2025-12-22",
              "snippet": "On the Midjourney V7 benchmark, Scaling Laws for Neural Language Models (2025) scored 84%..."
            },
            {
              "docId": "2026-01-15_openai_blog_8018",
              "url": "https://openai.com/blog/2026/01/15/scaling_laws_for_neural_langua",
              "published": "2026-01-15",
              "snippet": "Evaluation results show Scaling Laws for Neural Language Models (2025) reaching 98% on Midjourney V7..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_falcon_3_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:falcon_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-12_hugging_face_bl_7438",
              "url": "https://huggingface.co/blog/2026/01/12/llamacpp_falcon_3",
              "published": "2026-01-12",
              "snippet": "llama.cpp announced official support for Falcon 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_mmlu_evaluated_on",
          "source": "model:whisper_v4",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_laion_5b_trained_on",
          "source": "model:nemotron_5",
          "target": "dataset:laion_5b",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:transformers_midjourney_v7_integrates_with",
          "source": "repo:transformers",
          "target": "model:midjourney_v7",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-11_wired_6712",
              "url": "https://wired.com/2026/01/11/transformers_midjourney_v7",
              "published": "2026-01-11",
              "snippet": "transformers now supports Midjourney V7 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_claude_opus_45_v2_integrates_with",
          "source": "repo:langchain",
          "target": "model:claude_opus_45_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:streamlit_rotary_position_embedding_uses_tech",
          "source": "tool:streamlit",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-23_techcrunch_7678",
              "url": "https://techcrunch.com/2025/12/23/streamlit_rotary_position_embe",
              "published": "2025-12-23",
              "snippet": "Technical details reveal Streamlit relies heavily on Rotary Position Embedding..."
            },
            {
              "docId": "2026-01-21_google_ai_blog_4142",
              "url": "https://blog.google/technology/ai/2026/01/21/streamlit_rotary_position_embe",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Streamlit relies heavily on Rotary Position Embedding..."
            },
            {
              "docId": "2026-01-25_venturebeat_9077",
              "url": "https://venturebeat.com/2026/01/25/streamlit_rotary_position_embe",
              "published": "2026-01-25",
              "snippet": "Streamlit leverages Rotary Position Embedding to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_dall_e_4_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:dall_e_4",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_truthfulqa_evaluated_on",
          "source": "model:claude_sonnet_4",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-22_anthropic_blog_8304",
              "url": "https://anthropic.com/news/2026/01/22/claude_sonnet_4_truthfulqa",
              "published": "2026-01-22",
              "snippet": "Claude Sonnet 4 achieves 87% on TruthfulQA, setting a new record..."
            },
            {
              "docId": "2026-01-23_mit_technology__5217",
              "url": "https://technologyreview.com/2026/01/23/claude_sonnet_4_truthfulqa",
              "published": "2026-01-23",
              "snippet": "On the TruthfulQA benchmark, Claude Sonnet 4 scored 93%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_flash_attention_uses_tech",
          "source": "model:midjourney_v7",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-23_meta_ai_blog_3530",
              "url": "https://ai.meta.com/blog/2026/01/23/midjourney_v7_flash_attention",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Midjourney V7 relies heavily on Flash Attention..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_2937",
              "url": "https://blog.google/technology/ai/2026/01/25/midjourney_v7_flash_attention",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Midjourney V7 relies heavily on Flash Attention..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_multimodal_fusion_uses_tech",
          "source": "model:claude_opus_45",
          "target": "tech:multimodal_fusion",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:litellm_stable_diffusion_4_integrates_with",
          "source": "tool:litellm",
          "target": "model:stable_diffusion_4",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.86
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_yi_large_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:yi_large",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2025-12-30_the_gradient_7949",
              "url": "https://thegradient.pub/2025/12/30/tensorrt_llm_yi_large",
              "published": "2025-12-30",
              "snippet": "TensorRT-LLM now supports Yi-Large with full feature parity..."
            },
            {
              "docId": "2026-01-21_mit_technology__4429",
              "url": "https://technologyreview.com/2026/01/21/tensorrt_llm_yi_large",
              "published": "2026-01-21",
              "snippet": "TensorRT-LLM announced official support for Yi-Large..."
            },
            {
              "docId": "2026-01-25_openai_blog_8437",
              "url": "https://openai.com/blog/2026/01/25/tensorrt_llm_yi_large",
              "published": "2026-01-25",
              "snippet": "The latest release of TensorRT-LLM adds native Yi-Large integration..."
            },
            {
              "docId": "2026-01-25_bloomberg_2243",
              "url": "https://bloomberg.com/technology/2026/01/25/tensorrt_llm_yi_large",
              "published": "2026-01-25",
              "snippet": "TensorRT-LLM now supports Yi-Large with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:hellaswag_gemma_3_measures",
          "source": "benchmark:hellaswag",
          "target": "model:gemma_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-03_mit_technology__7171",
              "url": "https://technologyreview.com/2026/01/03/hellaswag_gemma_3",
              "published": "2026-01-03",
              "snippet": "HellaSwag has become the standard for evaluating Gemma 3..."
            },
            {
              "docId": "2026-01-18_anthropic_blog_6941",
              "url": "https://anthropic.com/news/2026/01/18/hellaswag_gemma_3",
              "published": "2026-01-18",
              "snippet": "HellaSwag has become the standard for evaluating Gemma 3..."
            },
            {
              "docId": "2026-01-25_openai_blog_8028",
              "url": "https://openai.com/blog/2026/01/25/hellaswag_gemma_3",
              "published": "2026-01-25",
              "snippet": "HellaSwag provides standardized evaluation of Gemma 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_gpt_5_integrates_with",
          "source": "repo:langchain",
          "target": "model:gpt_5",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.81
        }
      },
      {
        "data": {
          "id": "e:ollama_gemini_ultra_2_integrates_with",
          "source": "tool:ollama",
          "target": "model:gemini_ultra_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-01_reuters_4149",
              "url": "https://reuters.com/technology/2026/01/01/ollama_gemini_ultra_2",
              "published": "2026-01-01",
              "snippet": "Ollama announced official support for Gemini Ultra 2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_claude_sonnet_4_measures",
          "source": "benchmark:mmlu",
          "target": "model:claude_sonnet_4",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-24_the_gradient_8195",
              "url": "https://thegradient.pub/2026/01/24/mmlu_claude_sonnet_4",
              "published": "2026-01-24",
              "snippet": "MMLU has become the standard for evaluating Claude Sonnet 4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_common_crawl_trained_on",
          "source": "model:midjourney_v7",
          "target": "dataset:common_crawl",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:aya_3_truthfulqa_evaluated_on",
          "source": "model:aya_3",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-19_microsoft_resea_9312",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/19/aya_3_truthfulqa",
              "published": "2026-01-19",
              "snippet": "Evaluation results show Aya 3 reaching 96% on TruthfulQA..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_deepseek_v3_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:deepseek_v3",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_gpt_5_core_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:gpt_5_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-17_bloomberg_8248",
              "url": "https://bloomberg.com/technology/2026/01/17/direct_preference_optimization",
              "published": "2026-01-17",
              "snippet": "Direct Preference Optimization achieves 86% on GPT-5 Core, setting a new record..."
            },
            {
              "docId": "2026-01-18_weights_and_bia_9801",
              "url": "https://wandb.ai/articles/2026/01/18/direct_preference_optimization",
              "published": "2026-01-18",
              "snippet": "On the GPT-5 Core benchmark, Direct Preference Optimization scored 78%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_rotary_position_embedding_uses_tech",
          "source": "repo:transformers",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.66,
          "evidence": [
            {
              "docId": "2025-12-19_langchain_blog_2647",
              "url": "https://blog.langchain.dev/2025/12/19/transformers_rotary_position_e",
              "published": "2025-12-19",
              "snippet": "Technical details reveal transformers relies heavily on Rotary Position Embedding..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:qwen_3_starcoder_data_trained_on",
          "source": "model:qwen_3",
          "target": "dataset:starcoder_data",
          "rel": "TRAINED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_qlora_uses_tech",
          "source": "dataset:the_stack_v2",
          "target": "tech:qlora",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__midjourney_v7_evaluated_on",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "model:midjourney_v7",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.72
        }
      },
      {
        "data": {
          "id": "e:transformers_grok_3_integrates_with",
          "source": "repo:transformers",
          "target": "model:grok_3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:vllm_jamba_2_integrates_with",
          "source": "repo:vllm",
          "target": "model:jamba_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-22_mit_technology__3587",
              "url": "https://technologyreview.com/2025/12/22/vllm_jamba_2",
              "published": "2025-12-22",
              "snippet": "vllm now supports Jamba 2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mixtral_8x22b_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:mixtral_8x22b",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2026-01-24_the_verge_2747",
              "url": "https://theverge.com/2026/01/24/llamacpp_mixtral_8x22b",
              "published": "2026-01-24",
              "snippet": "llama.cpp now supports Mixtral 8x22B with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_gpt_5_core_evaluated_on",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "model:gpt_5_core",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2026-01-25_google_ai_blog_1145",
              "url": "https://blog.google/technology/ai/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Evaluation results show Chain-of-Thought Prompting Elicits Reasoning reaching 98% on GPT-5 Core..."
            },
            {
              "docId": "2026-01-25_microsoft_resea_2251",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning achieves 74% on GPT-5 Core, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_tree_of_thought_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-25_reuters_5046",
              "url": "https://reuters.com/technology/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning leverages Tree of Thought to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_hugging_face_bl_3890",
              "url": "https://huggingface.co/blog/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Chain-of-Thought Prompting Elicits Reasoning relies heavily on Tree of Thought..."
            },
            {
              "docId": "2026-01-25_google_ai_blog_2842",
              "url": "https://blog.google/technology/ai/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning leverages Tree of Thought to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_arxiv_9727",
              "url": "https://arxiv.org/abs/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Chain-of-Thought Prompting Elicits Reasoning relies heavily on Tree of Thought..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_flash_attention_uses_tech",
          "source": "repo:langchain",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:langchain_yi_large_integrates_with",
          "source": "repo:langchain",
          "target": "model:yi_large",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2025-12-22_techcrunch_6916",
              "url": "https://techcrunch.com/2025/12/22/langchain_yi_large",
              "published": "2025-12-22",
              "snippet": "langchain now supports Yi-Large with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_dall_e_4_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:dall_e_4",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:llamaindex_yi_large_integrates_with",
          "source": "tool:llamaindex",
          "target": "model:yi_large",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-19_venturebeat_5239",
              "url": "https://venturebeat.com/2026/01/19/llamaindex_yi_large",
              "published": "2026-01-19",
              "snippet": "LlamaIndex announced official support for Yi-Large..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_haystack_integrates_with",
          "source": "tool:semantic_kernel",
          "target": "tool:haystack",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-14_microsoft_resea_9727",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/14/semantic_kernel_haystack",
              "published": "2026-01-14",
              "snippet": "The latest release of Semantic Kernel adds native Haystack integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_qwen_3_integrates_with",
          "source": "repo:langchain",
          "target": "model:qwen_3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_chain_of_thought_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-08_meta_ai_blog_2321",
              "url": "https://ai.meta.com/blog/2026/01/08/flash_attention:_fast_and_memo",
              "published": "2026-01-08",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention leverages Chain-of-Thought to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-21_openai_blog_5654",
              "url": "https://openai.com/blog/2026/01/21/flash_attention:_fast_and_memo",
              "published": "2026-01-21",
              "snippet": "Technical details reveal Flash Attention: Fast and Memory-Efficient Attention relies heavily on Chain-of-Thought..."
            },
            {
              "docId": "2026-01-23_reuters_2787",
              "url": "https://reuters.com/technology/2026/01/23/flash_attention:_fast_and_memo",
              "published": "2026-01-23",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention leverages Chain-of-Thought to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-23_ars_technica_8123",
              "url": "https://arstechnica.com/2026/01/23/flash_attention:_fast_and_memo",
              "published": "2026-01-23",
              "snippet": "Technical details reveal Flash Attention: Fast and Memory-Efficient Attention relies heavily on Chain-of-Thought..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_codex_2_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:codex_2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:dolma_rlhf_uses_tech",
          "source": "dataset:dolma",
          "target": "tech:rlhf",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:the_pile_transformer_architecture_uses_tech",
          "source": "dataset:the_pile",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:mlflow_claude_opus_45_integrates_with",
          "source": "tool:mlflow",
          "target": "model:claude_opus_45",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-19_nextgov_7243",
              "url": "https://nextgov.com/2026/01/19/mlflow_claude_opus_45",
              "published": "2026-01-19",
              "snippet": "MLflow now supports Claude Opus 4.5 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:litellm_mixtral_8x22b_integrates_with",
          "source": "tool:litellm",
          "target": "model:mixtral_8x22b",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2026-01-22_arxiv_8117",
              "url": "https://arxiv.org/abs/2026/01/22/litellm_mixtral_8x22b",
              "published": "2026-01-22",
              "snippet": "LiteLLM now supports Mixtral 8x22B with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_tree_of_thought_uses_tech",
          "source": "repo:vllm",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:langchain_tool_use_uses_tech",
          "source": "repo:langchain",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-25_google_ai_blog_4609",
              "url": "https://blog.google/technology/ai/2025/12/25/langchain_tool_use",
              "published": "2025-12-25",
              "snippet": "Technical details reveal langchain relies heavily on Tool Use..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_mixture_of_experts_uses_tech",
          "source": "repo:llamacpp",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-20_nvidia_blog_5604",
              "url": "https://blogs.nvidia.com/2026/01/20/llamacpp_mixture_of_experts",
              "published": "2026-01-20",
              "snippet": "Under the hood, llama.cpp implements Mixture of Experts for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:grok_3_claude_opus_45_v2_depends_on",
          "source": "model:grok_3",
          "target": "model:claude_opus_45_v2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:gradio_autogpt_integrates_with",
          "source": "tool:gradio",
          "target": "tool:autogpt",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.65,
          "evidence": [
            {
              "docId": "2025-12-22_openai_blog_8175",
              "url": "https://openai.com/blog/2025/12/22/gradio_autogpt",
              "published": "2025-12-22",
              "snippet": "The latest release of Gradio adds native AutoGPT integration..."
            },
            {
              "docId": "2025-12-25_openai_blog_3048",
              "url": "https://openai.com/blog/2025/12/25/gradio_autogpt",
              "published": "2025-12-25",
              "snippet": "Gradio now supports AutoGPT with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamacpp_phi_4_integrates_with",
          "source": "repo:llamacpp",
          "target": "model:phi_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-11_bloomberg_3893",
              "url": "https://bloomberg.com/technology/2026/01/11/llamacpp_phi_4",
              "published": "2026-01-11",
              "snippet": "llama.cpp announced official support for Phi-4..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_lite_truthfulqa_evaluated_on",
          "source": "model:gemini_ultra_2_lite",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.89,
          "evidence": [
            {
              "docId": "2026-01-23_venturebeat_1052",
              "url": "https://venturebeat.com/2026/01/23/gemini_ultra_2_lite_truthfulqa",
              "published": "2026-01-23",
              "snippet": "Gemini Ultra 2 Lite achieves 72% on TruthfulQA, setting a new record..."
            },
            {
              "docId": "2026-01-23_wired_8969",
              "url": "https://wired.com/2026/01/23/gemini_ultra_2_lite_truthfulqa",
              "published": "2026-01-23",
              "snippet": "On the TruthfulQA benchmark, Gemini Ultra 2 Lite scored 81%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_tool_use_uses_tech",
          "source": "model:aya_3",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.77
        }
      },
      {
        "data": {
          "id": "e:gpt_5_hellaswag_evaluated_on",
          "source": "model:gpt_5",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-07_the_verge_5930",
              "url": "https://theverge.com/2026/01/07/gpt_5_hellaswag",
              "published": "2026-01-07",
              "snippet": "Evaluation results show GPT-5 reaching 76% on HellaSwag..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_gemini_ultra_2_integrates_with",
          "source": "tool:langchain",
          "target": "model:gemini_ultra_2",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:transformers_retrieval_augmented_generation_uses_tech",
          "source": "repo:transformers",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-10_weights_and_bia_3540",
              "url": "https://wandb.ai/articles/2025/12/10/transformers_retrieval_augment",
              "published": "2025-12-10",
              "snippet": "transformers leverages Retrieval-Augmented Generation to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_arc_agi_evaluated_on",
          "source": "model:sora_2",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-11-10_mit_technology__5504",
              "url": "https://technologyreview.com/2025/11/10/sora_2_arc_agi",
              "published": "2025-11-10",
              "snippet": "Evaluation results show Sora 2 reaching 96% on ARC-AGI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_gguf_uses_tech",
          "source": "model:gemini_ultra_2",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-13_weights_and_bia_1885",
              "url": "https://wandb.ai/articles/2025/12/13/gemini_ultra_2_gguf",
              "published": "2025-12-13",
              "snippet": "Under the hood, Gemini Ultra 2 implements GGUF for improved efficiency..."
            },
            {
              "docId": "2025-12-26_wired_5728",
              "url": "https://wired.com/2025/12/26/gemini_ultra_2_gguf",
              "published": "2025-12-26",
              "snippet": "Under the hood, Gemini Ultra 2 implements GGUF for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_constitutional_ai_uses_tech",
          "source": "repo:langchain",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-17_the_gradient_2938",
              "url": "https://thegradient.pub/2026/01/17/langchain_constitutional_ai",
              "published": "2026-01-17",
              "snippet": "Under the hood, langchain implements Constitutional AI for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_mixture_of_experts_uses_tech",
          "source": "tool:streamlit",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:laion_5b_kv_cache_optimization_uses_tech",
          "source": "dataset:laion_5b",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.63
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_sliding_window_attention_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:sliding_window_attention",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-31_nvidia_blog_8449",
              "url": "https://blogs.nvidia.com/2025/12/31/attention_is_all_you_need_v2_s",
              "published": "2025-12-31",
              "snippet": "Under the hood, Attention Is All You Need v2 implements Sliding Window Attention for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:semantic_kernel_multimodal_fusion_uses_tech",
          "source": "tool:semantic_kernel",
          "target": "tech:multimodal_fusion",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.68
        }
      },
      {
        "data": {
          "id": "e:arc_agi_sora_2_measures",
          "source": "benchmark:arc_agi",
          "target": "model:sora_2",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.88
        }
      },
      {
        "data": {
          "id": "e:transformers_palm_3_integrates_with",
          "source": "repo:transformers",
          "target": "model:palm_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.74,
          "evidence": [
            {
              "docId": "2025-12-10_wired_1737",
              "url": "https://wired.com/2025/12/10/transformers_palm_3",
              "published": "2025-12-10",
              "snippet": "transformers now supports PaLM 3 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llama_4_mixture_of_experts_uses_tech",
          "source": "model:llama_4",
          "target": "tech:mixture_of_experts",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:mlflow_deepseek_v3_integrates_with",
          "source": "tool:mlflow",
          "target": "model:deepseek_v3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.78
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_sora_2_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:sora_2",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.74
        }
      },
      {
        "data": {
          "id": "e:vllm_tree_of_thought_uses_tech",
          "source": "tool:vllm",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.68,
          "evidence": [
            {
              "docId": "2026-01-22_venturebeat_3101",
              "url": "https://venturebeat.com/2026/01/22/vllm_tree_of_thought",
              "published": "2026-01-22",
              "snippet": "Technical details reveal vLLM relies heavily on Tree of Thought..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_transformer_architecture_uses_tech",
          "source": "repo:langchain",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2026-01-08_arxiv_7591",
              "url": "https://arxiv.org/abs/2026/01/08/langchain_transformer_architec",
              "published": "2026-01-08",
              "snippet": "langchain leverages Transformer Architecture to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_whisper_v4_evaluated_on",
          "source": "paper:direct_preference_optimization",
          "target": "model:whisper_v4",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:gpqa_claude_opus_45_measures",
          "source": "benchmark:gpqa",
          "target": "model:claude_opus_45",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2026-01-20_microsoft_resea_2181",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/20/gpqa_claude_opus_45",
              "published": "2026-01-20",
              "snippet": "GPQA has become the standard for evaluating Claude Opus 4.5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_truthfulqa_evaluated_on",
          "source": "model:gemma_3",
          "target": "benchmark:truthfulqa",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.95
        }
      },
      {
        "data": {
          "id": "e:litellm_constitutional_ai_uses_tech",
          "source": "tool:litellm",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.62,
          "evidence": [
            {
              "docId": "2026-01-21_the_gradient_4632",
              "url": "https://thegradient.pub/2026/01/21/litellm_constitutional_ai",
              "published": "2026-01-21",
              "snippet": "Under the hood, LiteLLM implements Constitutional AI for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_haystack_integrates_with",
          "source": "tool:langchain",
          "target": "tool:haystack",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2026-01-04_openai_blog_1934",
              "url": "https://openai.com/blog/2026/01/04/langchain_haystack",
              "published": "2026-01-04",
              "snippet": "LangChain now supports Haystack with full feature parity..."
            },
            {
              "docId": "2026-01-17_arxiv_3686",
              "url": "https://arxiv.org/abs/2026/01/17/langchain_haystack",
              "published": "2026-01-17",
              "snippet": "LangChain announced official support for Haystack..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_v2_mmlu_evaluated_on",
          "source": "model:claude_opus_45_v2",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-24_the_gradient_9795",
              "url": "https://thegradient.pub/2026/01/24/claude_opus_45_v2_mmlu",
              "published": "2026-01-24",
              "snippet": "On the MMLU benchmark, Claude Opus 4.5 v2 scored 85%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:haystack_mlflow_integrates_with",
          "source": "tool:haystack",
          "target": "tool:mlflow",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-21_nvidia_blog_3055",
              "url": "https://blogs.nvidia.com/2026/01/21/haystack_mlflow",
              "published": "2026-01-21",
              "snippet": "Haystack now supports MLflow with full feature parity..."
            },
            {
              "docId": "2026-01-22_techcrunch_8390",
              "url": "https://techcrunch.com/2026/01/22/haystack_mlflow",
              "published": "2026-01-22",
              "snippet": "Haystack announced official support for MLflow..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_retrieval_augmented_generation_uses_tech",
          "source": "tool:vllm",
          "target": "tech:retrieval_augmented_generation",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.87
        }
      },
      {
        "data": {
          "id": "e:command_r_plus_hellaswag_evaluated_on",
          "source": "model:command_r_plus",
          "target": "benchmark:hellaswag",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.92
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__kv_cache_optimization_uses_tech",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "tech:kv_cache_optimization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.89
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_v2_arc_agi_evaluated_on",
          "source": "model:claude_opus_45_v2",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-11_reuters_6958",
              "url": "https://reuters.com/technology/2026/01/11/claude_opus_45_v2_arc_agi",
              "published": "2026-01-11",
              "snippet": "Claude Opus 4.5 v2 achieves 97% on ARC-AGI, setting a new record..."
            },
            {
              "docId": "2026-01-17_microsoft_resea_2961",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/17/claude_opus_45_v2_arc_agi",
              "published": "2026-01-17",
              "snippet": "On the ARC-AGI benchmark, Claude Opus 4.5 v2 scored 94%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_ollama_integrates_with",
          "source": "tool:crewai",
          "target": "tool:ollama",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.6,
          "evidence": [
            {
              "docId": "2026-01-03_ars_technica_3865",
              "url": "https://arstechnica.com/2026/01/03/crewai_ollama",
              "published": "2026-01-03",
              "snippet": "CrewAI announced official support for Ollama..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_arc_agi_evaluated_on",
          "source": "model:gemini_ultra_2",
          "target": "benchmark:arc_agi",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.82,
          "evidence": [
            {
              "docId": "2026-01-08_bloomberg_2801",
              "url": "https://bloomberg.com/technology/2026/01/08/gemini_ultra_2_arc_agi",
              "published": "2026-01-08",
              "snippet": "Evaluation results show Gemini Ultra 2 reaching 82% on ARC-AGI..."
            },
            {
              "docId": "2026-01-20_nextgov_9868",
              "url": "https://nextgov.com/2026/01/20/gemini_ultra_2_arc_agi",
              "published": "2026-01-20",
              "snippet": "On the ARC-AGI benchmark, Gemini Ultra 2 scored 80%..."
            },
            {
              "docId": "2026-01-20_the_gradient_7057",
              "url": "https://thegradient.pub/2026/01/20/gemini_ultra_2_arc_agi",
              "published": "2026-01-20",
              "snippet": "Evaluation results show Gemini Ultra 2 reaching 92% on ARC-AGI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_gpt_5_measures",
          "source": "benchmark:gpqa",
          "target": "model:gpt_5",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2026-01-15_bloomberg_4649",
              "url": "https://bloomberg.com/technology/2026/01/15/gpqa_gpt_5",
              "published": "2026-01-15",
              "snippet": "The GPQA benchmark measures GPT-5 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_gemini_ultra_2_lite_integrates_with",
          "source": "repo:vllm",
          "target": "model:gemini_ultra_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:haystack_vllm_integrates_with",
          "source": "tool:haystack",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_command_r_plus_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:command_r_plus",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.71
        }
      },
      {
        "data": {
          "id": "e:transformers_tree_of_thought_uses_tech",
          "source": "repo:transformers",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.75,
          "evidence": [
            {
              "docId": "2026-01-25_the_gradient_3228",
              "url": "https://thegradient.pub/2026/01/25/transformers_tree_of_thought",
              "published": "2026-01-25",
              "snippet": "transformers leverages Tree of Thought to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mt_bench_qwen_3_measures",
          "source": "benchmark:mt_bench",
          "target": "model:qwen_3",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.97,
          "evidence": [
            {
              "docId": "2026-01-12_techcrunch_7064",
              "url": "https://techcrunch.com/2026/01/12/mt_bench_qwen_3",
              "published": "2026-01-12",
              "snippet": "MT-Bench has become the standard for evaluating Qwen-3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:langchain_vllm_integrates_with",
          "source": "tool:langchain",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2025-12-25_weights_and_bia_5107",
              "url": "https://wandb.ai/articles/2025/12/25/langchain_vllm",
              "published": "2025-12-25",
              "snippet": "LangChain announced official support for vLLM..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_stack_v2_speculative_decoding_uses_tech",
          "source": "dataset:the_stack_v2",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:direct_preference_optimization_tool_use_uses_tech",
          "source": "paper:direct_preference_optimization",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-25_bloomberg_5597",
              "url": "https://bloomberg.com/technology/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Under the hood, Direct Preference Optimization implements Tool Use for improved efficiency..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_3322",
              "url": "https://wandb.ai/articles/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Under the hood, Direct Preference Optimization implements Tool Use for improved efficiency..."
            },
            {
              "docId": "2026-01-25_bloomberg_4310",
              "url": "https://bloomberg.com/technology/2026/01/25/direct_preference_optimization",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Direct Preference Optimization relies heavily on Tool Use..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_claude_opus_45_v2_integrates_with",
          "source": "repo:transformers",
          "target": "model:claude_opus_45_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.72,
          "evidence": [
            {
              "docId": "2026-01-15_arxiv_8099",
              "url": "https://arxiv.org/abs/2026/01/15/transformers_claude_opus_45_v2",
              "published": "2026-01-15",
              "snippet": "The latest release of transformers adds native Claude Opus 4.5 v2 integration..."
            },
            {
              "docId": "2026-01-15_arxiv_3035",
              "url": "https://arxiv.org/abs/2026/01/15/transformers_claude_opus_45_v2",
              "published": "2026-01-15",
              "snippet": "transformers now supports Claude Opus 4.5 v2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:starcoder_data_tool_use_uses_tech",
          "source": "dataset:starcoder_data",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.6
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_yi_large_evaluated_on",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "model:yi_large",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-19_meta_ai_blog_8007",
              "url": "https://ai.meta.com/blog/2025/12/19/attention_is_all_you_need_v2_y",
              "published": "2025-12-19",
              "snippet": "Evaluation results show Attention Is All You Need v2 reaching 87% on Yi-Large..."
            },
            {
              "docId": "2025-12-29_nvidia_blog_5314",
              "url": "https://blogs.nvidia.com/2025/12/29/attention_is_all_you_need_v2_y",
              "published": "2025-12-29",
              "snippet": "On the Yi-Large benchmark, Attention Is All You Need v2 scored 85%..."
            },
            {
              "docId": "2026-01-19_meta_ai_blog_1923",
              "url": "https://ai.meta.com/blog/2026/01/19/attention_is_all_you_need_v2_y",
              "published": "2026-01-19",
              "snippet": "Evaluation results show Attention Is All You Need v2 reaching 86% on Yi-Large..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:weights_and_biases_claude_opus_45_v2_integrates_with",
          "source": "tool:weights_and_biases",
          "target": "model:claude_opus_45_v2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2025-12-25_mit_technology__4029",
              "url": "https://technologyreview.com/2025/12/25/weights_and_biases_claude_opus",
              "published": "2025-12-25",
              "snippet": "Weights & Biases announced official support for Claude Opus 4.5 v2..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_math_evaluated_on",
          "source": "model:gemini_ultra_2",
          "target": "benchmark:math",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.85,
          "evidence": [
            {
              "docId": "2025-12-13_anthropic_blog_3085",
              "url": "https://anthropic.com/news/2025/12/13/gemini_ultra_2_math",
              "published": "2025-12-13",
              "snippet": "Gemini Ultra 2 achieves 74% on MATH, setting a new record..."
            },
            {
              "docId": "2026-01-08_techcrunch_7923",
              "url": "https://techcrunch.com/2026/01/08/gemini_ultra_2_math",
              "published": "2026-01-08",
              "snippet": "On the MATH benchmark, Gemini Ultra 2 scored 73%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_chain_of_thought_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.78,
          "evidence": [
            {
              "docId": "2026-01-18_the_gradient_3306",
              "url": "https://thegradient.pub/2026/01/18/toolformer:_language_models_ca",
              "published": "2026-01-18",
              "snippet": "Technical details reveal Toolformer: Language Models Can Teach Themselves to Use Tools relies heavily on Chain-of-Thought..."
            },
            {
              "docId": "2026-01-21_wired_6897",
              "url": "https://wired.com/2026/01/21/toolformer:_language_models_ca",
              "published": "2026-01-21",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements Chain-of-Thought for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gemma_3_gguf_uses_tech",
          "source": "model:gemma_3",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.84
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_mmlu_evaluated_on",
          "source": "model:claude_opus_45",
          "target": "benchmark:mmlu",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.96,
          "evidence": [
            {
              "docId": "2026-01-18_google_ai_blog_6535",
              "url": "https://blog.google/technology/ai/2026/01/18/claude_opus_45_mmlu",
              "published": "2026-01-18",
              "snippet": "On the MMLU benchmark, Claude Opus 4.5 scored 82%..."
            },
            {
              "docId": "2026-01-21_hugging_face_bl_6031",
              "url": "https://huggingface.co/blog/2026/01/21/claude_opus_45_mmlu",
              "published": "2026-01-21",
              "snippet": "Claude Opus 4.5 achieves 93% on MMLU, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_speculative_decoding_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-12_the_gradient_9244",
              "url": "https://thegradient.pub/2026/01/12/attention_is_all_you_need_v2_s",
              "published": "2026-01-12",
              "snippet": "Under the hood, Attention Is All You Need v2 implements Speculative Decoding for improved efficiency..."
            },
            {
              "docId": "2026-01-17_nvidia_blog_9584",
              "url": "https://blogs.nvidia.com/2026/01/17/attention_is_all_you_need_v2_s",
              "published": "2026-01-17",
              "snippet": "Under the hood, Attention Is All You Need v2 implements Speculative Decoding for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:scaling_laws_for_neural_language_models__aya_3_evaluated_on",
          "source": "paper:scaling_laws_for_neural_language_models_",
          "target": "model:aya_3",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2025-12-24_weights_and_bia_5770",
              "url": "https://wandb.ai/articles/2025/12/24/scaling_laws_for_neural_langua",
              "published": "2025-12-24",
              "snippet": "Scaling Laws for Neural Language Models (2025) achieves 70% on Aya 3, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_quantization_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:autogpt_vllm_integrates_with",
          "source": "tool:autogpt",
          "target": "tool:vllm",
          "rel": "INTEGRATES_WITH",
          "kind": "hypothesis",
          "confidence": 0.8
        }
      },
      {
        "data": {
          "id": "e:langchain_sparse_attention_uses_tech",
          "source": "tool:langchain",
          "target": "tech:sparse_attention",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_tool_use_uses_tech",
          "source": "model:gemini_ultra_2",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.63,
          "evidence": [
            {
              "docId": "2026-01-08_venturebeat_6461",
              "url": "https://venturebeat.com/2026/01/08/gemini_ultra_2_tool_use",
              "published": "2026-01-08",
              "snippet": "Gemini Ultra 2 leverages Tool Use to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpt_5_core_claude_opus_45_depends_on",
          "source": "model:gpt_5_core",
          "target": "model:claude_opus_45",
          "rel": "DEPENDS_ON",
          "kind": "inferred",
          "confidence": 0.49
        }
      },
      {
        "data": {
          "id": "e:langchain_qlora_uses_tech",
          "source": "repo:langchain",
          "target": "tech:qlora",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:whisper_v4_tool_use_uses_tech",
          "source": "model:whisper_v4",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.58,
          "evidence": [
            {
              "docId": "2025-11-22_langchain_blog_7189",
              "url": "https://blog.langchain.dev/2025/11/22/whisper_v4_tool_use",
              "published": "2025-11-22",
              "snippet": "Whisper v4 leverages Tool Use to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-06_hugging_face_bl_3451",
              "url": "https://huggingface.co/blog/2026/01/06/whisper_v4_tool_use",
              "published": "2026-01-06",
              "snippet": "Technical details reveal Whisper v4 relies heavily on Tool Use..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:attention_is_all_you_need_v2_constitutional_ai_uses_tech",
          "source": "paper:attention_is_all_you_need_v2",
          "target": "tech:constitutional_ai",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.9,
          "evidence": [
            {
              "docId": "2025-12-27_ars_technica_5219",
              "url": "https://arstechnica.com/2025/12/27/attention_is_all_you_need_v2_c",
              "published": "2025-12-27",
              "snippet": "Under the hood, Attention Is All You Need v2 implements Constitutional AI for improved efficiency..."
            },
            {
              "docId": "2026-01-11_langchain_blog_2253",
              "url": "https://blog.langchain.dev/2026/01/11/attention_is_all_you_need_v2_c",
              "published": "2026-01-11",
              "snippet": "Attention Is All You Need v2 leverages Constitutional AI to achieve state-of-the-art performance..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__transformer_architecture_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.83
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_llama_4_evaluated_on",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "model:llama_4",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.81,
          "evidence": [
            {
              "docId": "2025-12-31_weights_and_bia_1789",
              "url": "https://wandb.ai/articles/2025/12/31/flash_attention:_fast_and_memo",
              "published": "2025-12-31",
              "snippet": "On the Llama 4 benchmark, Flash Attention: Fast and Memory-Efficient Attention scored 85%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_sora_2_integrates_with",
          "source": "repo:vllm",
          "target": "model:sora_2",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.88,
          "evidence": [
            {
              "docId": "2026-01-16_nextgov_5210",
              "url": "https://nextgov.com/2026/01/16/vllm_sora_2",
              "published": "2026-01-16",
              "snippet": "vllm now supports Sora 2 with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:sora_2_laion_5b_trained_on",
          "source": "model:sora_2",
          "target": "dataset:laion_5b",
          "rel": "TRAINED_ON",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:langchain_gemini_ultra_2_lite_integrates_with",
          "source": "tool:langchain",
          "target": "model:gemini_ultra_2_lite",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-16_nextgov_4701",
              "url": "https://nextgov.com/2025/12/16/langchain_gemini_ultra_2_lite",
              "published": "2025-12-16",
              "snippet": "LangChain now supports Gemini Ultra 2 Lite with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:crewai_tree_of_thought_uses_tech",
          "source": "tool:crewai",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.76
        }
      },
      {
        "data": {
          "id": "e:haystack_claude_sonnet_4_integrates_with",
          "source": "tool:haystack",
          "target": "model:claude_sonnet_4",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2025-11-26_the_gradient_9514",
              "url": "https://thegradient.pub/2025/11/26/haystack_claude_sonnet_4",
              "published": "2025-11-26",
              "snippet": "The latest release of Haystack adds native Claude Sonnet 4 integration..."
            },
            {
              "docId": "2025-12-01_the_verge_1034",
              "url": "https://theverge.com/2025/12/01/haystack_claude_sonnet_4",
              "published": "2025-12-01",
              "snippet": "The latest release of Haystack adds native Claude Sonnet 4 integration..."
            },
            {
              "docId": "2025-12-05_openai_blog_7839",
              "url": "https://openai.com/blog/2025/12/05/haystack_claude_sonnet_4",
              "published": "2025-12-05",
              "snippet": "The latest release of Haystack adds native Claude Sonnet 4 integration..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_tree_of_thought_uses_tech",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "tech:tree_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.92,
          "evidence": [
            {
              "docId": "2026-01-22_wired_2252",
              "url": "https://wired.com/2026/01/22/toolformer:_language_models_ca",
              "published": "2026-01-22",
              "snippet": "Under the hood, Toolformer: Language Models Can Teach Themselves to Use Tools implements Tree of Thought for improved efficiency..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:transformers_aya_3_integrates_with",
          "source": "repo:transformers",
          "target": "model:aya_3",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.71,
          "evidence": [
            {
              "docId": "2026-01-14_the_gradient_8526",
              "url": "https://thegradient.pub/2026/01/14/transformers_aya_3",
              "published": "2026-01-14",
              "snippet": "transformers announced official support for Aya 3..."
            },
            {
              "docId": "2026-01-19_ars_technica_8936",
              "url": "https://arstechnica.com/2026/01/19/transformers_aya_3",
              "published": "2026-01-19",
              "snippet": "transformers announced official support for Aya 3..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:tensorrt_llm_nemotron_5_integrates_with",
          "source": "tool:tensorrt_llm",
          "target": "model:nemotron_5",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.76,
          "evidence": [
            {
              "docId": "2025-12-31_mit_technology__6163",
              "url": "https://technologyreview.com/2025/12/31/tensorrt_llm_nemotron_5",
              "published": "2025-12-31",
              "snippet": "The latest release of TensorRT-LLM adds native Nemotron-5 integration..."
            },
            {
              "docId": "2026-01-16_meta_ai_blog_9037",
              "url": "https://ai.meta.com/blog/2026/01/16/tensorrt_llm_nemotron_5",
              "published": "2026-01-16",
              "snippet": "TensorRT-LLM announced official support for Nemotron-5..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:streamlit_command_r_plus_integrates_with",
          "source": "tool:streamlit",
          "target": "model:command_r_plus",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2025-12-29_venturebeat_4405",
              "url": "https://venturebeat.com/2025/12/29/streamlit_command_r_plus",
              "published": "2025-12-29",
              "snippet": "The latest release of Streamlit adds native Command R+ integration..."
            },
            {
              "docId": "2026-01-17_google_ai_blog_4396",
              "url": "https://blog.google/technology/ai/2026/01/17/streamlit_command_r_plus",
              "published": "2026-01-17",
              "snippet": "The latest release of Streamlit adds native Command R+ integration..."
            },
            {
              "docId": "2026-01-25_venturebeat_7612",
              "url": "https://venturebeat.com/2026/01/25/streamlit_command_r_plus",
              "published": "2026-01-25",
              "snippet": "Streamlit announced official support for Command R+..."
            },
            {
              "docId": "2026-01-25_wired_8257",
              "url": "https://wired.com/2026/01/25/streamlit_command_r_plus",
              "published": "2026-01-25",
              "snippet": "Streamlit now supports Command R+ with full feature parity..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:aya_3_math_evaluated_on",
          "source": "model:aya_3",
          "target": "benchmark:math",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-12-28_wired_3777",
              "url": "https://wired.com/2025/12/28/aya_3_math",
              "published": "2025-12-28",
              "snippet": "Evaluation results show Aya 3 reaching 87% on MATH..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_lora_uses_tech",
          "source": "model:nemotron_5",
          "target": "tech:lora",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.73
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_sliding_window_attention_uses_tech",
          "source": "model:claude_opus_45",
          "target": "tech:sliding_window_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.58
        }
      },
      {
        "data": {
          "id": "e:arc_agi_mixtral_8x22b_measures",
          "source": "benchmark:arc_agi",
          "target": "model:mixtral_8x22b",
          "rel": "MEASURES",
          "kind": "inferred",
          "confidence": 0.94
        }
      },
      {
        "data": {
          "id": "e:truthfulqa_gpt_5_core_measures",
          "source": "benchmark:truthfulqa",
          "target": "model:gpt_5_core",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.87,
          "evidence": [
            {
              "docId": "2026-01-19_meta_ai_blog_5186",
              "url": "https://ai.meta.com/blog/2026/01/19/truthfulqa_gpt_5_core",
              "published": "2026-01-19",
              "snippet": "The TruthfulQA benchmark measures GPT-5 Core across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:the_pile_speculative_decoding_uses_tech",
          "source": "dataset:the_pile",
          "target": "tech:speculative_decoding",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_humaneval_evaluated_on",
          "source": "model:nemotron_5",
          "target": "benchmark:humaneval",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.86,
          "evidence": [
            {
              "docId": "2025-12-25_google_ai_blog_1758",
              "url": "https://blog.google/technology/ai/2025/12/25/nemotron_5_humaneval",
              "published": "2025-12-25",
              "snippet": "Nemotron-5 achieves 77% on HumanEval, setting a new record..."
            },
            {
              "docId": "2026-01-17_nextgov_8939",
              "url": "https://nextgov.com/2026/01/17/nemotron_5_humaneval",
              "published": "2026-01-17",
              "snippet": "Nemotron-5 achieves 95% on HumanEval, setting a new record..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:llamaindex_group_query_attention_uses_tech",
          "source": "tool:llamaindex",
          "target": "tech:group_query_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:jamba_2_flash_attention_uses_tech",
          "source": "model:jamba_2",
          "target": "tech:flash_attention",
          "rel": "USES_TECH",
          "kind": "inferred",
          "confidence": 0.79
        }
      },
      {
        "data": {
          "id": "e:sora_2_claude_sonnet_4_depends_on",
          "source": "model:sora_2",
          "target": "model:claude_sonnet_4",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:vllm_rlhf_uses_tech",
          "source": "repo:vllm",
          "target": "tech:rlhf",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.84,
          "evidence": [
            {
              "docId": "2026-01-12_wired_2834",
              "url": "https://wired.com/2026/01/12/vllm_rlhf",
              "published": "2026-01-12",
              "snippet": "Technical details reveal vllm relies heavily on RLHF..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:mmlu_nemotron_5_measures",
          "source": "benchmark:mmlu",
          "target": "model:nemotron_5",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.93,
          "evidence": [
            {
              "docId": "2026-01-23_bloomberg_2401",
              "url": "https://bloomberg.com/technology/2026/01/23/mmlu_nemotron_5",
              "published": "2026-01-23",
              "snippet": "The MMLU benchmark measures Nemotron-5 across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:gpqa_mixtral_8x22b_measures",
          "source": "benchmark:gpqa",
          "target": "model:mixtral_8x22b",
          "rel": "MEASURES",
          "kind": "asserted",
          "confidence": 0.91,
          "evidence": [
            {
              "docId": "2026-01-17_meta_ai_blog_2442",
              "url": "https://ai.meta.com/blog/2026/01/17/gpqa_mixtral_8x22b",
              "published": "2026-01-17",
              "snippet": "The GPQA benchmark measures Mixtral 8x22B across multiple tasks..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:midjourney_v7_gsm8k_evaluated_on",
          "source": "model:midjourney_v7",
          "target": "benchmark:gsm8k",
          "rel": "EVALUATED_ON",
          "kind": "asserted",
          "confidence": 0.83,
          "evidence": [
            {
              "docId": "2025-11-27_microsoft_resea_1319",
              "url": "https://microsoft.com/en-us/research/blog/2025/11/27/midjourney_v7_gsm8k",
              "published": "2025-11-27",
              "snippet": "On the GSM8K benchmark, Midjourney V7 scored 92%..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:chain_of_thought_prompting_elicits_reaso_quantization_uses_tech",
          "source": "paper:chain_of_thought_prompting_elicits_reaso",
          "target": "tech:quantization",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.8,
          "evidence": [
            {
              "docId": "2026-01-25_mit_technology__8530",
              "url": "https://technologyreview.com/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Chain-of-Thought Prompting Elicits Reasoning leverages Quantization to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_7451",
              "url": "https://wandb.ai/articles/2026/01/25/chain_of_thought_prompting_eli",
              "published": "2026-01-25",
              "snippet": "Technical details reveal Chain-of-Thought Prompting Elicits Reasoning relies heavily on Quantization..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:vllm_palm_3_integrates_with",
          "source": "repo:vllm",
          "target": "model:palm_3",
          "rel": "INTEGRATES_WITH",
          "kind": "inferred",
          "confidence": 0.75
        }
      },
      {
        "data": {
          "id": "e:claude_opus_45_v2_gemini_ultra_2_depends_on",
          "source": "model:claude_opus_45_v2",
          "target": "model:gemini_ultra_2",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.62
        }
      },
      {
        "data": {
          "id": "e:laion_5b_rotary_position_embedding_uses_tech",
          "source": "dataset:laion_5b",
          "target": "tech:rotary_position_embedding",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.48
        }
      },
      {
        "data": {
          "id": "e:nemotron_5_transformer_architecture_uses_tech",
          "source": "model:nemotron_5",
          "target": "tech:transformer_architecture",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.82
        }
      },
      {
        "data": {
          "id": "e:claude_sonnet_4_mixtral_8x22b_depends_on",
          "source": "model:claude_sonnet_4",
          "target": "model:mixtral_8x22b",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.45
        }
      },
      {
        "data": {
          "id": "e:constitutional_ai:_harmlessness_from_ai__chain_of_thought_uses_tech",
          "source": "paper:constitutional_ai:_harmlessness_from_ai_",
          "target": "tech:chain_of_thought",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.77,
          "evidence": [
            {
              "docId": "2026-01-06_bloomberg_8927",
              "url": "https://bloomberg.com/technology/2026/01/06/constitutional_ai:_harmlessnes",
              "published": "2026-01-06",
              "snippet": "Technical details reveal Constitutional AI: Harmlessness from AI Feedback relies heavily on Chain-of-Thought..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:toolformer:_language_models_can_teach_th_falcon_3_evaluated_on",
          "source": "paper:toolformer:_language_models_can_teach_th",
          "target": "model:falcon_3",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.7
        }
      },
      {
        "data": {
          "id": "e:gemma_3_gpqa_evaluated_on",
          "source": "model:gemma_3",
          "target": "benchmark:gpqa",
          "rel": "EVALUATED_ON",
          "kind": "inferred",
          "confidence": 0.9
        }
      },
      {
        "data": {
          "id": "e:gemini_ultra_2_lite_midjourney_v7_depends_on",
          "source": "model:gemini_ultra_2_lite",
          "target": "model:midjourney_v7",
          "rel": "DEPENDS_ON",
          "kind": "hypothesis",
          "confidence": 0.53
        }
      },
      {
        "data": {
          "id": "e:fineweb_gguf_uses_tech",
          "source": "dataset:fineweb",
          "target": "tech:gguf",
          "rel": "USES_TECH",
          "kind": "hypothesis",
          "confidence": 0.42
        }
      },
      {
        "data": {
          "id": "e:haystack_crewai_integrates_with",
          "source": "tool:haystack",
          "target": "tool:crewai",
          "rel": "INTEGRATES_WITH",
          "kind": "asserted",
          "confidence": 0.67,
          "evidence": [
            {
              "docId": "2025-12-26_google_ai_blog_4960",
              "url": "https://blog.google/technology/ai/2025/12/26/haystack_crewai",
              "published": "2025-12-26",
              "snippet": "Haystack announced official support for CrewAI..."
            },
            {
              "docId": "2026-01-03_nvidia_blog_8734",
              "url": "https://blogs.nvidia.com/2026/01/03/haystack_crewai",
              "published": "2026-01-03",
              "snippet": "Haystack announced official support for CrewAI..."
            }
          ]
        }
      },
      {
        "data": {
          "id": "e:flash_attention:_fast_and_memory_efficie_tool_use_uses_tech",
          "source": "paper:flash_attention:_fast_and_memory_efficie",
          "target": "tech:tool_use",
          "rel": "USES_TECH",
          "kind": "asserted",
          "confidence": 0.94,
          "evidence": [
            {
              "docId": "2026-01-11_openai_blog_6415",
              "url": "https://openai.com/blog/2026/01/11/flash_attention:_fast_and_memo",
              "published": "2026-01-11",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention leverages Tool Use to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-17_the_verge_8534",
              "url": "https://theverge.com/2026/01/17/flash_attention:_fast_and_memo",
              "published": "2026-01-17",
              "snippet": "Under the hood, Flash Attention: Fast and Memory-Efficient Attention implements Tool Use for improved efficiency..."
            },
            {
              "docId": "2026-01-24_microsoft_resea_5933",
              "url": "https://microsoft.com/en-us/research/blog/2026/01/24/flash_attention:_fast_and_memo",
              "published": "2026-01-24",
              "snippet": "Flash Attention: Fast and Memory-Efficient Attention leverages Tool Use to achieve state-of-the-art performance..."
            },
            {
              "docId": "2026-01-25_weights_and_bia_7125",
              "url": "https://wandb.ai/articles/2026/01/25/flash_attention:_fast_and_memo",
              "published": "2026-01-25",
              "snippet": "Under the hood, Flash Attention: Fast and Memory-Efficient Attention implements Tool Use for improved efficiency..."
            }
          ]
        }
      }
    ]
  }
}
